{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Self-Paced Practice: Test-Driven Debugging\n",
    "\n",
    "**Objective:** Reinforce the concepts of Day 4 by using an LLM to perform a complete test-driven debugging workflow on a new, isolated problem.\n",
    "\n",
    "**Estimated Time:** 45 minutes\n",
    "\n",
    "**Introduction:**\n",
    "A powerful workflow for fixing bugs is to first write a test that fails because of the bug, and then fix the code to make the test pass. This is a core principle of Test-Driven Development (TDD). In this lab, you will use an AI co-pilot to execute this entire workflow on a provided piece of buggy code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Let's start by setting up our environment and initializing the LLM client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, clean_llm_output\n",
    "\n",
    "# Models with strong reasoning and code understanding are best for this task.\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Buggy Code\n",
    "\n",
    "Here is a Python function that is supposed to calculate the total cost of a shopping cart. It takes a list of items (as dictionaries) and an optional discount percentage. However, it contains a subtle logical bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buggy_code = \"\"\" \n",
    "def calculate_total(items, discount_percent=0):\n",
    "    \\\"\\\"\\\"Calculates the total cost of a shopping cart with an optional discount.\\\"\\\"\\\" \n",
    "    total = sum(item['price'] * item['quantity'] for item in items)\n",
    "    if discount_percent > 0:\n",
    "        # The bug is here: This calculates the discount amount, not the final price.\n",
    "        total = total * (discount_percent / 100)\n",
    "    return total\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \u2b50 Deeper Dive: The Test-Driven Development (TDD) Cycle\n",
    "\n",
    "The workflow in this lab is a direct application of the **Red-Green-Refactor** cycle from Test-Driven Development, a professional software development practice:\n",
    "\n",
    "1.  **Red:** Write a test that fails. This is our AI-generated `failing_test_code`. The test fails because the bug exists, proving that the bug is real and reproducible.\n",
    "2.  **Green:** Write the minimum amount of code required to make the failing test pass. This is the `fixed_code` our AI will generate.\n",
    "3.  **Refactor:** With the test now passing, you can safely clean up or improve the code without changing its behavior, confident that the test will catch any new bugs.\n",
    "\n",
    "Using AI to accelerate the Red and Green steps can make this powerful practice much faster and more accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your Task\n",
    "\n",
    "Use an LLM to perform a three-step debugging process.\n",
    "\n",
    "1.  **Analyze the Bug:** Prompt the LLM to explain the logical error in the `buggy_code`.\n",
    "2.  **Write a Failing Test:** Prompt the LLM to write a `pytest` function that will fail because of the bug.\n",
    "3.  **Fix the Code:** Prompt the LLM to fix the `buggy_code`, using the failing test as context for what the correct behavior should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \ud83d\udca1 Pro-Tip: Fixing Bugs with Tests as Context\n",
    "\n",
    "The most effective way to ask an LLM to fix a bug is to provide it with **both the buggy code and a failing test case.**\n",
    "\n",
    "Why? The failing test gives the LLM an unambiguous, executable definition of what the *correct* output should be. Instead of just saying \"the discount is wrong,\" you're saying, `\"assert calculate_total(cart, 10) == 90.0\"`. This concrete example removes ambiguity and dramatically increases the chances that the LLM will produce the correct fix on the first try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 1: Analyze and explain the bug --- \n",
    "analyze_bug_prompt = f\"\"\"\n",
    "You are a senior Python developer acting as a code reviewer.\n",
    "Analyze the following Python function and explain the logical error in it. Be concise.\n",
    "\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "\"\"\"\n",
    "print(\"--- Analyzing Bug ---\")\n",
    "bug_explanation = get_completion(analyze_bug_prompt, client, model_name, api_provider)\n",
    "print(bug_explanation)\n",
    "\n",
    "# --- Task 2: Write a failing pytest test ---\n",
    "failing_test_prompt = f\"\"\"\n",
    "You are a QA engineer writing tests with pytest.\n",
    "Based on the following Python function, write a test function named `test_calculate_total_with_discount` that will FAIL because of the bug in the discount calculation. The test should assert the correct expected value.\n",
    "\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "\"\"\"\n",
    "print(\"\\n--- Generating Failing Test ---\")\n",
    "failing_test_code_raw = get_completion(failing_test_prompt, client, model_name, api_provider)\n",
    "failing_test_code = clean_llm_output(failing_test_code_raw, language='python')\n",
    "print(failing_test_code)\n",
    "\n",
    "# --- Task 3: Fix the code based on the failing test ---\n",
    "fix_code_prompt = f\"\"\"\n",
    "You are a senior Python developer tasked with fixing a bug.\n",
    "\n",
    "The following function is buggy:\n",
    "<buggy_code>\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "</buggy_code>\n",
    "\n",
    "The following pytest test fails when run against it, demonstrating the bug:\n",
    "<failing_test>\n",
    "```python\n",
    "{failing_test_code}\n",
    "```\n",
    "</failing_test>\n",
    "\n",
    "Your task is to fix the `calculate_total` function so that the provided test will pass. Output only the corrected function.\n",
    "\"\"\"\n",
    "print(\"\\n--- Generating Fixed Code ---\")\n",
    "fixed_code_raw = get_completion(fix_code_prompt, client, model_name, api_provider)\n",
    "fixed_code = clean_llm_output(fixed_code_raw, language='python')\n",
    "print(fixed_code)\n",
    "\n",
    "# Save the final corrected code\n",
    "if fixed_code:\n",
    "    save_artifact(fixed_code, \"app/day4_sp_fixed_shopping_cart.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent! You have successfully used an AI co-pilot to execute a professional, test-driven debugging workflow. This process\u2014identifying a bug, writing a test to prove it exists, and then fixing the code to make the test pass\u2014is a highly effective way to systematically improve code quality and prevent regressions. This is a skill you can apply to any codebase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}