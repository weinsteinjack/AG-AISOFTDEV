{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Self-Paced Practice: Test-Driven Debugging\n",
    "\n",
    "**Objective:** Reinforce the concepts of Day 4 by using an LLM to perform a complete test-driven debugging workflow on a new, isolated problem.\n",
    "\n",
    "**Estimated Time:** 45 minutes\n",
    "\n",
    "**Introduction:**\n",
    "A powerful workflow for fixing bugs is to first write a test that fails because of the bug, and then fix the code to make the test pass. This is a core principle of Test-Driven Development (TDD). In this lab, you will use an AI co-pilot to execute this entire workflow on a provided piece of buggy code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Let's start by setting up our environment and initializing the LLM client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 12:05:22,865 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, clean_llm_output\n",
    "\n",
    "# Models with strong reasoning and code understanding are best for this task.\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Buggy Code\n",
    "\n",
    "Here is a Python function that is supposed to calculate the total cost of a shopping cart. It takes a list of items (as dictionaries) and an optional discount percentage. However, it contains a subtle logical bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "buggy_code = \"\"\" \n",
    "def calculate_total(items, discount_percent=0):\n",
    "    \\\"\\\"\\\"Calculates the total cost of a shopping cart with an optional discount.\\\"\\\"\\\" \n",
    "    total = sum(item['price'] * item['quantity'] for item in items)\n",
    "    if discount_percent > 0:\n",
    "        # The bug is here: This calculates the discount amount, not the final price.\n",
    "        total = total * (discount_percent / 100)\n",
    "    return total\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⭐ Deeper Dive: The Test-Driven Development (TDD) Cycle\n",
    "\n",
    "The workflow in this lab is a direct application of the **Red-Green-Refactor** cycle from Test-Driven Development, a professional software development practice:\n",
    "\n",
    "1.  **Red:** Write a test that fails. This is our AI-generated `failing_test_code`. The test fails because the bug exists, proving that the bug is real and reproducible.\n",
    "2.  **Green:** Write the minimum amount of code required to make the failing test pass. This is the `fixed_code` our AI will generate.\n",
    "3.  **Refactor:** With the test now passing, you can safely clean up or improve the code without changing its behavior, confident that the test will catch any new bugs.\n",
    "\n",
    "Using AI to accelerate the Red and Green steps can make this powerful practice much faster and more accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Your Task\n",
    "\n",
    "Use an LLM to perform a three-step debugging process.\n",
    "\n",
    "1.  **Analyze the Bug:** Prompt the LLM to explain the logical error in the `buggy_code`.\n",
    "2.  **Write a Failing Test:** Prompt the LLM to write a `pytest` function that will fail because of the bug.\n",
    "3.  **Fix the Code:** Prompt the LLM to fix the `buggy_code`, using the failing test as context for what the correct behavior should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 Pro-Tip: Fixing Bugs with Tests as Context\n",
    "\n",
    "The most effective way to ask an LLM to fix a bug is to provide it with **both the buggy code and a failing test case.**\n",
    "\n",
    "Why? The failing test gives the LLM an unambiguous, executable definition of what the *correct* output should be. Instead of just saying \"the discount is wrong,\" you're saying, `\"assert calculate_total(cart, 10) == 90.0\"`. This concrete example removes ambiguity and dramatically increases the chances that the LLM will produce the correct fix on the first try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Bug ---\n",
      "The logical error is that the function returns the **discount amount** itself, not the final discounted price.\n",
      "\n",
      "The line `total = total * (discount_percent / 100)` calculates the value of the discount, but it fails to subtract this amount from the original total.\n",
      "\n",
      "The correct calculation should be `total = total * (1 - discount_percent / 100)`.\n",
      "\n",
      "--- Generating Failing Test ---\n",
      "import pytest\n",
      "from your_module import calculate_total # Assuming the function is in 'your_module.py'\n",
      "\n",
      "def test_calculate_total_with_discount():\n",
      "    \"\"\"\n",
      "    Tests the calculate_total function with a discount to expose a bug.\n",
      "    \n",
      "    The test will FAIL because the function incorrectly returns only the discount amount\n",
      "    instead of the final price after the discount has been applied.\n",
      "    \"\"\"\n",
      "    # Arrange: Set up the test data\n",
      "    shopping_cart_items = [\n",
      "        {'price': 50, 'quantity': 2},  # Subtotal for this item is 100\n",
      "        {'price': 25, 'quantity': 2},  # Subtotal for this item is 50\n",
      "    ]\n",
      "    # The total pre-discount cost is 100 + 50 = 150\n",
      "    \n",
      "    discount = 20  # 20% discount\n",
      "    \n",
      "    # Act: Calculate the expected correct value manually\n",
      "    # Expected discount amount = 150 * 0.20 = 30\n",
      "    # Expected final total = 150 - 30 = 120\n",
      "    expected_total = 120.0\n",
      "    \n",
      "    # Assert: Call the function and check if its output matches the expected value\n",
      "    actual_total = calculate_total(shopping_cart_items, discount_percent=discount)\n",
      "    \n",
      "    # This assertion will fail because the bug causes `actual_total` to be 30.0\n",
      "    assert actual_total == expected_total\n",
      "\n",
      "--- Generating Fixed Code ---\n",
      "def calculate_total(items, discount_percent=0):\n",
      "    \"\"\"Calculates the total cost of a shopping cart with an optional discount.\"\"\" \n",
      "    total = sum(item['price'] * item['quantity'] for item in items)\n",
      "    if discount_percent > 0:\n",
      "        # Apply the discount to find the final price.\n",
      "        discount_multiplier = 1 - (discount_percent / 100)\n",
      "        total *= discount_multiplier\n",
      "    return total\n"
     ]
    }
   ],
   "source": [
    "# --- Task 1: Analyze and explain the bug --- \n",
    "analyze_bug_prompt = f\"\"\"\n",
    "You are a senior Python developer acting as a code reviewer.\n",
    "Analyze the following Python function and explain the logical error in it. Be concise.\n",
    "\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "\"\"\"\n",
    "print(\"--- Analyzing Bug ---\")\n",
    "bug_explanation = get_completion(analyze_bug_prompt, client, model_name, api_provider)\n",
    "print(bug_explanation)\n",
    "\n",
    "# --- Task 2: Write a failing pytest test ---\n",
    "failing_test_prompt = f\"\"\"\n",
    "You are a QA engineer writing tests with pytest.\n",
    "Based on the following Python function, write a test function named `test_calculate_total_with_discount` that will FAIL because of the bug in the discount calculation. The test should assert the correct expected value.\n",
    "\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "\"\"\"\n",
    "print(\"\\n--- Generating Failing Test ---\")\n",
    "failing_test_code_raw = get_completion(failing_test_prompt, client, model_name, api_provider)\n",
    "failing_test_code = clean_llm_output(failing_test_code_raw, language='python')\n",
    "print(failing_test_code)\n",
    "\n",
    "# --- Task 3: Fix the code based on the failing test ---\n",
    "fix_code_prompt = f\"\"\"\n",
    "You are a senior Python developer tasked with fixing a bug.\n",
    "\n",
    "The following function is buggy:\n",
    "<buggy_code>\n",
    "```python\n",
    "{buggy_code}\n",
    "```\n",
    "</buggy_code>\n",
    "\n",
    "The following pytest test fails when run against it, demonstrating the bug:\n",
    "<failing_test>\n",
    "```python\n",
    "{failing_test_code}\n",
    "```\n",
    "</failing_test>\n",
    "\n",
    "Your task is to fix the `calculate_total` function so that the provided test will pass. Output only the corrected function.\n",
    "\"\"\"\n",
    "print(\"\\n--- Generating Fixed Code ---\")\n",
    "fixed_code_raw = get_completion(fix_code_prompt, client, model_name, api_provider)\n",
    "fixed_code = clean_llm_output(fixed_code_raw, language='python')\n",
    "print(fixed_code)\n",
    "\n",
    "# Save the final corrected code\n",
    "if fixed_code:\n",
    "    save_artifact(fixed_code, \"app/day4_sp_fixed_shopping_cart.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent! You have successfully used an AI co-pilot to execute a professional, test-driven debugging workflow. This process—identifying a bug, writing a test to prove it exists, and then fixing the code to make the test pass—is a highly effective way to systematically improve code quality and prevent regressions. This is a skill you can apply to any codebase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
