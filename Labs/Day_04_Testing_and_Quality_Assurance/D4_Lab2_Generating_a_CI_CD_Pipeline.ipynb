{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 2: Generating a CI/CD Pipeline\n",
    "\n",
    "**Objective:** Use an LLM to generate all necessary configuration files to create an automated Continuous Integration (CI) pipeline for the FastAPI application using Docker and GitHub Actions.\n",
    "\n",
    "**Estimated Time:** 75 minutes\n",
    "\n",
    "**Introduction:**\n",
    "A robust CI pipeline is the backbone of modern software development. It automatically builds and tests your code every time a change is made, catching bugs early and ensuring quality. In this lab, you will generate all the configuration-as-code artifacts needed to build a professional CI pipeline for our application.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load our application code to provide context for the LLM. The AI needs to see our code's imports to generate an accurate `requirements.txt` file.\n",
    "\n",
    "**Model Selection:**\n",
    "Models that are good at understanding code and structured data formats like YAML are ideal. `gpt-4.1`, `o3`, or `codex-mini` are strong choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated configuration files.\n",
    "- `clean_llm_output()`: To clean up the generated text and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a `requirements.txt`\n",
    "\n",
    "**Task:** Before we can build a Docker image, we need a list of our Python dependencies. Prompt the LLM to analyze your application code and generate a `requirements.txt` file.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt that provides the LLM with the source code of your FastAPI application (`app_code`).\n",
    "2.  Instruct it to analyze the `import` statements and generate a list of all external dependencies (like `fastapi`, `uvicorn`, `sqlalchemy`). You should also ask it to include `pytest` for testing.\n",
    "3.  The output should be formatted as a standard `requirements.txt` file.\n",
    "4.  Save the artifact to the project's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate a requirements.txt file.\n",
    "requirements_prompt = f\"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating requirements.txt ---\")\n",
    "if app_code:\n",
    "    requirements_content = get_completion(requirements_prompt, client, model_name, api_provider)\n",
    "    cleaned_reqs = clean_llm_output(requirements_content, language='text')\n",
    "    print(cleaned_reqs)\n",
    "    save_artifact(cleaned_reqs, \"requirements.txt\")\n",
    "else:\n",
    "    print(\"Skipping requirements generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a `Dockerfile`\n",
    "\n",
    "**Task:** Generate a multi-stage `Dockerfile` to create an optimized and secure container image for our application.\n",
    "\n",
    "> **Tip:** Why a multi-stage Dockerfile? The first stage (the 'builder') installs all dependencies, including build-time tools. The final stage copies only the application code and the necessary installed packages. This results in a much smaller, more secure production image because it doesn't contain any unnecessary build tools.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt asking for a multi-stage `Dockerfile` for a Python FastAPI application.\n",
    "2.  Specify the following requirements:\n",
    "    * Use a slim Python base image (e.g., `python:3.11-slim`).\n",
    "    * The first stage should install dependencies from `requirements.txt`.\n",
    "    * The final stage should copy the installed dependencies and the application code.\n",
    "    * The `CMD` should execute the application using `uvicorn`.\n",
    "3.  Save the generated file as `Dockerfile` in the project's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate a multi-stage Dockerfile.\n",
    "dockerfile_prompt = \"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Dockerfile ---\")\n",
    "dockerfile_content = get_completion(dockerfile_prompt, client, model_name, api_provider)\n",
    "cleaned_dockerfile = clean_llm_output(dockerfile_content, language='dockerfile')\n",
    "print(cleaned_dockerfile)\n",
    "\n",
    "if cleaned_dockerfile:\n",
    "    save_artifact(cleaned_dockerfile, \"Dockerfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating the GitHub Actions Workflow\n",
    "\n",
    "**Task:** Generate a complete GitHub Actions workflow file to automate the build and test process.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt to generate a GitHub Actions workflow file named `ci.yml`.\n",
    "2.  Specify the following requirements for the workflow:\n",
    "    * It should trigger on any `push` to the `main` branch.\n",
    "    * It should define a single job named `build-and-test` that runs on `ubuntu-latest`.\n",
    "    * The job should have steps to: 1) Check out the code, 2) Set up a Python environment, 3) Install dependencies from `requirements.txt`, and 4) Run the test suite using `pytest`.\n",
    "3.  Save the generated YAML file to `.github/workflows/ci.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to generate the GitHub Actions ci.yml file.\n",
    "ci_workflow_prompt = \"\"\"\n",
    "# Your prompt here\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating GitHub Actions Workflow ---\")\n",
    "ci_workflow_content = get_completion(ci_workflow_prompt, client, model_name, api_provider)\n",
    "cleaned_yaml = clean_llm_output(ci_workflow_content, language='yaml')\n",
    "print(cleaned_yaml)\n",
    "\n",
    "if cleaned_yaml:\n",
    "    # Note: The save_artifact helper creates directories as needed.\n",
    "    save_artifact(cleaned_yaml, \".github/workflows/ci.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent! You have now generated a complete, professional Continuous Integration pipeline using AI. You created the dependency list, the containerization configuration, and the automation workflow, all from simple prompts. This is a powerful demonstration of how AI can automate complex DevOps tasks, allowing teams to build and ship software with greater speed and confidence.\n",
    "\n",
    "> **Key Takeaway:** AI is a powerful tool for generating 'Configuration as Code' artifacts. By prompting for specific formats like `requirements.txt`, `Dockerfile`, or `ci.yml`, you can automate the creation of the files that define your entire build, test, and deployment processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}