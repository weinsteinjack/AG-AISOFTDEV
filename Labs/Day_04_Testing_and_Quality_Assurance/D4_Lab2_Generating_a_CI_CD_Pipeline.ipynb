{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 2: Generating a CI/CD Pipeline\n",
    "\n",
    "**Objective:** Use an LLM to generate all necessary configuration files to create an automated Continuous Integration (CI) pipeline for the FastAPI application using Docker and GitHub Actions.\n",
    "\n",
    "**Estimated Time:** 75 minutes\n",
    "\n",
    "**Introduction:**\n",
    "A robust CI pipeline is the backbone of modern software development. It automatically builds and tests your code every time a change is made, catching bugs early and ensuring quality. In this lab, you will generate all the configuration-as-code artifacts needed to build a professional CI pipeline for our application.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load our application code to provide context for the LLM. The AI needs to see our code's imports to generate an accurate `requirements.txt` file.\n",
    "\n",
    "**Model Selection:**\n",
    "Models that are good at understanding code and structured data formats like YAML are ideal. `gpt-4.1`, `o3`, or `codex-mini` are strong choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated configuration files.\n",
    "- `clean_llm_output()`: To clean up the generated text and code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-31 11:35:05,022 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating a `requirements.txt`\n",
    "\n",
    "**Task:** Before we can build a Docker image, we need a list of our Python dependencies. Prompt the LLM to analyze your application code and generate a `requirements.txt` file.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt that provides the LLM with the source code of your FastAPI application (`app_code`).\n",
    "2.  Instruct it to analyze the `import` statements and generate a list of all external dependencies (like `fastapi`, `uvicorn`, `sqlalchemy`). You should also ask it to include `pytest` for testing.\n",
    "3.  The output should be formatted as a standard `requirements.txt` file.\n",
    "4.  Save the artifact to the project's root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating requirements.txt ---\n",
      "# Core Framework\n",
      "fastapi>=0.111.0,<1.0.0\n",
      "uvicorn[standard]>=0.29.0,<1.0.0\n",
      "\n",
      "# Database\n",
      "sqlalchemy~=2.0.29\n",
      "\n",
      "# Validation and Settings\n",
      "email-validator>=2.0.0,<3.0.0  # Required by Pydantic's EmailStr\n",
      "pydantic>=2.7.0,<3.0.0\n",
      "pydantic-settings>=2.2.0,<3.0.0\n",
      "\n",
      "# Testing\n",
      "httpx>=0.27.0,<1.0.0  # Required by FastAPI's TestClient\n",
      "pytest>=8.0.0,<9.0.0\n"
     ]
    }
   ],
   "source": [
    "# Detailed prompt to generate a requirements.txt file\n",
    "requirements_prompt = f\"\"\"You are a Python dependency management expert. Analyze the following FastAPI application code and generate a complete, production-ready `requirements.txt` file.\n",
    "\n",
    "**Application Code:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "**Task Requirements:**\n",
    "\n",
    "1. **Systematically analyze ALL import statements** in the code, including:\n",
    "   - Direct imports (e.g., `import uvicorn`)\n",
    "   - From imports (e.g., `from fastapi import FastAPI`)\n",
    "   - Third-party packages (exclude standard library modules like `os`, `sys`, `datetime`, `enum`, `typing`, `logging`)\n",
    "\n",
    "2. **Include ALL necessary dependencies:**\n",
    "   - Core FastAPI dependencies: `fastapi`, `uvicorn[standard]` (the [standard] extra includes performance optimizations)\n",
    "   - Pydantic and validation: `pydantic`, `pydantic-settings`, `email-validator` (required for EmailStr)\n",
    "   - Database: `sqlalchemy`\n",
    "   - Testing: `pytest`, `httpx` (required by FastAPI's TestClient)\n",
    "\n",
    "3. **Version pinning strategy:**\n",
    "   - Use compatibility version pinning (e.g., `fastapi>=0.104.0,<1.0.0`)\n",
    "   - This allows bug fixes and minor updates while preventing breaking changes\n",
    "   - For stable packages, use `~=` operator (e.g., `sqlalchemy~=2.0.0` means >=2.0.0, <2.1.0)\n",
    "\n",
    "4. **Organization and formatting:**\n",
    "   - Group dependencies by category with comments (Core, Database, Testing, etc.)\n",
    "   - List packages alphabetically within each category\n",
    "   - Include brief inline comments for non-obvious dependencies\n",
    "\n",
    "5. **Best practices:**\n",
    "   - DO NOT include standard library modules\n",
    "   - Include transitive dependencies that are directly imported (like `email-validator`)\n",
    "   - Ensure the file works for both development and CI/CD environments\n",
    "   - Include pytest for the test suite\n",
    "\n",
    "**Expected Output Format:**\n",
    "Generate ONLY the contents of the requirements.txt file. No explanations, no markdown code blocks, no additional commentary. The output should be ready to save directly to a file.\n",
    "\n",
    "Example of desired format:\n",
    "```\n",
    "# Core Framework\n",
    "fastapi>=0.104.0,<1.0.0\n",
    "uvicorn[standard]>=0.24.0,<1.0.0\n",
    "\n",
    "# Database\n",
    "sqlalchemy~=2.0.0\n",
    "\n",
    "# Validation and Settings\n",
    "pydantic>=2.0.0,<3.0.0\n",
    "pydantic-settings>=2.0.0,<3.0.0\n",
    "email-validator>=2.0.0,<3.0.0\n",
    "\n",
    "# Testing\n",
    "pytest>=7.4.0,<8.0.0\n",
    "httpx>=0.25.0,<1.0.0\n",
    "```\n",
    "\n",
    "Generate the complete requirements.txt file now:\"\"\"\n",
    "\n",
    "print(\"--- Generating requirements.txt ---\")\n",
    "if app_code:\n",
    "    requirements_content = get_completion(requirements_prompt, client, model_name, api_provider)\n",
    "    cleaned_reqs = clean_llm_output(requirements_content, language='text')\n",
    "    print(cleaned_reqs)\n",
    "    save_artifact(cleaned_reqs, \"requirements.txt\")\n",
    "else:\n",
    "    print(\"Skipping requirements generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating a `Dockerfile`\n",
    "\n",
    "**Task:** Generate a multi-stage `Dockerfile` to create an optimized and secure container image for our application.\n",
    "\n",
    "> **Tip:** Why a multi-stage Dockerfile? The first stage (the 'builder') installs all dependencies, including build-time tools. The final stage copies only the application code and the necessary installed packages. This results in a much smaller, more secure production image because it doesn't contain any unnecessary build tools.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt asking for a multi-stage `Dockerfile` for a Python FastAPI application.\n",
    "2.  Specify the following requirements:\n",
    "    * Use a slim Python base image (e.g., `python:3.11-slim`).\n",
    "    * The first stage should install dependencies from `requirements.txt`.\n",
    "    * The final stage should copy the installed dependencies and the application code.\n",
    "    * The `CMD` should execute the application using `uvicorn`.\n",
    "3.  Save the generated file as `Dockerfile` in the project's root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Dockerfile ---\n",
      "# For reference, a good .dockerignore file would contain:\n",
      "# .git\n",
      "# .gitignore\n",
      "# .dockerignore\n",
      "# __pycache__/\n",
      "# *.pyc\n",
      "# *.pyo\n",
      "# *.pyd\n",
      "# .pytest_cache/\n",
      "# .venv/\n",
      "# venv/\n",
      "# env/\n",
      "\n",
      "# ---- Stage 1: Builder ----\n",
      "# This stage installs dependencies into a virtual environment.\n",
      "FROM python:3.11-slim AS builder\n",
      "\n",
      "# Set environment variables for Python\n",
      "ENV PYTHONDONTWRITEBYTECODE=1\n",
      "ENV PYTHONUNBUFFERED=1\n",
      "\n",
      "# Set the working directory\n",
      "WORKDIR /opt/venv\n",
      "\n",
      "# Create a virtual environment\n",
      "RUN python -m venv .\n",
      "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
      "\n",
      "# Upgrade pip and install wheel to build packages efficiently\n",
      "RUN pip install --no-cache-dir --upgrade pip wheel\n",
      "\n",
      "# Copy requirements file and install dependencies\n",
      "# This is done in a separate layer to leverage Docker's caching mechanism.\n",
      "# The layer will only be rebuilt if requirements.txt changes.\n",
      "COPY requirements.txt .\n",
      "RUN pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "\n",
      "# ---- Stage 2: Runtime ----\n",
      "# This stage creates the final, minimal production image.\n",
      "FROM python:3.11-slim\n",
      "\n",
      "# Set the working directory in the final image\n",
      "WORKDIR /app\n",
      "\n",
      "# Copy the virtual environment from the builder stage\n",
      "COPY --from=builder /opt/venv /opt/venv\n",
      "\n",
      "# Set the path to include the virtual environment's binaries\n",
      "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
      "\n",
      "# Security: Create a dedicated non-root user and group\n",
      "RUN adduser --system --group appuser\n",
      "\n",
      "# Copy application code into the container\n",
      "# The context for COPY is the root of the project where Dockerfile is located.\n",
      "COPY artifacts/app/ /app/\n",
      "\n",
      "# Grant ownership of the app directory to the non-root user\n",
      "# This is crucial for security and for allowing the application\n",
      "# (e.g., SQLite) to write files if needed.\n",
      "RUN chown -R appuser:appuser /app\n",
      "\n",
      "# Switch to the non-root user\n",
      "USER appuser\n",
      "\n",
      "# Set environment variables for Python again in the final stage\n",
      "ENV PYTHONDONTWRITEBYTECODE=1\n",
      "ENV PYTHONUNBUFFERED=1\n",
      "\n",
      "# Expose the port the application will run on\n",
      "EXPOSE 8000\n",
      "\n",
      "# Define the command to run the application\n",
      "# Using exec form (JSON array) is recommended.\n",
      "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n"
     ]
    }
   ],
   "source": [
    "# Detailed prompt to generate a multi-stage Dockerfile\n",
    "dockerfile_prompt = \"\"\"You are a Docker and containerization expert specializing in Python applications. Generate a production-ready, optimized multi-stage Dockerfile for a FastAPI application.\n",
    "\n",
    "**Project Context:**\n",
    "- FastAPI application with SQLAlchemy database (SQLite)\n",
    "- Application code located in `artifacts/app/` directory\n",
    "- Main application file: `artifacts/app/main.py`\n",
    "- Dependencies listed in `requirements.txt` (already generated)\n",
    "- Database file: `artifacts/app/onboarding.db`\n",
    "- Application should run on port 8000\n",
    "\n",
    "**Task Requirements:**\n",
    "\n",
    "1. **Multi-Stage Build Structure:**\n",
    "   - **Stage 1 (Builder)**: Install all Python dependencies and build wheels\n",
    "     - Base image: `python:3.11-slim`\n",
    "     - Install dependencies from `requirements.txt`\n",
    "     - Use a virtual environment for dependency isolation\n",
    "   - **Stage 2 (Runtime)**: Create minimal production image\n",
    "     - Base image: `python:3.11-slim`\n",
    "     - Copy only the installed dependencies (not build tools)\n",
    "     - Copy application code\n",
    "     - Set up runtime configuration\n",
    "\n",
    "2. **Security Best Practices:**\n",
    "   - Run as non-root user (create and use an `appuser`)\n",
    "   - Use `--no-cache-dir` flag when installing pip packages to reduce image size\n",
    "   - Set `PYTHONUNBUFFERED=1` environment variable for proper logging\n",
    "   - Set `PYTHONDONTWRITEBYTECODE=1` to prevent .pyc file generation\n",
    "\n",
    "3. **Optimization Requirements:**\n",
    "   - Leverage Docker layer caching (copy requirements.txt before application code)\n",
    "   - Minimize final image size by excluding build tools\n",
    "   - Use `.dockerignore` patterns in comments (for user reference)\n",
    "   - Set appropriate WORKDIR paths\n",
    "\n",
    "4. **Application Configuration:**\n",
    "   - WORKDIR should be `/app`\n",
    "   - Copy the entire `artifacts/app/` directory to `/app/`\n",
    "   - Ensure the database directory is writable\n",
    "   - EXPOSE port 8000\n",
    "   - CMD should run: `uvicorn main:app --host 0.0.0.0 --port 8000`\n",
    "   - Note: Since app files are in artifacts/app/, the import path is `main:app`\n",
    "\n",
    "5. **Directory Structure Awareness:**\n",
    "   - Application structure:\n",
    "     ```\n",
    "     artifacts/\n",
    "       app/\n",
    "         main.py          # FastAPI app\n",
    "         onboarding.db    # SQLite database\n",
    "     requirements.txt     # Dependencies\n",
    "     ```\n",
    "\n",
    "**Expected Output Format:**\n",
    "Generate ONLY the Dockerfile contents. No explanations before or after, no markdown code blocks wrapper. Include helpful inline comments in the Dockerfile itself explaining each section. The output should be ready to save directly as a Dockerfile.\n",
    "\n",
    "**Example of desired format and style:**\n",
    "```dockerfile\n",
    "# Stage 1: Builder - Install dependencies\n",
    "FROM python:3.11-slim AS builder\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements first for layer caching\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install dependencies in a virtual environment\n",
    "RUN python -m venv /opt/venv\n",
    "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
    "RUN pip install --no-cache-dir --upgrade pip &&\n",
    "    pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Stage 2: Runtime - Minimal production image\n",
    "FROM python:3.11-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy virtual environment from builder\n",
    "COPY --from=builder /opt/venv /opt/venv\n",
    "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
    "\n",
    "# Security: Create non-root user\n",
    "RUN useradd --create-home --shell /bin/bash appuser\n",
    "\n",
    "# Copy application code\n",
    "COPY artifacts/app/ /app/\n",
    "\n",
    "# Ensure database directory is writable\n",
    "RUN chown -R appuser:appuser /app\n",
    "\n",
    "# Switch to non-root user\n",
    "USER appuser\n",
    "\n",
    "# Python optimization flags\n",
    "ENV PYTHONUNBUFFERED=1\n",
    "    PYTHONDONTWRITEBYTECODE=1\n",
    "\n",
    "# Expose application port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "Generate the complete, production-ready Dockerfile now:\"\"\"\n",
    "\n",
    "print(\"--- Generating Dockerfile ---\")\n",
    "dockerfile_content = get_completion(dockerfile_prompt, client, model_name, api_provider)\n",
    "cleaned_dockerfile = clean_llm_output(dockerfile_content, language='dockerfile')\n",
    "print(cleaned_dockerfile)\n",
    "\n",
    "if cleaned_dockerfile:\n",
    "    save_artifact(cleaned_dockerfile, \"Dockerfile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Generating the GitHub Actions Workflow\n",
    "\n",
    "**Task:** Generate a complete GitHub Actions workflow file to automate the build and test process.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt to generate a GitHub Actions workflow file named `ci.yml`.\n",
    "2.  Specify the following requirements for the workflow:\n",
    "    * It should trigger on any `push` to the `main` branch.\n",
    "    * It should define a single job named `build-and-test` that runs on `ubuntu-latest`.\n",
    "    * The job should have steps to: 1) Check out the code, 2) Set up a Python environment, 3) Install dependencies from `requirements.txt`, and 4) Run the test suite using `pytest`.\n",
    "3.  Save the generated YAML file to `.github/workflows/ci.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating GitHub Actions Workflow ---\n",
      "name: Python FastAPI CI\n",
      "\n",
      "# This workflow defines the Continuous Integration (CI) pipeline for the FastAPI application.\n",
      "# It runs on pushes and pull requests to the main branch, and can also be triggered manually.\n",
      "\n",
      "on:\n",
      "  # Trigger on push events to the main branch\n",
      "  push:\n",
      "    branches: [ \"main\" ]\n",
      "  # Trigger on pull request events targeting the main branch\n",
      "  pull_request:\n",
      "    branches: [ \"main\" ]\n",
      "  # Allow manual triggering of the workflow from the GitHub Actions UI\n",
      "  workflow_dispatch:\n",
      "\n",
      "# Concurrency configuration ensures that for a given branch, only the latest workflow run\n",
      "# will proceed, cancelling any older, in-progress runs. This saves runner minutes.\n",
      "concurrency:\n",
      "  group: ${{ github.workflow }}-${{ github.ref }}\n",
      "  cancel-in-progress: true\n",
      "\n",
      "jobs:\n",
      "  # The main job for building, linting, and testing the application.\n",
      "  build-and-test:\n",
      "    name: Build and Test Application\n",
      "    # Use the latest stable version of Ubuntu provided by GitHub.\n",
      "    runs-on: ubuntu-latest\n",
      "    # Set a timeout to prevent jobs from running indefinitely.\n",
      "    timeout-minutes: 10\n",
      "\n",
      "    steps:\n",
      "      # Step 1: Checkout the repository code\n",
      "      # This step checks out a copy of your repository on the runner.\n",
      "      - name: Checkout repository code\n",
      "        uses: actions/checkout@v4\n",
      "        with:\n",
      "          # Fetch all git history. This is useful for tools that perform git analysis.\n",
      "          fetch-depth: 0\n",
      "\n",
      "      # Step 2: Set up the Python environment\n",
      "      # This step installs a specific version of Python and sets up caching for dependencies.\n",
      "      - name: Set up Python 3.11\n",
      "        uses: actions/setup-python@v5\n",
      "        with:\n",
      "          python-version: '3.11'\n",
      "          # Enable caching for pip. This will cache downloaded packages to speed up\n",
      "          # subsequent workflow runs. The cache is invalidated when requirements.txt changes.\n",
      "          cache: 'pip'\n",
      "\n",
      "      # Step 3: Install Python dependencies\n",
      "      # This step installs the packages required by the application and for testing.\n",
      "      - name: Install Python dependencies\n",
      "        run: |\n",
      "          # Upgrade pip to its latest version for better performance and features.\n",
      "          python -m pip install --upgrade pip\n",
      "          # Install testing-specific dependencies. It's a good practice to install them\n",
      "          # explicitly here in case they are not in the main requirements file.\n",
      "          pip install pytest pytest-cov\n",
      "          # Install application dependencies from the requirements.txt file.\n",
      "          # The --no-cache-dir flag is used to disable the local pip cache, which is\n",
      "          # redundant with the runner-level cache configured in the previous step.\n",
      "          pip install --no-cache-dir -r requirements.txt\n",
      "\n",
      "      # Step 4: Run the test suite with pytest\n",
      "      # This step executes the test suite and generates a code coverage report.\n",
      "      - name: Run tests with pytest and generate coverage report\n",
      "        run: |\n",
      "          # Execute pytest on the 'artifacts/tests/' directory.\n",
      "          # -v: Enables verbose output for detailed test results.\n",
      "          # --cov: Specifies the source code directory to measure coverage against.\n",
      "          # --cov-report=xml: Generates a coverage report in XML format, which can be\n",
      "          # consumed by external tools like Codecov or SonarQube.\n",
      "          pytest artifacts/tests/ -v --cov=artifacts/app --cov-report=xml\n"
     ]
    }
   ],
   "source": [
    "# Detailed prompt to generate a GitHub Actions CI workflow file\n",
    "ci_workflow_prompt = \"\"\"You are a DevOps and CI/CD expert specializing in GitHub Actions. Generate a production-ready, optimized GitHub Actions workflow file for continuous integration of a FastAPI Python application.\n",
    "\n",
    "**Project Context:**\n",
    "- FastAPI application with SQLAlchemy database\n",
    "- Application code located in `artifacts/app/` directory\n",
    "- Test suite located in `artifacts/tests/` directory with pytest\n",
    "- Dependencies managed in `requirements.txt`\n",
    "- Tests use pytest with fixtures (conftest.py) and in-memory database\n",
    "- Python version: 3.11\n",
    "\n",
    "**Task Requirements:**\n",
    "\n",
    "1. **Workflow Triggers:**\n",
    "   - Trigger on `push` events to the `main` branch\n",
    "   - Also trigger on `pull_request` events targeting `main` (best practice for PR validation)\n",
    "   - Allow manual workflow dispatch for on-demand runs\n",
    "\n",
    "2. **Job Configuration:**\n",
    "   - Job name: `build-and-test`\n",
    "   - Runner: `ubuntu-latest` (latest stable Ubuntu environment)\n",
    "   - Use concurrency groups to cancel redundant runs on the same branch\n",
    "\n",
    "3. **Required Steps (in order):**\n",
    "   \n",
    "   **Step 1: Checkout Code**\n",
    "   - Use `actions/checkout@v4` (latest stable version)\n",
    "   - Fetch full git history for better context\n",
    "   \n",
    "   **Step 2: Set Up Python Environment**\n",
    "   - Use `actions/setup-python@v5` (latest stable version)\n",
    "   - Python version: `3.11`\n",
    "   - Include caching for pip dependencies to speed up builds\n",
    "   \n",
    "   **Step 3: Install Dependencies**\n",
    "   - Upgrade pip to latest version first\n",
    "   - Install dependencies from `requirements.txt`\n",
    "   - Use `--no-cache-dir` flag for consistency with Docker builds\n",
    "   \n",
    "   **Step 4: Run Test Suite**\n",
    "   - Execute pytest on the `artifacts/tests/` directory\n",
    "   - Use verbose output (`-v`) for detailed test results\n",
    "   - Generate coverage report (if applicable)\n",
    "   - Ensure proper exit codes for CI failure detection\n",
    "\n",
    "4. **GitHub Actions Best Practices:**\n",
    "   - Use semantic, descriptive step names\n",
    "   - Include comments explaining each major section\n",
    "   - Use proper YAML formatting and indentation (2 spaces)\n",
    "   - Set timeout for the job (e.g., 10 minutes) to prevent hanging builds\n",
    "   - Use environment variables where appropriate\n",
    "   - Include proper error handling (fail-fast behavior)\n",
    "\n",
    "5. **Performance Optimizations:**\n",
    "   - Cache pip dependencies using `actions/setup-python`'s built-in caching\n",
    "   - This significantly speeds up subsequent workflow runs\n",
    "   - Cache key should be based on `requirements.txt` content hash\n",
    "\n",
    "6. **Test Execution Details:**\n",
    "   - Tests are in `artifacts/tests/` directory\n",
    "   - Tests include: `test_main_simple.py`, `test_main_with_fixture.py`, `test_edge_cases.py`\n",
    "   - Tests use `conftest.py` for pytest fixtures\n",
    "   - Pytest command should target the test directory: `pytest artifacts/tests/ -v`\n",
    "\n",
    "**Expected Output Format:**\n",
    "Generate ONLY the ci.yml file contents. No explanations before or after, no markdown code blocks wrapper. Include helpful inline comments in the YAML file itself. The output should be ready to save directly to `.github/workflows/ci.yml`.\n",
    "\n",
    "**Example of desired format and structure:**\n",
    "```yaml\n",
    "name: CI Pipeline\n",
    "\n",
    "# Trigger workflow on push or pull request to main branch\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "  workflow_dispatch:  # Allow manual triggers\n",
    "\n",
    "# Cancel redundant runs for the same branch\n",
    "concurrency:\n",
    "  group: ${{ github.workflow }}-${{ github.ref }}\n",
    "  cancel-in-progress: true\n",
    "\n",
    "jobs:\n",
    "  build-and-test:\n",
    "    name: Build and Test Application\n",
    "    runs-on: ubuntu-latest\n",
    "    timeout-minutes: 10\n",
    "    \n",
    "    steps:\n",
    "      - name: Checkout code\n",
    "        uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Set up Python 3.11\n",
    "        uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "          cache: 'pip'  # Cache pip dependencies for faster builds\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install --no-cache-dir -r requirements.txt\n",
    "      \n",
    "      - name: Run test suite\n",
    "        run: |\n",
    "          pytest artifacts/tests/ -v\n",
    "```\n",
    "\n",
    "Generate the complete, production-ready GitHub Actions workflow file now:\"\"\"\n",
    "\n",
    "print(\"--- Generating GitHub Actions Workflow ---\")\n",
    "ci_workflow_content = get_completion(ci_workflow_prompt, client, model_name, api_provider)\n",
    "cleaned_yaml = clean_llm_output(ci_workflow_content, language='yaml')\n",
    "print(cleaned_yaml)\n",
    "\n",
    "if cleaned_yaml:\n",
    "    # Note: The save_artifact helper creates directories as needed.\n",
    "    save_artifact(cleaned_yaml, \".github/workflows/ci.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Excellent! You have now generated a complete, professional Continuous Integration pipeline using AI. You created the dependency list, the containerization configuration, and the automation workflow, all from simple prompts. This is a powerful demonstration of how AI can automate complex DevOps tasks, allowing teams to build and ship software with greater speed and confidence.\n",
    "\n",
    "> **Key Takeaway:** AI is a powerful tool for generating 'Configuration as Code' artifacts. By prompting for specific formats like `requirements.txt`, `Dockerfile`, or `ci.yml`, you can automate the creation of the files that define your entire build, test, and deployment processes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
