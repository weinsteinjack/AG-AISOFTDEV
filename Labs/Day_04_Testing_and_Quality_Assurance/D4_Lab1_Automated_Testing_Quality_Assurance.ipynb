{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 4! An application without tests is an application that is broken by design. Today, we focus on quality assurance. You will act as a QA Engineer, using an AI co-pilot to build a robust test suite for the API you created yesterday. This is a critical step to ensure our application is reliable and ready for production.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the source code for our main application from `app/main.py`. Providing the full code as context is essential for the LLM to generate accurate and relevant tests.\n",
    "\n",
    "**Model Selection:**\n",
    "For generating tests, models with strong code understanding and logical reasoning are best. `gpt-4.1`, `o3`, `codex-mini`, and `gemini-2.5-pro` are all excellent choices for this task.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated test files.\n",
    "- `clean_llm_output()`: To clean up the generated Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-30 10:44:59,476 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Task:** Generate basic `pytest` tests for the ideal or \"happy path\" scenarios of your CRUD endpoints.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to act as a QA Engineer.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the response body).\n",
    "4.  Generate another test for the `GET /users/` endpoint.\n",
    "5.  Save the generated tests into a file named `tests/test_main_simple.py`.\n",
    "\n",
    "**Expected Quality:** A Python script containing valid `pytest` functions that test the basic, successful operation of your API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "# test_users_happy_path.py\n",
      "\n",
      "\"\"\"\n",
      "Pytest test suite for the \"happy path\" scenarios of the User CRUD endpoints.\n",
      "\n",
      "This suite covers:\n",
      "- Successful creation of a user (POST /users/).\n",
      "- Successful retrieval of all users (GET /users/).\n",
      "- Successful retrieval of a specific user by ID (GET /users/{user_id}).\n",
      "\n",
      "Best Practices Followed:\n",
      "- **Test Database Isolation**: A separate SQLite database (`test_onboarding.db`) is used for testing\n",
      "  to ensure tests are independent and do not interfere with development data.\n",
      "- **Dependency Injection Override**: FastAPI's dependency injection system is used to override\n",
      "  the `get_db` dependency, pointing it to the test database during test execution.\n",
      "- **Fixtures for Setup/Teardown**: A pytest fixture (`client`) handles the creation and cleanup\n",
      "  of the database tables and the TestClient instance for each test function, ensuring a clean state.\n",
      "- **Arrange-Act-Assert Pattern**: Each test is clearly structured for readability and maintainability.\n",
      "- **Descriptive Naming and Docstrings**: Test functions and variables have clear names, and\n",
      "  docstrings explain the purpose of each test.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "from datetime import date, datetime\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "\n",
      "# Import the FastAPI app and other necessary components from the main application file\n",
      "from main import Base, UserRole, app, get_db\n",
      "\n",
      "# --- 1. Test Database Configuration ---\n",
      "\n",
      "# Define the URL for the test database.\n",
      "# Using a file-based SQLite DB for simplicity and speed.\n",
      "TEST_DATABASE_URL = \"sqlite:///./test_onboarding.db\"\n",
      "\n",
      "# Create the SQLAlchemy engine for the test database.\n",
      "# `connect_args` is necessary for SQLite to allow multi-threaded access.\n",
      "engine = create_engine(\n",
      "    TEST_DATABASE_URL, connect_args={\"check_same_thread\": False}\n",
      ")\n",
      "\n",
      "# Create a session maker for the test database. This will be used to create sessions.\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
      "\n",
      "\n",
      "# --- 2. Dependency Override ---\n",
      "\n",
      "def override_get_db():\n",
      "    \"\"\"\n",
      "    A dependency override for get_db that provides a session to the test database.\n",
      "    This function replaces the production `get_db` dependency during testing.\n",
      "    It ensures that all API calls within a test use an isolated database session.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        db = TestingSessionLocal()\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "# Apply the dependency override to the FastAPI app.\n",
      "# This tells the app to use `override_get_db` whenever `get_db` is requested.\n",
      "app.dependency_overrides[get_db] = override_get_db\n",
      "\n",
      "\n",
      "# --- 3. Pytest Fixture for Test Client and DB Management ---\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def client():\n",
      "    \"\"\"\n",
      "    Pytest fixture to set up and tear down the test environment for each test function.\n",
      "\n",
      "    Yields:\n",
      "        TestClient: An instance of FastAPI's TestClient configured for the app.\n",
      "    \"\"\"\n",
      "    # Arrange: Create all database tables before running a test.\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    \n",
      "    # Yield the test client to the test function.\n",
      "    with TestClient(app) as test_client:\n",
      "        yield test_client\n",
      "    \n",
      "    # Teardown: Drop all database tables after the test is complete.\n",
      "    # This ensures a clean state for the next test.\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "    # Optional: remove the test database file after all tests are done (can be done in a session-scoped fixture)\n",
      "    # if os.path.exists(\"./test_onboarding.db\"):\n",
      "    #     os.remove(\"./test_onboarding.db\")\n",
      "\n",
      "\n",
      "# --- 4. Test Functions for Happy Path Scenarios ---\n",
      "\n",
      "def test_create_user_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests the successful creation of a new user via the POST /users/ endpoint.\n",
      "\n",
      "    Scenario:\n",
      "    - A valid user payload is sent to the API.\n",
      "\n",
      "    Expected Outcome:\n",
      "    - The API returns a 201 Created status code.\n",
      "    - The response body contains the data of the newly created user.\n",
      "    - Auto-generated fields like `user_id` and `created_at` are present and valid.\n",
      "    \"\"\"\n",
      "    # Arrange: Define the valid payload for the new user.\n",
      "    new_user_data = {\n",
      "        \"first_name\": \"Jane\",\n",
      "        \"last_name\": \"Smith\",\n",
      "        \"email\": \"jane.smith@example.com\",\n",
      "        \"job_title\": \"Product Manager\",\n",
      "        \"department\": \"Product\",\n",
      "        \"start_date\": \"2024-02-01\",\n",
      "        \"role\": UserRole.MANAGER.value, # Use the enum value for correctness\n",
      "        \"mentor_id\": None,\n",
      "        \"onboarding_path_id\": None,\n",
      "    }\n",
      "\n",
      "    # Act: Make the POST request to the /users/ endpoint.\n",
      "    response = client.post(\"/users/\", json=new_user_data)\n",
      "\n",
      "    # Assert: Verify the response.\n",
      "    assert response.status_code == 201, f\"Expected status 201, but got {response.status_code}\"\n",
      "    \n",
      "    data = response.json()\n",
      "    \n",
      "    # Verify the structure and auto-generated fields\n",
      "    assert \"user_id\" in data\n",
      "    assert isinstance(data[\"user_id\"], int)\n",
      "    assert data[\"user_id\"] > 0\n",
      "    \n",
      "    assert \"created_at\" in data\n",
      "    # Attempt to parse the datetime string to ensure it's a valid format\n",
      "    try:\n",
      "        datetime.fromisoformat(data[\"created_at\"].replace('Z', '+00:00'))\n",
      "    except ValueError:\n",
      "        pytest.fail(\"`created_at` is not a valid ISO 8601 datetime string.\")\n",
      "\n",
      "    # Verify the returned data matches the input data\n",
      "    assert data[\"first_name\"] == new_user_data[\"first_name\"]\n",
      "    assert data[\"last_name\"] == new_user_data[\"last_name\"]\n",
      "    assert data[\"email\"] == new_user_data[\"email\"]\n",
      "    assert data[\"job_title\"] == new_user_data[\"job_title\"]\n",
      "    assert data[\"start_date\"] == new_user_data[\"start_date\"]\n",
      "    assert data[\"role\"] == new_user_data[\"role\"]\n",
      "\n",
      "\n",
      "def test_list_users_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests the successful retrieval of a list of users via the GET /users/ endpoint.\n",
      "\n",
      "    Scenario:\n",
      "    - Create two users to populate the database.\n",
      "    - Make a GET request to retrieve all users.\n",
      "\n",
      "    Expected Outcome:\n",
      "    - The API returns a 200 OK status code.\n",
      "    - The response body is a list containing the two created users.\n",
      "    - The structure of each user object in the list is correct.\n",
      "    \"\"\"\n",
      "    # Arrange: Create two users to ensure the database is not empty.\n",
      "    user1_data = {\n",
      "        \"first_name\": \"Alice\",\n",
      "        \"last_name\": \"Wonderland\",\n",
      "        \"email\": \"alice.w@example.com\",\n",
      "        \"start_date\": \"2023-10-01\",\n",
      "        \"role\": \"New Hire\",\n",
      "    }\n",
      "    user2_data = {\n",
      "        \"first_name\": \"Bob\",\n",
      "        \"last_name\": \"Builder\",\n",
      "        \"email\": \"bob.b@example.com\",\n",
      "        \"start_date\": \"2023-11-15\",\n",
      "        \"role\": \"Mentor\",\n",
      "    }\n",
      "    client.post(\"/users/\", json=user1_data)\n",
      "    client.post(\"/users/\", json=user2_data)\n",
      "\n",
      "    # Act: Make the GET request to the /users/ endpoint.\n",
      "    response = client.get(\"/users/\")\n",
      "\n",
      "    # Assert: Verify the response.\n",
      "    assert response.status_code == 200, f\"Expected status 200, but got {response.status_code}\"\n",
      "    \n",
      "    data = response.json()\n",
      "    \n",
      "    assert isinstance(data, list), \"Response should be a list of users.\"\n",
      "    assert len(data) == 2, \"Expected to retrieve 2 users.\"\n",
      "    \n",
      "    # Verify the content and structure of the returned users\n",
      "    assert data[0][\"email\"] == user1_data[\"email\"]\n",
      "    assert data[1][\"email\"] == user2_data[\"email\"]\n",
      "    assert \"user_id\" in data[0]\n",
      "    assert \"created_at\" in data[1]\n",
      "\n",
      "\n",
      "def test_get_specific_user_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Tests the successful retrieval of a single user by ID via GET /users/{user_id}.\n",
      "\n",
      "    Scenario:\n",
      "    - Create a new user.\n",
      "    - Use the ID from the creation response to fetch that specific user.\n",
      "\n",
      "    Expected Outcome:\n",
      "    - The API returns a 200 OK status code.\n",
      "    - The response body contains the correct details for the requested user.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a user to fetch.\n",
      "    user_to_create = {\n",
      "        \"first_name\": \"Charlie\",\n",
      "        \"last_name\": \"Chocolate\",\n",
      "        \"email\": \"charlie.c@example.com\",\n",
      "        \"job_title\": \"Confectioner\",\n",
      "        \"department\": \"Sweets\",\n",
      "        \"start_date\": \"2024-03-10\",\n",
      "        \"role\": UserRole.CONTENT_CREATOR.value,\n",
      "    }\n",
      "    create_response = client.post(\"/users/\", json=user_to_create)\n",
      "    assert create_response.status_code == 201\n",
      "    created_user_id = create_response.json()[\"user_id\"]\n",
      "\n",
      "    # Act: Make the GET request to fetch the newly created user by their ID.\n",
      "    response = client.get(f\"/users/{created_user_id}\")\n",
      "\n",
      "    # Assert: Verify the response.\n",
      "    assert response.status_code == 200, f\"Expected status 200, but got {response.status_code}\"\n",
      "    \n",
      "    data = response.json()\n",
      "    \n",
      "    # Verify that the retrieved user's data matches the data used for creation.\n",
      "    assert data[\"user_id\"] == created_user_id\n",
      "    assert data[\"email\"] == user_to_create[\"email\"]\n",
      "    assert data[\"first_name\"] == user_to_create[\"first_name\"]\n",
      "    assert data[\"last_name\"] == user_to_create[\"last_name\"]\n",
      "    assert data[\"job_title\"] == user_to_create[\"job_title\"]\n",
      "    assert data[\"department\"] == user_to_create[\"department\"]\n",
      "    assert data[\"start_date\"] == user_to_create[\"start_date\"]\n",
      "    assert data[\"role\"] == user_to_create[\"role\"]\n",
      "\n",
      "✅ Happy path tests saved to: artifacts/tests/test_main_simple.py\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive prompt for generating happy path tests\n",
    "happy_path_tests_prompt = f\"\"\"\n",
    "You are an expert QA Engineer specializing in API testing with pytest and FastAPI. Your task is to generate comprehensive, professional-grade pytest test functions for the \"happy path\" scenarios of a FastAPI application.\n",
    "\n",
    "## Context\n",
    "Below is the complete FastAPI application code for an Employee Onboarding System. This application provides CRUD operations for managing users with a SQLite database backend.\n",
    "\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "## Requirements\n",
    "Generate pytest test functions for the following happy path scenarios:\n",
    "\n",
    "1. **Test POST /users/** - Test successful user creation\n",
    "   - Create a new user with valid data\n",
    "   - Assert the response status code is 201 (Created)\n",
    "   - Verify the response body contains the correct user data\n",
    "   - Verify that auto-generated fields (user_id, created_at) are present\n",
    "   - Verify that the email matches what was sent\n",
    "\n",
    "2. **Test GET /users/** - Test retrieving all users\n",
    "   - Retrieve the list of users\n",
    "   - Assert the response status code is 200 (OK)\n",
    "   - Verify the response is a list\n",
    "   - If users exist, verify the structure of user objects in the response\n",
    "\n",
    "3. **Test GET /users/{{user_id}}** - Test retrieving a specific user\n",
    "   - First create a user, then retrieve it by ID\n",
    "   - Assert the response status code is 200 (OK)\n",
    "   - Verify all user fields match the created user\n",
    "\n",
    "## Technical Specifications\n",
    "- Use FastAPI's TestClient for making requests\n",
    "- Import all necessary modules (pytest, TestClient, datetime, etc.)\n",
    "- Use descriptive test function names following the pattern `test_<action>_<expected_outcome>`\n",
    "- Include docstrings for each test function explaining what it tests\n",
    "- Use proper assertions with clear, descriptive messages\n",
    "- For date fields, use valid date formats (YYYY-MM-DD string format)\n",
    "- Use valid enum values for the role field (e.g., \"New Hire\", \"HR Admin\", \"Manager\")\n",
    "- Include appropriate type checking where relevant\n",
    "\n",
    "## Code Quality Guidelines\n",
    "- Follow PEP 8 style guidelines\n",
    "- Use clear variable names\n",
    "- Add comments where logic might not be immediately obvious\n",
    "- Structure tests with the Arrange-Act-Assert pattern\n",
    "- Make tests independent and repeatable\n",
    "\n",
    "## Output Format\n",
    "Provide a complete, ready-to-run Python test file that includes:\n",
    "- All necessary imports\n",
    "- A TestClient instance configured for the FastAPI app\n",
    "- At least 3 test functions covering the scenarios listed above\n",
    "- Clean, professional code that could be used in a production environment\n",
    "\n",
    "Generate the complete test file now.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\")\n",
    "    print(\"\\n✅ Happy path tests saved to: artifacts/tests/test_main_simple.py\")\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Task:** Prompt the LLM to generate tests for common edge cases, such as providing invalid data or requesting a non-existent resource.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to write two new test functions:\n",
    "    * A test for the `POST /users/` endpoint that tries to create a user with an email that already exists, asserting that the API returns a `400 Bad Request` error.\n",
    "    * A test for the `GET /users/{user_id}` endpoint that requests a non-existent user ID, asserting that the API returns a `404 Not Found` error.\n",
    "\n",
    "**Expected Quality:** Two new `pytest` functions that verify the application handles common error scenarios correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Edge Case Tests ---\n",
      "\"\"\"\n",
      "Production-grade tests for the Employee Onboarding System API (/users endpoints).\n",
      "\n",
      "This test suite covers critical edge cases and error handling scenarios for the\n",
      "user creation (POST) and retrieval (GET) endpoints. It uses FastAPI's TestClient\n",
      "to simulate HTTP requests and pytest for structuring the tests.\n",
      "\n",
      "Key Principles Followed:\n",
      "- Arrange-Act-Assert Pattern: Each test clearly separates setup, execution, and verification.\n",
      "- Test Isolation: Tests are independent and do not rely on a specific execution order.\n",
      "- Descriptive Naming: Test function names clearly state the scenario being tested.\n",
      "- Parameterization: `pytest.mark.parametrize` is used to test multiple invalid inputs\n",
      "  efficiently, keeping the code DRY (Don't Repeat Yourself).\n",
      "- Informative Assertions: Assertions include custom messages to aid in debugging failures.\n",
      "\"\"\"\n",
      "\n",
      "import pytest\n",
      "from datetime import date\n",
      "from fastapi.testclient import TestClient\n",
      "\n",
      "# Assuming the FastAPI app instance is in a file named `main.py`\n",
      "# If your file is named differently, update the import accordingly.\n",
      "from main import app\n",
      "\n",
      "# Create a TestClient instance for making requests to the FastAPI application\n",
      "client = TestClient(app)\n",
      "\n",
      "# --- Test Data ---\n",
      "\n",
      "# A valid user payload to be used as a base for many tests\n",
      "VALID_USER_PAYLOAD = {\n",
      "    \"first_name\": \"Jane\",\n",
      "    \"last_name\": \"Doe\",\n",
      "    \"email\": \"jane.doe@example.com\",\n",
      "    \"job_title\": \"QA Engineer\",\n",
      "    \"department\": \"Engineering\",\n",
      "    \"start_date\": \"2024-05-10\",\n",
      "    \"role\": \"New Hire\",\n",
      "    \"mentor_id\": None,\n",
      "    \"onboarding_path_id\": None,\n",
      "}\n",
      "\n",
      "\n",
      "# --- Test Functions ---\n",
      "\n",
      "def test_create_user_duplicate_email_returns_409():\n",
      "    \"\"\"\n",
      "    Tests that creating a user with a duplicate email fails with a 409 Conflict status.\n",
      "\n",
      "    Why it's important:\n",
      "    The email field has a unique constraint in the database. The API must prevent the\n",
      "    creation of duplicate records to maintain data integrity.\n",
      "\n",
      "    Expected behavior:\n",
      "    The first request to create the user should succeed (201 Created). The second\n",
      "    request with the same email should fail (409 Conflict) with a clear error message.\n",
      "    \"\"\"\n",
      "    # Arrange: Define a unique email for this test to avoid conflicts with other tests\n",
      "    unique_email = \"duplicate.test@example.com\"\n",
      "    payload = VALID_USER_PAYLOAD.copy()\n",
      "    payload[\"email\"] = unique_email\n",
      "\n",
      "    # Act & Assert (First creation): Create the initial user\n",
      "    response_1 = client.post(\"/users/\", json=payload)\n",
      "    assert response_1.status_code == 201, f\"Expected 201, but got {response_1.status_code}: {response_1.json()}\"\n",
      "\n",
      "    # Act (Second creation): Attempt to create another user with the same email\n",
      "    response_2 = client.post(\"/users/\", json=payload)\n",
      "\n",
      "    # Assert (Second creation): Verify the conflict error\n",
      "    # NOTE: The requirement asked for a 400, but the application code correctly implements\n",
      "    # a 409 Conflict, which is the more appropriate status code for this error.\n",
      "    # We test against the actual implementation.\n",
      "    assert response_2.status_code == 409, f\"Expected 409, but got {response_2.status_code}: {response_2.json()}\"\n",
      "    \n",
      "    error_detail = response_2.json()[\"detail\"]\n",
      "    assert unique_email in error_detail\n",
      "    assert \"already exists\" in error_detail.lower()\n",
      "\n",
      "\n",
      "def test_get_nonexistent_user_returns_404():\n",
      "    \"\"\"\n",
      "    Tests that requesting a user with an ID that does not exist returns a 404 Not Found.\n",
      "\n",
      "    Why it's important:\n",
      "    The API should gracefully handle requests for resources that don't exist,\n",
      "    providing a clear and standard HTTP response.\n",
      "\n",
      "    Expected behavior:\n",
      "    A GET request to /users/{non_existent_id} should result in a 404 status code\n",
      "    and an informative error message.\n",
      "    \"\"\"\n",
      "    # Arrange: Choose an ID that is highly unlikely to exist\n",
      "    non_existent_user_id = 999999\n",
      "\n",
      "    # Act: Make a GET request for the non-existent user\n",
      "    response = client.get(f\"/users/{non_existent_user_id}\")\n",
      "\n",
      "    # Assert: Verify the 404 Not Found response\n",
      "    assert response.status_code == 404, f\"Expected 404, but got {response.status_code}: {response.json()}\"\n",
      "    \n",
      "    error_detail = response.json()[\"detail\"]\n",
      "    assert str(non_existent_user_id) in error_detail\n",
      "    assert \"not found\" in error_detail.lower()\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"invalid_email\", [\n",
      "    \"invalid-email\",          # Missing '@' symbol\n",
      "    \"user@\",                  # Missing domain part\n",
      "    \"@example.com\",           # Missing local part\n",
      "    \"user@domain\",            # Missing top-level domain\n",
      "    \"user@.com\",              # Invalid domain format\n",
      "    \"\",                       # Empty string\n",
      "])\n",
      "def test_create_user_invalid_email_returns_422(invalid_email):\n",
      "    \"\"\"\n",
      "    Tests user creation with various invalid email formats fails with 422 Unprocessable Entity.\n",
      "\n",
      "    Why it's important:\n",
      "    Ensures that Pydantic's `EmailStr` validation is working correctly to maintain\n",
      "    valid email data in the system.\n",
      "\n",
      "    Expected behavior:\n",
      "    Any attempt to create a user with a structurally invalid email address should be\n",
      "    rejected with a 422 status code.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a payload with the invalid email\n",
      "    payload = VALID_USER_PAYLOAD.copy()\n",
      "    payload[\"email\"] = invalid_email\n",
      "\n",
      "    # Act: Attempt to create the user\n",
      "    response = client.post(\"/users/\", json=payload)\n",
      "\n",
      "    # Assert: Verify the 422 Unprocessable Entity response\n",
      "    assert response.status_code == 422, f\"Expected 422 for email '{invalid_email}', but got {response.status_code}\"\n",
      "    \n",
      "    validation_errors = response.json()[\"detail\"]\n",
      "    assert any(err[\"loc\"] == [\"body\", \"email\"] for err in validation_errors), \\\n",
      "        \"The validation error should specify the 'email' field.\"\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"missing_field\", [\n",
      "    \"first_name\",\n",
      "    \"last_name\",\n",
      "    \"email\",\n",
      "    \"start_date\",\n",
      "    \"role\",\n",
      "])\n",
      "def test_create_user_missing_required_fields_returns_422(missing_field):\n",
      "    \"\"\"\n",
      "    Tests that creating a user without required fields fails with 422 Unprocessable Entity.\n",
      "\n",
      "    Why it's important:\n",
      "    Guarantees that essential user information is always provided, enforcing the\n",
      "    API contract defined by the Pydantic models.\n",
      "\n",
      "    Expected behavior:\n",
      "    A POST request with a payload missing a required field should be rejected with\n",
      "    a 422 status code and an error message indicating the missing field.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a payload and remove one of the required fields\n",
      "    payload = VALID_USER_PAYLOAD.copy()\n",
      "    del payload[missing_field]\n",
      "\n",
      "    # Act: Attempt to create the user\n",
      "    response = client.post(\"/users/\", json=payload)\n",
      "\n",
      "    # Assert: Verify the 422 response and error message\n",
      "    assert response.status_code == 422, f\"Expected 422 when missing '{missing_field}', but got {response.status_code}\"\n",
      "    \n",
      "    validation_errors = response.json()[\"detail\"]\n",
      "    assert any(err[\"loc\"] == [\"body\", missing_field] and err[\"type\"] == \"missing\" for err in validation_errors), \\\n",
      "        f\"The validation error should specify that the '{missing_field}' field is missing.\"\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"invalid_role\", [\n",
      "    \"InvalidRole\",\n",
      "    \"CEO\",\n",
      "    \"new hire\",  # Case-sensitive check\n",
      "    \"\",\n",
      "    None,\n",
      "])\n",
      "def test_create_user_invalid_role_enum_returns_422(invalid_role):\n",
      "    \"\"\"\n",
      "    Tests that creating a user with a role not in the UserRole enum fails with 422.\n",
      "\n",
      "    Why it's important:\n",
      "    Enforces that the 'role' field can only contain predefined, valid values,\n",
      "    preventing inconsistent or garbage data.\n",
      "\n",
      "    Expected behavior:\n",
      "    A POST request with an invalid 'role' value should be rejected with a 422\n",
      "    status code.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a payload with the invalid role\n",
      "    payload = VALID_USER_PAYLOAD.copy()\n",
      "    payload[\"role\"] = invalid_role\n",
      "\n",
      "    # Act: Attempt to create the user\n",
      "    response = client.post(\"/users/\", json=payload)\n",
      "\n",
      "    # Assert: Verify the 422 response\n",
      "    assert response.status_code == 422, f\"Expected 422 for role '{invalid_role}', but got {response.status_code}\"\n",
      "    \n",
      "    validation_errors = response.json()[\"detail\"]\n",
      "    assert any(err[\"loc\"] == [\"body\", \"role\"] for err in validation_errors), \\\n",
      "        \"The validation error should specify the 'role' field.\"\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"field, invalid_value, error_type\", [\n",
      "    (\"start_date\", \"not-a-date\", \"date_from_datetime_parsing\"),\n",
      "    (\"start_date\", 12345, \"date_from_datetime_parsing\"),\n",
      "    (\"first_name\", 123, \"string_type\"),\n",
      "    (\"last_name\", [\"list\", \"is\", \"not\", \"string\"], \"string_type\"),\n",
      "    (\"mentor_id\", \"not-an-integer\", \"int_parsing\"),\n",
      "])\n",
      "def test_create_user_invalid_data_type_returns_422(field, invalid_value, error_type):\n",
      "    \"\"\"\n",
      "    Tests that creating a user with incorrect data types for fields fails with 422.\n",
      "\n",
      "    Why it's important:\n",
      "    Ensures strong type validation at the API boundary, protecting the application\n",
      "    logic and database from malformed data.\n",
      "\n",
      "    Expected behavior:\n",
      "    A POST request where a field has the wrong data type should be rejected with a\n",
      "    422 status code and a type-specific error message.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a payload with the invalid data type\n",
      "    payload = VALID_USER_PAYLOAD.copy()\n",
      "    payload[field] = invalid_value\n",
      "\n",
      "    # Act: Attempt to create the user\n",
      "    response = client.post(\"/users/\", json=payload)\n",
      "\n",
      "    # Assert: Verify the 422 response and specific error type\n",
      "    assert response.status_code == 422, f\"Expected 422 for field '{field}' with value '{invalid_value}', but got {response.status_code}\"\n",
      "    \n",
      "    validation_errors = response.json()[\"detail\"]\n",
      "    assert any(err[\"loc\"] == [\"body\", field] and error_type in err[\"type\"] for err in validation_errors), \\\n",
      "        f\"The validation error for '{field}' should be of type '{error_type}'.\"\n",
      "\n",
      "\n",
      "@pytest.mark.parametrize(\"field, invalid_value, error_type\", [\n",
      "    (\"first_name\", \"\", \"string_too_short\"),\n",
      "    (\"last_name\", \"\", \"string_too_short\"),\n",
      "    (\"first_name\", \"a\" * 51, \"string_too_long\"),\n",
      "    (\"last_name\", \"b\" * 51, \"string_too_long\"),\n",
      "    (\"job_title\", \"c\" * 101, \"string_too_long\"),\n",
      "    (\"department\", \"d\" * 101, \"string_too_long\"),\n",
      "])\n",
      "def test_create_user_field_length_constraint_returns_422(field, invalid_value, error_type):\n",
      "    \"\"\"\n",
      "    Tests that creating a user with values violating length constraints fails with 422.\n",
      "\n",
      "    Why it's important:\n",
      "    Validates that min/max length constraints on string fields are enforced,\n",
      "    aligning with database schema limits and business rules.\n",
      "\n",
      "    Expected behavior:\n",
      "    A POST request with a string value that is too short or too long should be\n",
      "    rejected with a 422 status code.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a payload with a value that violates length constraints\n",
      "    payload = VALID_USER_PAYLOAD.copy()\n",
      "    payload[field] = invalid_value\n",
      "\n",
      "    # Act: Attempt to create the user\n",
      "    response = client.post(\"/users/\", json=payload)\n",
      "\n",
      "    # Assert: Verify the 422 response and specific error type\n",
      "    assert response.status_code == 422, f\"Expected 422 for field '{field}' with length {len(invalid_value)}, but got {response.status_code}\"\n",
      "    \n",
      "    validation_errors = response.json()[\"detail\"]\n",
      "    assert any(err[\"loc\"] == [\"body\", field] and err[\"type\"] == error_type for err in validation_errors), \\\n",
      "        f\"The validation error for '{field}' should be of type '{error_type}'.\"\n",
      "\n",
      "✅ Edge case tests saved to: artifacts/tests/test_edge_cases.py\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive prompt for generating edge case tests\n",
    "edge_case_tests_prompt = f\"\"\"\n",
    "You are an expert QA Engineer specializing in API testing, edge case analysis, and pytest with FastAPI. \n",
    "Your task is to generate production-grade pytest test functions that cover critical edge cases and error scenarios.\n",
    "\n",
    "## Context\n",
    "Below is the complete FastAPI application code for an Employee Onboarding System. Study it carefully to understand \n",
    "the validation rules, constraints, and error handling mechanisms.\n",
    "\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "## Edge Case Testing Requirements\n",
    "\n",
    "Generate pytest test functions for the following edge case scenarios:\n",
    "\n",
    "### 1. Duplicate Email (400 Bad Request)\n",
    "**Scenario:** Attempt to create a user with an email that already exists in the database\n",
    "- First, create a user with email \"john.doe@example.com\"\n",
    "- Then, attempt to create another user with the same email\n",
    "- **Expected:** The response status code should be 400 (Bad Request)\n",
    "- **Verify:** The error message clearly indicates the email already exists or violates a uniqueness constraint\n",
    "- **Test Name:** `test_create_user_duplicate_email_returns_400`\n",
    "\n",
    "### 2. Non-existent User Retrieval (404 Not Found)\n",
    "**Scenario:** Request a user by ID that does not exist in the database\n",
    "- Attempt to GET a user with an ID that has never been created (e.g., ID 9999)\n",
    "- **Expected:** The response status code should be 404 (Not Found)\n",
    "- **Verify:** The response contains an appropriate error message\n",
    "- **Test Name:** `test_get_nonexistent_user_returns_404`\n",
    "\n",
    "### 3. Invalid Email Format (422 Unprocessable Entity)\n",
    "**Scenario:** Attempt to create a user with an invalid email format\n",
    "- Try to POST with email values like \"invalid-email\", \"user@\", \"@example.com\", or empty string\n",
    "- **Expected:** The response status code should be 422 (Unprocessable Entity)\n",
    "- **Verify:** Pydantic validation errors are properly returned\n",
    "- **Test Name:** `test_create_user_invalid_email_returns_422`\n",
    "\n",
    "### 4. Missing Required Fields (422 Unprocessable Entity)\n",
    "**Scenario:** Attempt to create a user without providing required fields\n",
    "- Try to POST without first_name, last_name, email, start_date, or role\n",
    "- **Expected:** The response status code should be 422 (Unprocessable Entity)\n",
    "- **Verify:** The response indicates which fields are missing\n",
    "- **Test Name:** `test_create_user_missing_required_fields_returns_422`\n",
    "\n",
    "### 5. Invalid Enum Value for Role (422 Unprocessable Entity)\n",
    "**Scenario:** Attempt to create a user with an invalid role value\n",
    "- Try to POST with role=\"InvalidRole\" or role=\"CEO\" (not in UserRole enum)\n",
    "- The valid roles from UserRole enum are: \"New Hire\", \"HR Admin\", \"Content Creator\", \"Manager\", \"Mentor\"\n",
    "- **Expected:** The response status code should be 422 (Unprocessable Entity)\n",
    "- **Verify:** Pydantic validation detects the invalid enum value\n",
    "- **Test Name:** `test_create_user_invalid_role_enum_returns_422`\n",
    "\n",
    "### 6. Invalid Data Type (422 Unprocessable Entity)\n",
    "**Scenario:** Attempt to create a user with incorrect data types\n",
    "- Try to POST with start_date as a number instead of a date string (e.g., 12345 instead of \"2024-01-15\")\n",
    "- Try to POST with first_name as an integer instead of a string\n",
    "- **Expected:** The response status code should be 422 (Unprocessable Entity)\n",
    "- **Verify:** Type validation errors are returned\n",
    "- **Test Name:** `test_create_user_invalid_data_type_returns_422`\n",
    "\n",
    "### 7. Field Length Constraints (422 Unprocessable Entity)\n",
    "**Scenario:** Attempt to create a user with field values that violate length constraints\n",
    "- first_name and last_name have max_length=50, min_length=1\n",
    "- job_title and department have max_length=100\n",
    "- Try to create a user with first_name=\"\" (empty string, min_length violation)\n",
    "- Try to create a user with first_name of 100 characters (max_length violation)\n",
    "- **Expected:** The response status code should be 422 (Unprocessable Entity)\n",
    "- **Test Name:** `test_create_user_field_length_constraint_returns_422`\n",
    "\n",
    "## Technical Requirements\n",
    "\n",
    "- Use FastAPI's TestClient for all HTTP requests\n",
    "- Import all necessary modules at the top of the file (pytest, TestClient, datetime, etc.)\n",
    "- Follow this naming convention: `test_<scenario>_<expected_result>`\n",
    "- Each test function must have a clear docstring explaining:\n",
    "  * What edge case is being tested\n",
    "  * Why it's important\n",
    "  * What the expected behavior is\n",
    "- Use the Arrange-Act-Assert pattern:\n",
    "  * **Arrange:** Set up the necessary data and state\n",
    "  * **Act:** Make the API request\n",
    "  * **Assert:** Verify the response status code and body\n",
    "- Add meaningful assertion messages (e.g., `assert response.status_code == 400, f\"Expected 400, got response.status_code: response.json()\"`)\n",
    "- For tests that need existing data (like duplicate email), create the required objects first\n",
    "- Use appropriate HTTP methods and endpoints:\n",
    "  * `POST /users/` for user creation\n",
    "  * `GET /users/user_id` for retrieving specific users\n",
    "\n",
    "## Output Format\n",
    "\n",
    "Provide a complete, ready-to-run Python test file that includes:\n",
    "- All necessary imports (pytest, TestClient, datetime, etc.)\n",
    "- A properly configured TestClient instance\n",
    "- At least 7 test functions covering all edge cases listed above\n",
    "- Professional, production-ready code with proper error messages\n",
    "- Tests that are independent and can run in any order\n",
    "- Clear comments explaining the purpose of each test\n",
    "\n",
    "## Code Quality Standards\n",
    "\n",
    "- Follow PEP 8 style guidelines\n",
    "- Use descriptive variable names\n",
    "- Include comments where the testing logic is non-obvious\n",
    "- Ensure all tests are isolated and don't depend on execution order\n",
    "- Make assertions specific and informative\n",
    "- Use f-strings for string formatting\n",
    "\n",
    "Generate the complete, professional test file now.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "    save_artifact(cleaned_edge_case_tests, \"tests/test_edge_cases.py\")\n",
    "    print(\"\\n✅ Edge case tests saved to: artifacts/tests/test_edge_cases.py\")\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Task:** Generate a `pytest` fixture that creates a fresh, isolated, in-memory database for each test session. Then, refactor your tests to use this fixture. This is a critical pattern for professional-grade testing.\n",
    "\n",
    "> **Hint:** Why use an isolated database? Running tests against your actual development database can lead to data corruption and flaky, unreliable tests. A pytest fixture that creates a fresh, in-memory database for each test ensures that your tests are independent, repeatable, and have no side effects.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to generate a `pytest` fixture.\n",
    "2.  This fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
    "3.  It needs to create all the database tables before the test runs and tear them down afterward.\n",
    "4.  Crucially, it must override the `get_db` dependency in your FastAPI app to use this temporary database during tests.\n",
    "5.  Save the generated fixture code to a special file named `tests/conftest.py`.\n",
    "6.  Finally, create a new test file, `tests/test_main_with_fixture.py`, and ask the LLM to rewrite the happy-path tests from Challenge 1 to use the new database fixture.\n",
    "\n",
    "**Expected Quality:** Two new files, `tests/conftest.py` and `tests/test_main_with_fixture.py`, containing a professional `pytest` fixture for database isolation and tests that are correctly refactored to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Pytest DB Fixture ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# artifacts/tests/conftest.py\n",
      "\n",
      "\"\"\"\n",
      "Pytest configuration file for the Employee Onboarding System.\n",
      "\n",
      "This file provides reusable fixtures for setting up an isolated test environment,\n",
      "including a temporary in-memory database and a FastAPI TestClient.\n",
      "\n",
      "Fixtures Provided:\n",
      "- `db_session`: Creates and tears down an isolated in-memory SQLite database for\n",
      "                each test function, ensuring complete test isolation.\n",
      "- `client`: Provides a FastAPI `TestClient` instance that is configured to use\n",
      "            the isolated test database provided by the `db_session` fixture.\n",
      "\"\"\"\n",
      "\n",
      "# 1. Path Setup Section (MANDATORY)\n",
      "# This section ensures that the test runner can find and import the main application module.\n",
      "# -----------------------------------------------------------------------------------------\n",
      "import os\n",
      "import sys\n",
      "\n",
      "# Add the 'app' directory to the Python path to enable imports from 'main'\n",
      "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
      "app_dir = os.path.join(os.path.dirname(current_dir), 'app')\n",
      "if app_dir not in sys.path:\n",
      "    sys.path.insert(0, app_dir)\n",
      "\n",
      "# 2. Standard and Third-Party Imports\n",
      "# -----------------------------------\n",
      "import pytest\n",
      "from typing import Generator\n",
      "from fastapi.testclient import TestClient\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker, Session\n",
      "\n",
      "# 3. Application Imports\n",
      "# These imports are now possible due to the sys.path modification above.\n",
      "# We import the FastAPI app instance, the base for SQLAlchemy models, and the\n",
      "# production database dependency function which we will override.\n",
      "# ---------------------------------------------------------------------------\n",
      "from main import app, get_db, Base\n",
      "\n",
      "\n",
      "# 4. Test Database Configuration\n",
      "# ------------------------------\n",
      "# Use an in-memory SQLite database for fast, isolated tests.\n",
      "TEST_DATABASE_URL = \"sqlite:///:memory:\"\n",
      "\n",
      "# Create a new SQLAlchemy engine for the test database.\n",
      "# `connect_args` is required for SQLite to allow it to be used across different threads,\n",
      "# which is a scenario that can occur during testing.\n",
      "test_engine = create_engine(\n",
      "    TEST_DATABASE_URL, connect_args={\"check_same_thread\": False}\n",
      ")\n",
      "\n",
      "# Create a sessionmaker factory that will be used to create sessions for the test database.\n",
      "TestingSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=test_engine)\n",
      "\n",
      "\n",
      "# 5. Database Fixture Section\n",
      "# ---------------------------\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def db_session() -> Generator[Session, None, None]:\n",
      "    \"\"\"\n",
      "    Pytest fixture to provide an isolated database session for each test function.\n",
      "\n",
      "    This fixture handles the complete lifecycle of a test database transaction:\n",
      "    1.  Creates all database tables defined in the SQLAlchemy models before a test runs.\n",
      "    2.  Yields a new database session for the test to use.\n",
      "    3.  Closes the session after the test is complete.\n",
      "    4.  Drops all database tables after the test, ensuring no state is carried over\n",
      "        to subsequent tests. This guarantees 100% test isolation.\n",
      "    \"\"\"\n",
      "    # Create tables in the test database\n",
      "    Base.metadata.create_all(bind=test_engine)\n",
      "    db = TestingSessionLocal()\n",
      "    try:\n",
      "        # Yield the session to the test function\n",
      "        yield db\n",
      "    finally:\n",
      "        # Ensure the session is closed after the test\n",
      "        db.close()\n",
      "        # Drop all tables to clean up the database for the next test\n",
      "        Base.metadata.drop_all(bind=test_engine)\n",
      "\n",
      "\n",
      "# 6. Client Fixture Section\n",
      "# -------------------------\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def client(db_session: Session) -> Generator[TestClient, None, None]:\n",
      "    \"\"\"\n",
      "    Pytest fixture to provide a FastAPI TestClient that uses the isolated test database.\n",
      "\n",
      "    This fixture performs the following steps:\n",
      "    1.  Depends on the `db_session` fixture to get an isolated database session.\n",
      "    2.  Defines a dependency override for `get_db` to ensure the app uses the\n",
      "        test session instead of the production database connection.\n",
      "    3.  Applies this override to the FastAPI application instance.\n",
      "    4.  Yields a `TestClient` configured to communicate with the in-memory app.\n",
      "    5.  Crucially, it cleans up the dependency override after the test is finished,\n",
      "        restoring the application to its original state.\n",
      "    \"\"\"\n",
      "    def override_get_db() -> Generator[Session, None, None]:\n",
      "        \"\"\"Dependency override that yields the test database session.\"\"\"\n",
      "        try:\n",
      "            yield db_session\n",
      "        finally:\n",
      "            # The session lifecycle is managed by the `db_session` fixture,\n",
      "            # so we don't need to close it here.\n",
      "            pass\n",
      "\n",
      "    # Apply the dependency override to the app\n",
      "    app.dependency_overrides[get_db] = override_get_db\n",
      "\n",
      "    try:\n",
      "        # Use a `with` statement for the TestClient to handle startup/shutdown events\n",
      "        with TestClient(app) as test_client:\n",
      "            yield test_client\n",
      "    finally:\n",
      "        # Clean up the dependency override to prevent test pollution\n",
      "        app.dependency_overrides.clear()\n",
      "Database fixture saved to: artifacts/tests/conftest.py\n",
      "\n",
      "--- Generating Refactored Tests ---\n",
      "# artifacts/tests/test_main_with_fixture.py\n",
      "\n",
      "\"\"\"\n",
      "Pytest integration tests for the FastAPI Employee Onboarding System.\n",
      "\n",
      "This test suite validates the CRUD functionality of the /users/ endpoint.\n",
      "It is specifically designed to use pytest fixtures defined in a conftest.py\n",
      "file, ensuring proper database isolation for each test function.\n",
      "\n",
      "Key Features of this Test Suite:\n",
      "- Utilizes the `client` fixture for making API requests to a test instance\n",
      "  of the FastAPI application.\n",
      "- Relies on fixtures for automatic setup and teardown of a clean, in-memory\n",
      "  test database for each test, guaranteeing test isolation.\n",
      "- Follows the Arrange-Act-Assert pattern for clear and maintainable tests.\n",
      "- Includes comprehensive docstrings and assertion messages for clarity.\n",
      "- Adheres to pytest best practices for testing FastAPI applications.\n",
      "\"\"\"\n",
      "\n",
      "# --- 1. Path Setup Section (MANDATORY) ---\n",
      "# This block ensures that the main application code in the 'app' directory\n",
      "# can be imported correctly by the test suite located in the 'tests' directory.\n",
      "# This setup is crucial for the test runner to find the application modules.\n",
      "import os\n",
      "import sys\n",
      "\n",
      "# Add the app directory to Python path to enable imports\n",
      "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
      "app_dir = os.path.join(os.path.dirname(current_dir), 'app')\n",
      "if app_dir not in sys.path:\n",
      "    sys.path.insert(0, app_dir)\n",
      "\n",
      "# --- 2. Imports Section ---\n",
      "from datetime import date\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "\n",
      "# Imports from the application code (main.py)\n",
      "# Note: We only import necessary models/enums, not the `app` or `get_db`\n",
      "# objects, as those are managed by the test fixtures.\n",
      "from main import UserRole\n",
      "\n",
      "\n",
      "# --- 3. Test Functions Section ---\n",
      "\n",
      "def test_create_user_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Test successful user creation via POST /users/.\n",
      "\n",
      "    - Arrange: Define valid user data payload.\n",
      "    - Act: Send a POST request to the /users/ endpoint.\n",
      "    - Assert:\n",
      "        - The HTTP status code is 201 (Created).\n",
      "        - The response body contains the user data sent in the request.\n",
      "        - The response body includes auto-generated fields 'user_id' and 'created_at'.\n",
      "    \"\"\"\n",
      "    # Arrange: Define valid data for a new user\n",
      "    user_data = {\n",
      "        \"first_name\": \"Jane\",\n",
      "        \"last_name\": \"Smith\",\n",
      "        \"email\": \"jane.smith@example.com\",\n",
      "        \"job_title\": \"Product Manager\",\n",
      "        \"department\": \"Product\",\n",
      "        \"start_date\": \"2024-03-01\",\n",
      "        \"role\": UserRole.MANAGER.value, # Use enum value for correctness\n",
      "        \"mentor_id\": None,\n",
      "        \"onboarding_path_id\": None,\n",
      "    }\n",
      "\n",
      "    # Act: Send the POST request to the API\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "\n",
      "    # Assert: Verify the response\n",
      "    assert response.status_code == 201, f\"Expected status 201, but got {response.status_code}\"\n",
      "    response_data = response.json()\n",
      "\n",
      "    # Verify auto-generated fields are present\n",
      "    assert \"user_id\" in response_data, \"Response should contain 'user_id'\"\n",
      "    assert \"created_at\" in response_data, \"Response should contain 'created_at'\"\n",
      "    assert isinstance(response_data[\"user_id\"], int), \"'user_id' should be an integer\"\n",
      "\n",
      "    # Verify the returned data matches the input data\n",
      "    for key, value in user_data.items():\n",
      "        assert response_data[key] == value, f\"Field '{key}' did not match\"\n",
      "\n",
      "\n",
      "def test_list_users_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Test retrieving a list of users via GET /users/.\n",
      "\n",
      "    - Arrange: Create a new user to ensure the database is not empty.\n",
      "    - Act: Send a GET request to the /users/ endpoint.\n",
      "    - Assert:\n",
      "        - The HTTP status code is 200 (OK).\n",
      "        - The response body is a list.\n",
      "        - The created user is present in the returned list.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a user so the list is not empty\n",
      "    user_to_create = {\n",
      "        \"first_name\": \"Alex\",\n",
      "        \"last_name\": \"Ray\",\n",
      "        \"email\": \"alex.ray@example.com\",\n",
      "        \"job_title\": \"Data Scientist\",\n",
      "        \"department\": \"Analytics\",\n",
      "        \"start_date\": \"2024-05-10\",\n",
      "        \"role\": UserRole.NEW_HIRE.value,\n",
      "    }\n",
      "    create_response = client.post(\"/users/\", json=user_to_create)\n",
      "    assert create_response.status_code == 201, \"Setup failed: Could not create user\"\n",
      "\n",
      "    # Act: Retrieve the list of all users\n",
      "    response = client.get(\"/users/\")\n",
      "\n",
      "    # Assert: Verify the response\n",
      "    assert response.status_code == 200, f\"Expected status 200, but got {response.status_code}\"\n",
      "    users_list = response.json()\n",
      "    assert isinstance(users_list, list), \"The response should be a list of users\"\n",
      "    assert len(users_list) > 0, \"The users list should not be empty\"\n",
      "\n",
      "    # Check if the created user is in the list\n",
      "    emails_in_response = [user[\"email\"] for user in users_list]\n",
      "    assert user_to_create[\"email\"] in emails_in_response, \\\n",
      "        \"The newly created user was not found in the list\"\n",
      "\n",
      "\n",
      "def test_get_specific_user_success(client: TestClient):\n",
      "    \"\"\"\n",
      "    Test retrieving a single, specific user by ID via GET /users/{user_id}.\n",
      "\n",
      "    - Arrange: Create a new user and extract their 'user_id' from the response.\n",
      "    - Act: Send a GET request to /users/{user_id} using the extracted ID.\n",
      "    - Assert:\n",
      "        - The HTTP status code is 200 (OK).\n",
      "        - The response body contains the correct data for the requested user.\n",
      "        - The 'user_id' in the response matches the requested ID.\n",
      "    \"\"\"\n",
      "    # Arrange: Create a user and get their ID\n",
      "    user_to_create = {\n",
      "        \"first_name\": \"Sam\",\n",
      "        \"last_name\": \"Jones\",\n",
      "        \"email\": \"sam.jones@example.com\",\n",
      "        \"job_title\": \"DevOps Engineer\",\n",
      "        \"department\": \"Engineering\",\n",
      "        \"start_date\": str(date(2024, 6, 20)), # Using datetime for robustness\n",
      "        \"role\": UserRole.MENTOR.value,\n",
      "    }\n",
      "    create_response = client.post(\"/users/\", json=user_to_create)\n",
      "    assert create_response.status_code == 201, \"Setup failed: Could not create user\"\n",
      "    created_user_data = create_response.json()\n",
      "    user_id = created_user_data[\"user_id\"]\n",
      "\n",
      "    # Act: Request the specific user by their ID\n",
      "    response = client.get(f\"/users/{user_id}\")\n",
      "\n",
      "    # Assert: Verify the response\n",
      "    assert response.status_code == 200, f\"Expected status 200, but got {response.status_code}\"\n",
      "    retrieved_user = response.json()\n",
      "\n",
      "    # Verify that the retrieved user's ID and email match the created user\n",
      "    assert retrieved_user[\"user_id\"] == user_id, \"User ID in response does not match requested ID\"\n",
      "    assert retrieved_user[\"email\"] == user_to_create[\"email\"], \"User email does not match\"\n",
      "    assert retrieved_user[\"first_name\"] == user_to_create[\"first_name\"], \"User first name does not match\"\n",
      "    assert retrieved_user[\"start_date\"] == user_to_create[\"start_date\"], \"User start date does not match\"\n",
      "\n",
      "✅ Refactored tests saved to: artifacts/tests/test_main_with_fixture.py\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive prompt to generate pytest fixture for isolated test database\n",
    "db_fixture_prompt = \"\"\"\n",
    "You are an expert Python QA Engineer specializing in pytest, FastAPI testing, and test database isolation patterns. Your task is to generate a production-grade `conftest.py` file that provides reusable pytest fixtures for database isolation.\n",
    "\n",
    "## Context\n",
    "Below is the complete FastAPI application code for an Employee Onboarding System that uses SQLAlchemy ORM with a SQLite database:\n",
    "\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "## Critical Import Path Issue\n",
    "**IMPORTANT:** The test files are located in `artifacts/tests/` directory, while the main application code is in `artifacts/app/` directory. This creates an import path problem that MUST be solved.\n",
    "\n",
    "You MUST include the following path setup code at the very beginning of conftest.py (before any other imports from main):\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the app directory to Python path to enable imports\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "app_dir = os.path.join(os.path.dirname(current_dir), 'app')\n",
    "if app_dir not in sys.path:\n",
    "    sys.path.insert(0, app_dir)\n",
    "```\n",
    "\n",
    "This path setup is NON-NEGOTIABLE and must be included to avoid ModuleNotFoundError.\n",
    "\n",
    "## Requirements for conftest.py\n",
    "\n",
    "### 1. Database Isolation Fixture\n",
    "Create a pytest fixture named `db_session` that:\n",
    "- Uses a **temporary, in-memory SQLite database** (`sqlite:///:memory:`) for maximum test isolation\n",
    "- Creates a new SQLAlchemy engine for the test database\n",
    "- Creates a SessionLocal factory using `sessionmaker`\n",
    "- Creates all database tables using `Base.metadata.create_all(bind=engine)` before tests run\n",
    "- Provides a database session that yields to the test\n",
    "- Properly closes the session after each test\n",
    "- Drops all tables using `Base.metadata.drop_all(bind=engine)` for cleanup\n",
    "- Uses pytest fixture decorators appropriately\n",
    "\n",
    "### 2. FastAPI TestClient Fixture  \n",
    "Create a pytest fixture named `client` that:\n",
    "- Depends on the `db_session` fixture (uses `db_session` parameter)\n",
    "- Overrides the FastAPI app's `get_db` dependency to use the test database session\n",
    "- Creates and returns a `TestClient` instance\n",
    "- Properly handles dependency override cleanup in a try-finally block\n",
    "- Ensures the test database is used instead of the production database\n",
    "\n",
    "### 3. Session Scope Considerations\n",
    "- The `db_session` fixture should have `scope=\"function\"` to ensure each test gets a fresh database\n",
    "- The `client` fixture should also have `scope=\"function\"` to work with the db_session\n",
    "\n",
    "## Technical Specifications\n",
    "\n",
    "**Must Include:**\n",
    "- Path setup code at the top (as specified above)\n",
    "- Proper imports: `pytest`, `TestClient` from `fastapi.testclient`, SQLAlchemy components\n",
    "- Import from main: `app`, `get_db`, `Base` (and create a new test engine, don't import production engine)\n",
    "- Clear docstrings explaining the purpose of each fixture\n",
    "- Proper exception handling and cleanup\n",
    "- Type hints where appropriate\n",
    "\n",
    "**Database Configuration:**\n",
    "- Use in-memory SQLite: `sqlite:///:memory:`\n",
    "- Configure with `connect_args={{\"check_same_thread\": False}}` for SQLite\n",
    "- Ensure complete isolation between test runs\n",
    "\n",
    "**Dependency Override Pattern:**\n",
    "- Use `app.dependency_overrides[get_db] = override_get_db` to inject test database\n",
    "- Create an `override_get_db` function that yields the test session\n",
    "- Clean up overrides after tests: `app.dependency_overrides.clear()`\n",
    "\n",
    "## Code Quality Standards\n",
    "- Follow PEP 8 style guidelines\n",
    "- Use descriptive variable names\n",
    "- Include comprehensive docstrings for each fixture\n",
    "- Add inline comments explaining critical setup/teardown steps\n",
    "- Structure with clear sections (imports, fixtures, helper functions)\n",
    "- Make fixtures reusable and independent\n",
    "\n",
    "## Expected Output Structure\n",
    "\n",
    "Your conftest.py should have this structure:\n",
    "\n",
    "1. **Path Setup Section** (MANDATORY - include first)\n",
    "   - sys.path manipulation to add app directory\n",
    "\n",
    "2. **Standard Imports Section**\n",
    "   - pytest, FastAPI TestClient, SQLAlchemy components\n",
    "\n",
    "3. **Application Imports Section**  \n",
    "   - Import from main module: app, get_db, Base, etc.\n",
    "\n",
    "4. **Database Fixture Section**\n",
    "   - `db_session` fixture with full setup/teardown\n",
    "\n",
    "5. **Client Fixture Section**\n",
    "   - `client` fixture that uses db_session and overrides dependencies\n",
    "\n",
    "## Example Pattern (for reference)\n",
    "\n",
    "Your fixture should follow this pattern:\n",
    "\n",
    "```python\n",
    "@pytest.fixture(scope=\"function\")\n",
    "def db_session():\n",
    "    Fixture that provides an isolated test database session.\n",
    "    # Create test engine\n",
    "    # Create all tables\n",
    "    # Create session\n",
    "    # Yield session to test\n",
    "    # Cleanup: close session and drop tables\n",
    "    \n",
    "@pytest.fixture(scope=\"function\")\n",
    "def client(db_session):\n",
    "    Fixture that provides a TestClient with isolated test database.\n",
    "    # Override get_db dependency\n",
    "    # Create TestClient\n",
    "    # Yield client to test\n",
    "    # Cleanup: clear dependency overrides\n",
    "```\n",
    "\n",
    "## Output Format\n",
    "Provide a complete, ready-to-run `conftest.py` file that:\n",
    "- Includes the mandatory path setup code at the top\n",
    "- Provides both `db_session` and `client` fixtures\n",
    "- Has comprehensive docstrings and comments\n",
    "- Follows all technical specifications above\n",
    "- Is production-ready and can be used immediately\n",
    "\n",
    "Generate the complete conftest.py file now.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt.format(app_code=app_code), client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\")\n",
    "    print(\"Database fixture saved to: artifacts/tests/conftest.py\")\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "# Comprehensive prompt to refactor happy path tests to use the new fixture\n",
    "refactor_tests_prompt = \"\"\"\n",
    "You are an expert Python QA Engineer specializing in pytest best practices, FastAPI testing, and test refactoring. Your task is to generate refactored test functions that use the pytest fixtures defined in conftest.py for proper database isolation.\n",
    "\n",
    "## Context\n",
    "\n",
    "### Original Application Code\n",
    "Below is the complete FastAPI application code for an Employee Onboarding System:\n",
    "\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "### Available Pytest Fixtures\n",
    "The `conftest.py` file provides two fixtures that MUST be used:\n",
    "\n",
    "1. **`db_session`** - Provides an isolated in-memory test database session\n",
    "2. **`client`** - Provides a TestClient configured to use the test database\n",
    "\n",
    "These fixtures handle:\n",
    "- Database setup and teardown automatically\n",
    "- Dependency injection override (get_db)\n",
    "- Complete test isolation between test runs\n",
    "\n",
    "## Critical Import Path Issue\n",
    "**IMPORTANT:** The test files are located in `artifacts/tests/` directory, while the main application code is in `artifacts/app/` directory.\n",
    "\n",
    "You MUST include the following path setup code at the very beginning of the test file (before any imports from main):\n",
    "\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the app directory to Python path to enable imports\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "app_dir = os.path.join(os.path.dirname(current_dir), 'app')\n",
    "if app_dir not in sys.path:\n",
    "    sys.path.insert(0, app_dir)\n",
    "```\n",
    "\n",
    "This path setup is NON-NEGOTIABLE and must be included to avoid ModuleNotFoundError.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Generate refactored pytest test functions for the following happy path scenarios:\n",
    "\n",
    "### 1. Test POST /users/ - Successful User Creation\n",
    "**Test Name:** `test_create_user_success`\n",
    "- Use the `client` fixture as a parameter (pytest will inject it automatically)\n",
    "- Create a new user with valid data\n",
    "- Assert the response status code is 201 (Created) or 200 (OK)\n",
    "- Verify the response body contains the correct user data\n",
    "- Verify auto-generated fields (user_id, created_at) are present\n",
    "- Use valid enum values for role: \"New Hire\", \"HR Admin\", \"Manager\", \"Mentor\", or \"Content Creator\"\n",
    "- Use valid date format: \"YYYY-MM-DD\" string format\n",
    "\n",
    "### 2. Test GET /users/ - List All Users\n",
    "**Test Name:** `test_list_users_success`\n",
    "- Use the `client` fixture\n",
    "- First, create a test user using POST /users/\n",
    "- Then retrieve the list of users using GET /users/\n",
    "- Assert the response status code is 200 (OK)\n",
    "- Verify the response is a list\n",
    "- Verify the created user appears in the list\n",
    "\n",
    "### 3. Test GET /users/user_id - Retrieve Specific User\n",
    "**Test Name:** `test_get_specific_user_success`\n",
    "- Use the `client` fixture\n",
    "- First, create a test user using POST /users/ and capture the returned user_id\n",
    "- Then retrieve that specific user using GET /users/user_id\n",
    "- Assert the response status code is 200 (OK)\n",
    "- Verify all user fields match the created user\n",
    "\n",
    "## Technical Specifications\n",
    "\n",
    "**Fixture Usage:**\n",
    "- Each test function MUST accept `client` as a parameter\n",
    "- Do NOT create your own TestClient instance\n",
    "- Do NOT manually set up or tear down the database\n",
    "- Let pytest handle fixture injection automatically\n",
    "\n",
    "**Example Function Signature:**\n",
    "```python\n",
    "def test_create_user_success(client):\n",
    "    Test successful user creation via POST /users/.\n",
    "    # Test implementation here\n",
    "```\n",
    "\n",
    "**Test Data:**\n",
    "- Use realistic test data (names, emails, dates)\n",
    "- Use valid UserRole enum values: \"New Hire\", \"HR Admin\", \"Content Creator\", \"Manager\", \"Mentor\"\n",
    "- Use date strings in YYYY-MM-DD format\n",
    "- Include all required fields: first_name, last_name, email, start_date, role\n",
    "\n",
    "**Code Quality:**\n",
    "- Follow the Arrange-Act-Assert pattern\n",
    "- Include descriptive docstrings for each test\n",
    "- Use clear variable names\n",
    "- Add assertion messages for better debugging\n",
    "- Follow PEP 8 style guidelines\n",
    "\n",
    "**DO NOT:**\n",
    "- Do NOT create TestClient manually\n",
    "- Do NOT set up database tables manually\n",
    "- Do NOT import or use `Base.metadata.create_all()`\n",
    "- Do NOT manage database sessions manually\n",
    "- The fixtures handle all of this automatically!\n",
    "\n",
    "## Expected Output Structure\n",
    "\n",
    "Your test file should have this structure:\n",
    "\n",
    "1. **Path Setup Section** (MANDATORY)\n",
    "   - sys.path manipulation to add app directory\n",
    "\n",
    "2. **Imports Section**\n",
    "   - Standard library imports (datetime, etc.)\n",
    "   - pytest import\n",
    "   - Any needed imports from main (UserRole, etc. - but NOT app, get_db, Base, engine)\n",
    "\n",
    "3. **Test Functions Section**\n",
    "   - At least 3 test functions as specified above\n",
    "   - Each using the `client` fixture parameter\n",
    "\n",
    "## Output Format\n",
    "Provide a complete, ready-to-run `test_main_with_fixture.py` file that:\n",
    "- Includes the mandatory path setup code at the top\n",
    "- Provides at least 3 test functions covering the scenarios above\n",
    "- Uses the `client` fixture correctly (as a parameter)\n",
    "- Has comprehensive docstrings explaining each test\n",
    "- Follows all technical specifications above\n",
    "- Is production-ready and can be run with: `pytest artifacts/tests/test_main_with_fixture.py -v`\n",
    "\n",
    "Generate the complete test file now.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt.format(app_code=app_code), client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\")\n",
    "    print(\"\\n✅ Refactored tests saved to: artifacts/tests/test_main_with_fixture.py\")\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
