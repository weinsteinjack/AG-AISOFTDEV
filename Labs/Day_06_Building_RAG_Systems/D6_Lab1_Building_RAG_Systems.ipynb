{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 - Lab 1: Building RAG Systems\n",
    "\n",
    "**Objective:** Build a RAG (Retrieval-Augmented Generation) system orchestrated by LangGraph, scaling in complexity from a simple retriever to a multi-agent team that includes a grader and a router.\n",
    "\n",
    "**Estimated Time:** 180 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 6! Today, we build one of the most powerful and common patterns for enterprise AI: a system that can answer questions about your private documents. We will use LangGraph to create a 'research team' of AI agents. Each agent will have a specific job, and LangGraph will act as the manager, orchestrating their collaboration to find the best possible answer.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We need several libraries for this lab. `langgraph` is the core orchestrator, `langchain` provides the building blocks, `faiss-cpu` is for our vector store, and `pypdf` is for loading documents.\n",
    "\n",
    "**Model Selection:**\n",
    "For RAG and agentic workflows, models with strong instruction-following and reasoning are best. `gpt-4.1`, `o3`, or `gemini-2.5-pro` are excellent choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `load_artifact()`: To read the project documents that will form our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss-cpu not found, installing...\n",
      "pypdf not found, installing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-04 13:23:08,817 ag_aisoftdev.utils WARNING Optional core dependencies not found. Some features will be degraded. provider=None model=None latency_ms=None artifacts_path=None\n",
      "2025-11-04 13:23:08,817 ag_aisoftdev.utils WARNING To enable full functionality run: pip install python-dotenv ipython plantuml provider=None model=None latency_ms=None artifacts_path=None\n",
      "2025-11-04 13:23:08,836 ag_aisoftdev.utils WARNING python-dotenv not installed; .env will not be loaded. provider=None model=None latency_ms=None artifacts_path=None\n",
      "2025-11-04 13:23:09,243 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('langgraph')\n",
    "install_if_missing('langchain')\n",
    "install_if_missing('langchain_community')\n",
    "install_if_missing('langchain_openai')\n",
    "install_if_missing('faiss-cpu')\n",
    "install_if_missing('pypdf')\n",
    "\n",
    "from utils import setup_llm_client, load_artifact\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building the Knowledge Base\n",
    "\n",
    "An agent is only as smart as the information it can access. We will create a vector store containing all the project artifacts we've created so far. This will be our agent's 'knowledge base'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 33 document splits...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_knowledge_base(file_paths):\n",
    "    \"\"\"Loads documents from given paths and creates a FAISS vector store.\"\"\" \n",
    "    all_docs = []\n",
    "    for path in file_paths:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            loader = TextLoader(full_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata={\"source\": path} # Add source metadata\n",
    "            all_docs.extend(docs)\n",
    "        else:\n",
    "            print(f\"Warning: Artifact not found at {full_path}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"No documents found to create knowledge base.\")\n",
    "        return None\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    print(f\"Creating vector store from {len(splits)} document splits...\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "all_artifact_paths = [\"artifacts/day1_prd.md\", \"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"]\n",
    "retriever = create_knowledge_base(all_artifact_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): A Simple RAG Graph\n",
    "\n",
    "**Task:** Build a simple LangGraph with two nodes: one to retrieve documents and one to generate an answer.\n",
    "\n",
    "> **Tip:** Think of `AgentState` as the shared 'whiteboard' for your agent team. Every agent (or 'node' in the graph) can read from and write to this state, allowing them to pass information to each other as they work on a problem.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Define the state for your graph using a `TypedDict`. It should contain keys for `question` and `documents`.\n",
    "2.  Create a \"Retriever\" node. This is a Python function that takes the state, uses the `retriever` to get relevant documents, and updates the state with the results.\n",
    "3.  Create a \"Generator\" node. This function takes the state, creates a prompt with the question and retrieved documents, calls the LLM, and stores the answer.\n",
    "4.  Build the `StateGraph`, add the nodes, and define the edges (`RETRIEVE` -> `GENERATE`).\n",
    "5.  Compile the graph and invoke it with a question about your project.\n",
    "\n",
    "**Expected Quality:** A functional graph that can answer a simple question (e.g., \"What is the purpose of this project?\") by retrieving context from the project artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Building Simple RAG Graph...\n",
      "\n",
      "ðŸ“‹ Testing with question: 'What is the purpose of this project?'\n",
      "==================================================\n",
      "ðŸ” Retrieving documents for: What is the purpose of this project?\n",
      "ðŸ“„ Found 4 relevant documents\n",
      "ðŸ¤– Generating answer...\n",
      "ðŸ“„ Found 4 relevant documents\n",
      "ðŸ¤– Generating answer...\n",
      "âœ… Answer generated\n",
      "==================================================\n",
      "ðŸŽ¯ Final Answer:\n",
      "The purpose of this project is to revolutionize the company's onboarding process\n",
      "by creating a centralized, intuitive platform that addresses the current\n",
      "challenges of fragmented information and manual administrative tasks. The New\n",
      "Hire Experience Platform aims to provide a seamless, engaging, and efficient\n",
      "onboarding journey for new employees, enabling them to feel prepared, connected,\n",
      "and productive from day one, while also reducing the administrative burden on HR\n",
      "and hiring managers.\n",
      "\n",
      "ðŸ” Retrieved Documents:\n",
      "1. Source: artifacts/day1_prd.md\n",
      "   Content preview: ## 2. The Problem A detailed look at the pain points this\n",
      "   product will solve. This section justifies...\n",
      "2. Source: artifacts/day1_prd.md\n",
      "   Content preview: ## 3. Goals & Success Metrics How will we measure\n",
      "   success? This section defines the specific, measur...\n",
      "3. Source: artifacts/day1_prd.md\n",
      "   Content preview: # Product Requirements Document: New Hire Experience\n",
      "   Platform  | Status | **Draft** | | :--- | :--- ...\n",
      "4. Source: artifacts/day1_prd.md\n",
      "   Content preview: **7.2. Future Work:** -   Deeper integration with\n",
      "   existing HRIS for automated new hire provisioning ...\n",
      "âœ… Answer generated\n",
      "==================================================\n",
      "ðŸŽ¯ Final Answer:\n",
      "The purpose of this project is to revolutionize the company's onboarding process\n",
      "by creating a centralized, intuitive platform that addresses the current\n",
      "challenges of fragmented information and manual administrative tasks. The New\n",
      "Hire Experience Platform aims to provide a seamless, engaging, and efficient\n",
      "onboarding journey for new employees, enabling them to feel prepared, connected,\n",
      "and productive from day one, while also reducing the administrative burden on HR\n",
      "and hiring managers.\n",
      "\n",
      "ðŸ” Retrieved Documents:\n",
      "1. Source: artifacts/day1_prd.md\n",
      "   Content preview: ## 2. The Problem A detailed look at the pain points this\n",
      "   product will solve. This section justifies...\n",
      "2. Source: artifacts/day1_prd.md\n",
      "   Content preview: ## 3. Goals & Success Metrics How will we measure\n",
      "   success? This section defines the specific, measur...\n",
      "3. Source: artifacts/day1_prd.md\n",
      "   Content preview: # Product Requirements Document: New Hire Experience\n",
      "   Platform  | Status | **Draft** | | :--- | :--- ...\n",
      "4. Source: artifacts/day1_prd.md\n",
      "   Content preview: **7.2. Future Work:** -   Deeper integration with\n",
      "   existing HRIS for automated new hire provisioning ...\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Step 1: Define the state for the graph using TypedDict\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "# Step 2: Create a \"Retriever\" node\n",
    "def retrieve_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Retrieves relevant documents based on the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(f\"ðŸ” Retrieving documents for: {question}\")\n",
    "    \n",
    "    # Use the retriever to get relevant documents\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    print(f\"ðŸ“„ Found {len(documents)} relevant documents\")\n",
    "    \n",
    "    # Update state with retrieved documents\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "# Step 3: Create a \"Generator\" node\n",
    "def generate_node(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generates an answer using the question and retrieved documents.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    print(f\"ðŸ¤– Generating answer...\")\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Create prompt with question and retrieved documents\n",
    "    prompt = f\"\"\"Based on the following context, answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Call the LLM to generate response\n",
    "    if api_provider == \"openai\":\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "    else:\n",
    "        # Handle other providers if needed\n",
    "        answer = \"Error: Provider not supported in this example\"\n",
    "    \n",
    "    print(f\"âœ… Answer generated\")\n",
    "    \n",
    "    # Store the answer in state\n",
    "    state[\"answer\"] = answer\n",
    "    return state\n",
    "\n",
    "# Step 4: Build the StateGraph, add nodes, and define edges\n",
    "def create_simple_rag_graph():\n",
    "    \"\"\"Creates a simple RAG graph with retrieve and generate nodes.\"\"\"\n",
    "    \n",
    "    # Create the state graph\n",
    "    workflow = StateGraph(AgentState)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    workflow.add_node(\"RETRIEVE\", retrieve_node)\n",
    "    workflow.add_node(\"GENERATE\", generate_node)\n",
    "    \n",
    "    # Define the edges: RETRIEVE -> GENERATE\n",
    "    workflow.set_entry_point(\"RETRIEVE\")\n",
    "    workflow.add_edge(\"RETRIEVE\", \"GENERATE\")\n",
    "    workflow.add_edge(\"GENERATE\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Step 5: Compile the graph and invoke it with a question\n",
    "print(\"ðŸš€ Building Simple RAG Graph...\")\n",
    "rag_app = create_simple_rag_graph()\n",
    "\n",
    "# Test with a question about the project\n",
    "test_question = \"What is the purpose of this project?\"\n",
    "print(f\"\\nðŸ“‹ Testing with question: '{test_question}'\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Invoke the graph\n",
    "result = rag_app.invoke({\n",
    "    \"question\": test_question,\n",
    "    \"documents\": [],\n",
    "    \"answer\": \"\"\n",
    "})\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ðŸŽ¯ Final Answer:\")\n",
    "# Use text wrapping for better readability\n",
    "print_wrapped(result[\"answer\"])\n",
    "\n",
    "print(\"\\nðŸ” Retrieved Documents:\")\n",
    "for i, doc in enumerate(result[\"documents\"], 1):\n",
    "    print(f\"{i}. Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "    print_wrapped(f\"Content preview: {doc.page_content[:100]}...\", \"   \", width=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for text wrapping to improve readability\n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, prefix=\"\", width=80):\n",
    "    \"\"\"Print text with wrapping to avoid horizontal scrolling.\"\"\"\n",
    "    if prefix:\n",
    "        # For prefixed text, use subsequent_indent to align continuation lines\n",
    "        wrapped = textwrap.fill(f\"{prefix}{text}\", width=width, \n",
    "                               subsequent_indent=\" \" * len(prefix))\n",
    "    else:\n",
    "        wrapped = textwrap.fill(text, width=width)\n",
    "    print(wrapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): A Graph with a Grader Agent\n",
    "\n",
    "**Task:** Add a second agent to your graph that acts as a \"Grader,\" deciding if the retrieved documents are relevant enough to answer the question.\n",
    "\n",
    "> **What is a conditional edge?** It's a decision point. After a node completes its task (like our 'Grader'), the conditional edge runs a function to decide which node to go to next. This allows your agent to change its plan based on new information.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Keep your `RETRIEVE` and `GENERATE` nodes from the previous challenge.\n",
    "2.  Create a new \"Grader\" node. This function takes the state (question and documents) and calls an LLM with a specific prompt: \"Based on the question and the following documents, is the information sufficient to answer the question? Answer with only 'yes' or 'no'.\"\n",
    "3.  Add a **conditional edge** to your graph. After the `RETRIEVE` node, the graph should go to the `GRADE` node. After the `GRADE` node, it should check the grader's response. If 'yes', it proceeds to the `GENERATE` node. If 'no', it goes to an `END` node, concluding that it cannot answer the question.\n",
    "\n",
    "**Expected Quality:** A more robust graph that can gracefully handle cases where its knowledge base doesn't contain the answer, preventing it from hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Building Graded RAG Graph with Conditional Edges...\n",
      "\n",
      "============================================================\n",
      "ðŸ§ª TEST 1: Question that should have sufficient information\n",
      "============================================================\n",
      "ðŸ“‹ Testing with: 'What is the purpose of this project?'\n",
      "ðŸ” Retrieving documents for: What is the purpose of this project?\n",
      "ðŸ“„ Found 4 relevant documents\n",
      "âš–ï¸  Grading document relevance...\n",
      "ðŸ“Š Grader decision: yes\n",
      "âœ… Grade: YES - Proceeding to generate answer\n",
      "ðŸ¤– Generating answer...\n",
      "ðŸ“Š Grader decision: yes\n",
      "âœ… Grade: YES - Proceeding to generate answer\n",
      "ðŸ¤– Generating answer...\n",
      "âœ… Answer generated\n",
      "\n",
      "ðŸŽ¯ Result:\n",
      "Answer: The purpose of this project is to revolutionize the company's onboarding\n",
      "process by creating a centralized, intuitive platform that addresses the current\n",
      "challenges of fragmented information and manual administrative tasks. The New\n",
      "Hire Experience Platform aims to provide a seamless, engaging, and e...\n",
      "\n",
      "============================================================\n",
      "ðŸ§ª TEST 2: Question that should have insufficient information\n",
      "============================================================\n",
      "ðŸ“‹ Testing with: 'What is the weather forecast for tomorrow in Tokyo?'\n",
      "ðŸ” Retrieving documents for: What is the weather forecast for tomorrow in Tokyo?\n",
      "ðŸ“„ Found 4 relevant documents\n",
      "âš–ï¸  Grading document relevance...\n",
      "âœ… Answer generated\n",
      "\n",
      "ðŸŽ¯ Result:\n",
      "Answer: The purpose of this project is to revolutionize the company's onboarding\n",
      "process by creating a centralized, intuitive platform that addresses the current\n",
      "challenges of fragmented information and manual administrative tasks. The New\n",
      "Hire Experience Platform aims to provide a seamless, engaging, and e...\n",
      "\n",
      "============================================================\n",
      "ðŸ§ª TEST 2: Question that should have insufficient information\n",
      "============================================================\n",
      "ðŸ“‹ Testing with: 'What is the weather forecast for tomorrow in Tokyo?'\n",
      "ðŸ” Retrieving documents for: What is the weather forecast for tomorrow in Tokyo?\n",
      "ðŸ“„ Found 4 relevant documents\n",
      "âš–ï¸  Grading document relevance...\n",
      "ðŸ“Š Grader decision: no\n",
      "âŒ Grade: NO - Insufficient information, ending\n",
      "\n",
      "ðŸŽ¯ Result:\n",
      "âœ… Correctly identified insufficient information - no answer generated\n",
      "\n",
      "ðŸŽ‰ Challenge 2 Complete! The graded RAG system can now:\n",
      "   âœ… Evaluate document relevance before generating answers\n",
      "   âœ… Prevent hallucination by refusing to answer when information is insufficient\n",
      "   âœ… Use conditional edges to make intelligent routing decisions\n",
      "ðŸ“Š Grader decision: no\n",
      "âŒ Grade: NO - Insufficient information, ending\n",
      "\n",
      "ðŸŽ¯ Result:\n",
      "âœ… Correctly identified insufficient information - no answer generated\n",
      "\n",
      "ðŸŽ‰ Challenge 2 Complete! The graded RAG system can now:\n",
      "   âœ… Evaluate document relevance before generating answers\n",
      "   âœ… Prevent hallucination by refusing to answer when information is insufficient\n",
      "   âœ… Use conditional edges to make intelligent routing decisions\n"
     ]
    }
   ],
   "source": [
    "# Challenge 2: RAG Graph with Grader Agent and Conditional Edges\n",
    "\n",
    "# Step 1: Update the state to include grader results\n",
    "class GradedAgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "    grade: str  # New field for grader response\n",
    "\n",
    "# Step 2: Keep the RETRIEVE and GENERATE nodes from Challenge 1 (with updated state)\n",
    "def retrieve_node_v2(state: GradedAgentState) -> GradedAgentState:\n",
    "    \"\"\"Retrieves relevant documents based on the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(f\"ðŸ” Retrieving documents for: {question}\")\n",
    "    \n",
    "    # Use the retriever to get relevant documents\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    print(f\"ðŸ“„ Found {len(documents)} relevant documents\")\n",
    "    \n",
    "    # Update state with retrieved documents\n",
    "    state[\"documents\"] = documents\n",
    "    return state\n",
    "\n",
    "def generate_node_v2(state: GradedAgentState) -> GradedAgentState:\n",
    "    \"\"\"Generates an answer using the question and retrieved documents.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    print(f\"ðŸ¤– Generating answer...\")\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Create prompt with question and retrieved documents\n",
    "    prompt = f\"\"\"Based on the following context, answer the question accurately and concisely.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Call the LLM to generate response\n",
    "    if api_provider == \"openai\":\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "    else:\n",
    "        answer = \"Error: Provider not supported in this example\"\n",
    "    \n",
    "    print(f\"âœ… Answer generated\")\n",
    "    \n",
    "    # Store the answer in state\n",
    "    state[\"answer\"] = answer\n",
    "    return state\n",
    "\n",
    "# Step 3: Create the new \"Grader\" node\n",
    "def grade_node(state: GradedAgentState) -> GradedAgentState:\n",
    "    \"\"\"Grades if the retrieved documents are sufficient to answer the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    print(f\"âš–ï¸  Grading document relevance...\")\n",
    "    \n",
    "    # Create context from retrieved documents for grading\n",
    "    context = \"\\n\\n\".join([f\"Document {i+1}: {doc.page_content[:500]}\" \n",
    "                          for i, doc in enumerate(documents)])\n",
    "    \n",
    "    # Improved grader prompt - more specific instructions\n",
    "    grade_prompt = f\"\"\"You are a document relevance grader. Your task is to determine if the provided documents contain enough relevant information to answer the given question.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Documents:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Answer \"yes\" if the documents contain relevant information that can be used to answer the question\n",
    "- Answer \"no\" if the documents do not contain relevant information to answer the question\n",
    "- Be objective and focus on content relevance, not perfect completeness\n",
    "\n",
    "Decision (yes/no):\"\"\"\n",
    "    \n",
    "    # Call the LLM to grade\n",
    "    if api_provider == \"openai\":\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": grade_prompt}],\n",
    "            temperature=0.0  # Use 0 temperature for consistent grading\n",
    "        )\n",
    "        grade = response.choices[0].message.content.strip().lower()\n",
    "        # Clean the response to ensure it's just \"yes\" or \"no\"\n",
    "        if \"yes\" in grade:\n",
    "            grade = \"yes\"\n",
    "        else:\n",
    "            grade = \"no\"\n",
    "    else:\n",
    "        grade = \"no\"  # Default to no if provider not supported\n",
    "    \n",
    "    print(f\"ðŸ“Š Grader decision: {grade}\")\n",
    "    \n",
    "    # Store the grade in state\n",
    "    state[\"grade\"] = grade\n",
    "    return state\n",
    "\n",
    "# Step 4: Create conditional edge function\n",
    "def decide_next_step(state: GradedAgentState) -> str:\n",
    "    \"\"\"Decides whether to generate answer or end based on grader decision.\"\"\"\n",
    "    grade = state.get(\"grade\", \"no\")\n",
    "    \n",
    "    if grade == \"yes\":\n",
    "        print(\"âœ… Grade: YES - Proceeding to generate answer\")\n",
    "        return \"GENERATE\"\n",
    "    else:\n",
    "        print(\"âŒ Grade: NO - Insufficient information, ending\")\n",
    "        return \"END\"\n",
    "\n",
    "# Step 5: Build the enhanced StateGraph with conditional edges\n",
    "def create_graded_rag_graph():\n",
    "    \"\"\"Creates a RAG graph with grader and conditional edges.\"\"\"\n",
    "    \n",
    "    # Create the state graph\n",
    "    workflow = StateGraph(GradedAgentState)\n",
    "    \n",
    "    # Add nodes to the graph\n",
    "    workflow.add_node(\"RETRIEVE\", retrieve_node_v2)\n",
    "    workflow.add_node(\"GRADE\", grade_node)\n",
    "    workflow.add_node(\"GENERATE\", generate_node_v2)\n",
    "    \n",
    "    # Define the flow: RETRIEVE -> GRADE -> (conditional) -> GENERATE or END\n",
    "    workflow.set_entry_point(\"RETRIEVE\")\n",
    "    workflow.add_edge(\"RETRIEVE\", \"GRADE\")\n",
    "    \n",
    "    # Add conditional edge after GRADE node\n",
    "    workflow.add_conditional_edges(\n",
    "        \"GRADE\",\n",
    "        decide_next_step,\n",
    "        {\n",
    "            \"GENERATE\": \"GENERATE\",\n",
    "            \"END\": END\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # GENERATE always goes to END\n",
    "    workflow.add_edge(\"GENERATE\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Step 6: Test the graded RAG system\n",
    "print(\"\\nðŸš€ Building Graded RAG Graph with Conditional Edges...\")\n",
    "graded_rag_app = create_graded_rag_graph()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ§ª TEST 1: Question that should have sufficient information\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with a question that should have good documents\n",
    "test_question_1 = \"What is the purpose of this project?\"\n",
    "print(f\"ðŸ“‹ Testing with: '{test_question_1}'\")\n",
    "\n",
    "result_1 = graded_rag_app.invoke({\n",
    "    \"question\": test_question_1,\n",
    "    \"documents\": [],\n",
    "    \"answer\": \"\",\n",
    "    \"grade\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Result:\")\n",
    "if result_1.get(\"answer\"):\n",
    "    print(\"Answer:\")\n",
    "    print_wrapped(result_1['answer'], width=80)  # Show full answer without truncation\n",
    "else:\n",
    "    print(\"No answer generated - insufficient information\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ðŸ§ª TEST 2: Question that should have insufficient information\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with a question that likely won't have good documents\n",
    "test_question_2 = \"What is the weather forecast for tomorrow in Tokyo?\"\n",
    "print(f\"ðŸ“‹ Testing with: '{test_question_2}'\")\n",
    "\n",
    "result_2 = graded_rag_app.invoke({\n",
    "    \"question\": test_question_2,\n",
    "    \"documents\": [],\n",
    "    \"answer\": \"\",\n",
    "    \"grade\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Result:\")\n",
    "if result_2.get(\"answer\"):\n",
    "    print(\"Answer:\")\n",
    "    print_wrapped(result_2['answer'], width=80)  # Show full answer without truncation\n",
    "else:\n",
    "    print(\"âœ… Correctly identified insufficient information - no answer generated\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Challenge 2 Complete! The graded RAG system can now:\")\n",
    "print(\"   âœ… Evaluate document relevance before generating answers\")\n",
    "print(\"   âœ… Prevent hallucination by refusing to answer when information is insufficient\")\n",
    "print(\"   âœ… Use conditional edges to make intelligent routing decisions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): A Multi-Agent Research Team\n",
    "\n",
    "**Task:** Build a sophisticated \"research team\" of specialized agents that includes a router to delegate tasks to the correct specialist.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Specialize your retriever:** Create two separate retrievers. One for the PRD (`prd_retriever`) and one for the technical documents (`tech_retriever` for schema and ADRs).\n",
    "2.  **Define the Agents:**\n",
    "    * `ProjectManagerAgent`: This will be the entry point and will act as a router. It uses an LLM to decide whether the user's question is about product requirements or technical details, and routes to the appropriate researcher.\n",
    "    * `PRDResearcherAgent`: A node that uses the `prd_retriever`.\n",
    "    * `TechResearcherAgent`: A node that uses the `tech_retriever`.\n",
    "    * `SynthesizerAgent`: A node that takes the collected documents from either researcher and synthesizes a final answer.\n",
    "3.  **Build the Graph:** Use conditional edges to orchestrate the flow: The entry point is the `ProjectManager`, which then routes to either the `PRD_RESEARCHER` or `TECH_RESEARCHER`. Both of those nodes should then route to the `SYNTHESIZE` node, which then goes to the `END`.\n",
    "\n",
    "**Expected Quality:** A highly advanced agentic system that mimics a real-world research workflow, including a router and specialist roles, to improve the accuracy and efficiency of the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ PRD Retriever created with 20 document splits\n",
      "ðŸ”§ Tech Retriever created with 13 document splits\n",
      "\n",
      "ðŸš€ Building Multi-Agent Research Team...\n",
      "\n",
      "======================================================================\n",
      "ðŸ§ª TEST 1: Product Requirements Question\n",
      "======================================================================\n",
      "ðŸ“‹ Testing PRD routing with: 'What are the main goals and success\n",
      "metrics for this project?'\n",
      "ðŸ‘” Project Manager analyzing: What are the main goals and success metrics for this project?\n",
      "ðŸ”§ Tech Retriever created with 13 document splits\n",
      "\n",
      "ðŸš€ Building Multi-Agent Research Team...\n",
      "\n",
      "======================================================================\n",
      "ðŸ§ª TEST 1: Product Requirements Question\n",
      "======================================================================\n",
      "ðŸ“‹ Testing PRD routing with: 'What are the main goals and success\n",
      "metrics for this project?'\n",
      "ðŸ‘” Project Manager analyzing: What are the main goals and success metrics for this project?\n",
      "ðŸŽ¯ Routing decision: PRD\n",
      "âž¡ï¸  Routing to PRD Researcher\n",
      "ðŸ“‹ PRD Researcher investigating: What are the main goals and success metrics for this project?\n",
      "ðŸ“„ Found 4 PRD-related documents\n",
      "ðŸ§  Synthesizer creating final answer from Product Requirements research...\n",
      "ðŸŽ¯ Routing decision: PRD\n",
      "âž¡ï¸  Routing to PRD Researcher\n",
      "ðŸ“‹ PRD Researcher investigating: What are the main goals and success metrics for this project?\n",
      "ðŸ“„ Found 4 PRD-related documents\n",
      "ðŸ§  Synthesizer creating final answer from Product Requirements research...\n",
      "âœ… Synthesis complete\n",
      "\n",
      "ðŸŽ¯ PRD Research Result:\n",
      "Research Type: Product Requirements\n",
      "Answer:\n",
      "The product requirements research conducted for this project outlines clear\n",
      "goals and success metrics aimed at addressing the identified onboarding\n",
      "challenges. The primary objectives are to enhance the onboarding experience for\n",
      "new hires, reduce the workload on onboarding orchestrators, and improve overall\n",
      "engagement and retention.  **Goals and Success Metrics:**  1. **Performance:**\n",
      "The applicati...\n",
      "\n",
      "======================================================================\n",
      "ðŸ§ª TEST 2: Technical Implementation Question\n",
      "======================================================================\n",
      "ðŸ“‹ Testing Tech routing with: 'What database technology was chosen and\n",
      "why?'\n",
      "ðŸ‘” Project Manager analyzing: What database technology was chosen and why?\n",
      "âœ… Synthesis complete\n",
      "\n",
      "ðŸŽ¯ PRD Research Result:\n",
      "Research Type: Product Requirements\n",
      "Answer:\n",
      "The product requirements research conducted for this project outlines clear\n",
      "goals and success metrics aimed at addressing the identified onboarding\n",
      "challenges. The primary objectives are to enhance the onboarding experience for\n",
      "new hires, reduce the workload on onboarding orchestrators, and improve overall\n",
      "engagement and retention.  **Goals and Success Metrics:**  1. **Performance:**\n",
      "The applicati...\n",
      "\n",
      "======================================================================\n",
      "ðŸ§ª TEST 2: Technical Implementation Question\n",
      "======================================================================\n",
      "ðŸ“‹ Testing Tech routing with: 'What database technology was chosen and\n",
      "why?'\n",
      "ðŸ‘” Project Manager analyzing: What database technology was chosen and why?\n",
      "ðŸŽ¯ Routing decision: TECHNICAL\n",
      "âž¡ï¸  Routing to Tech Researcher\n",
      "ðŸ”§ Tech Researcher investigating: What database technology was chosen and why?\n",
      "ðŸŽ¯ Routing decision: TECHNICAL\n",
      "âž¡ï¸  Routing to Tech Researcher\n",
      "ðŸ”§ Tech Researcher investigating: What database technology was chosen and why?\n",
      "ðŸ“„ Found 4 technical documents\n",
      "ðŸ§  Synthesizer creating final answer from Technical Implementation research...\n",
      "ðŸ“„ Found 4 technical documents\n",
      "ðŸ§  Synthesizer creating final answer from Technical Implementation research...\n",
      "âœ… Synthesis complete\n",
      "\n",
      "ðŸŽ¯ Tech Research Result:\n",
      "Research Type: Technical Implementation\n",
      "Answer:\n",
      "The technical implementation research conducted focused on selecting an\n",
      "appropriate database technology for storing and querying data, including content\n",
      "text, metadata, and vector embeddings. The chosen technology is PostgreSQL,\n",
      "enhanced with the `pgvector` extension. This decision was driven by several key\n",
      "factors:  1. **Unified Data Store & Simplified Architecture:** By storing all\n",
      "data types wi...\n",
      "\n",
      "======================================================================\n",
      "ðŸ§ª TEST 3: Ambiguous Question (Router Decision)\n",
      "======================================================================\n",
      "ðŸ“‹ Testing Router intelligence with: 'How will users interact with the\n",
      "system?'\n",
      "ðŸ‘” Project Manager analyzing: How will users interact with the system?\n",
      "âœ… Synthesis complete\n",
      "\n",
      "ðŸŽ¯ Tech Research Result:\n",
      "Research Type: Technical Implementation\n",
      "Answer:\n",
      "The technical implementation research conducted focused on selecting an\n",
      "appropriate database technology for storing and querying data, including content\n",
      "text, metadata, and vector embeddings. The chosen technology is PostgreSQL,\n",
      "enhanced with the `pgvector` extension. This decision was driven by several key\n",
      "factors:  1. **Unified Data Store & Simplified Architecture:** By storing all\n",
      "data types wi...\n",
      "\n",
      "======================================================================\n",
      "ðŸ§ª TEST 3: Ambiguous Question (Router Decision)\n",
      "======================================================================\n",
      "ðŸ“‹ Testing Router intelligence with: 'How will users interact with the\n",
      "system?'\n",
      "ðŸ‘” Project Manager analyzing: How will users interact with the system?\n",
      "ðŸŽ¯ Routing decision: PRD\n",
      "âž¡ï¸  Routing to PRD Researcher\n",
      "ðŸ“‹ PRD Researcher investigating: How will users interact with the system?\n",
      "ðŸŽ¯ Routing decision: PRD\n",
      "âž¡ï¸  Routing to PRD Researcher\n",
      "ðŸ“‹ PRD Researcher investigating: How will users interact with the system?\n",
      "ðŸ“„ Found 4 PRD-related documents\n",
      "ðŸ§  Synthesizer creating final answer from Product Requirements research...\n",
      "ðŸ“„ Found 4 PRD-related documents\n",
      "ðŸ§  Synthesizer creating final answer from Product Requirements research...\n",
      "âœ… Synthesis complete\n",
      "\n",
      "ðŸŽ¯ Router Decision Result:\n",
      "Routed to: Product Requirements\n",
      "Answer:\n",
      "Based on the product requirements research conducted, users will interact with\n",
      "the system through an intuitive and consistent user interface designed to\n",
      "minimize the need for extensive training. This interface will cater to new\n",
      "hires, onboarding orchestrators, and knowledge curators, ensuring a seamless\n",
      "onboarding experience. The system's usability is a priority, aiming to reduce\n",
      "the learning curv...\n",
      "\n",
      "ðŸŽ‰ Challenge 3 Complete! The Multi-Agent Research Team features:\n",
      "   âœ… Intelligent routing based on question content\n",
      "   âœ… Specialized retrievers for different document types\n",
      "   âœ… Collaborative workflow with distinct agent roles\n",
      "   âœ… Synthesis of research findings into comprehensive answers\n",
      "   âœ… Production-ready multi-agent architecture\n",
      "\n",
      "ðŸ† LAB COMPLETE! You've built a sophisticated RAG system that:\n",
      "   ðŸ”¹ Started with simple retrieval and generation\n",
      "   ðŸ”¹ Added quality control with grading\n",
      "   ðŸ”¹ Evolved into a multi-agent research team\n",
      "   ðŸ”¹ Demonstrates real-world enterprise AI patterns\n",
      "âœ… Synthesis complete\n",
      "\n",
      "ðŸŽ¯ Router Decision Result:\n",
      "Routed to: Product Requirements\n",
      "Answer:\n",
      "Based on the product requirements research conducted, users will interact with\n",
      "the system through an intuitive and consistent user interface designed to\n",
      "minimize the need for extensive training. This interface will cater to new\n",
      "hires, onboarding orchestrators, and knowledge curators, ensuring a seamless\n",
      "onboarding experience. The system's usability is a priority, aiming to reduce\n",
      "the learning curv...\n",
      "\n",
      "ðŸŽ‰ Challenge 3 Complete! The Multi-Agent Research Team features:\n",
      "   âœ… Intelligent routing based on question content\n",
      "   âœ… Specialized retrievers for different document types\n",
      "   âœ… Collaborative workflow with distinct agent roles\n",
      "   âœ… Synthesis of research findings into comprehensive answers\n",
      "   âœ… Production-ready multi-agent architecture\n",
      "\n",
      "ðŸ† LAB COMPLETE! You've built a sophisticated RAG system that:\n",
      "   ðŸ”¹ Started with simple retrieval and generation\n",
      "   ðŸ”¹ Added quality control with grading\n",
      "   ðŸ”¹ Evolved into a multi-agent research team\n",
      "   ðŸ”¹ Demonstrates real-world enterprise AI patterns\n"
     ]
    }
   ],
   "source": [
    "# Challenge 3: Multi-Agent Research Team with Specialized Retrievers and Router\n",
    "\n",
    "# Step 1: Create specialized retrievers\n",
    "def create_specialized_retrievers():\n",
    "    \"\"\"Creates specialized retrievers for different types of documents.\"\"\"\n",
    "    \n",
    "    # PRD Retriever - Only for Product Requirements Document\n",
    "    prd_paths = [\"artifacts/day1_prd.md\"]\n",
    "    prd_docs = []\n",
    "    for path in prd_paths:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            loader = TextLoader(full_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata = {\"source\": path, \"type\": \"prd\"}\n",
    "            prd_docs.extend(docs)\n",
    "    \n",
    "    if prd_docs:\n",
    "        prd_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        prd_splits = prd_splitter.split_documents(prd_docs)\n",
    "        prd_vectorstore = FAISS.from_documents(documents=prd_splits, embedding=OpenAIEmbeddings())\n",
    "        prd_retriever = prd_vectorstore.as_retriever()\n",
    "        print(f\"ðŸ“‹ PRD Retriever created with {len(prd_splits)} document splits\")\n",
    "    else:\n",
    "        prd_retriever = None\n",
    "        print(\"âŒ No PRD documents found\")\n",
    "    \n",
    "    # Tech Retriever - For schema and ADRs\n",
    "    tech_paths = [\"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"]\n",
    "    tech_docs = []\n",
    "    for path in tech_paths:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            loader = TextLoader(full_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata = {\"source\": path, \"type\": \"technical\"}\n",
    "            tech_docs.extend(docs)\n",
    "    \n",
    "    if tech_docs:\n",
    "        tech_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "        tech_splits = tech_splitter.split_documents(tech_docs)\n",
    "        tech_vectorstore = FAISS.from_documents(documents=tech_splits, embedding=OpenAIEmbeddings())\n",
    "        tech_retriever = tech_vectorstore.as_retriever()\n",
    "        print(f\"ðŸ”§ Tech Retriever created with {len(tech_splits)} document splits\")\n",
    "    else:\n",
    "        tech_retriever = None\n",
    "        print(\"âŒ No technical documents found\")\n",
    "    \n",
    "    return prd_retriever, tech_retriever\n",
    "\n",
    "# Create specialized retrievers\n",
    "prd_retriever, tech_retriever = create_specialized_retrievers()\n",
    "\n",
    "# Step 2: Define the Multi-Agent State\n",
    "class MultiAgentState(TypedDict):\n",
    "    question: str\n",
    "    route_decision: str  # \"prd\" or \"technical\"\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "    research_type: str  # Track which researcher was used\n",
    "\n",
    "# Step 3: Define the Agent Nodes\n",
    "\n",
    "def project_manager_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Router agent that decides which specialist to use based on the question.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(f\"ðŸ‘” Project Manager analyzing: {question}\")\n",
    "    \n",
    "    # Router prompt to classify the question\n",
    "    router_prompt = f\"\"\"You are a project manager routing questions to the appropriate specialist.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Analyze this question and determine if it's about:\n",
    "- \"prd\": Product requirements, features, goals, user stories, or business objectives\n",
    "- \"technical\": Technical implementation, database schema, architecture decisions, or technical specifications\n",
    "\n",
    "Respond with exactly one word: either \"prd\" or \"technical\"\n",
    "\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    # Get routing decision from LLM\n",
    "    if api_provider == \"openai\":\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": router_prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        route_decision = response.choices[0].message.content.strip().lower()\n",
    "        \n",
    "        # Clean the response to ensure it's either \"prd\" or \"technical\"\n",
    "        if \"prd\" in route_decision:\n",
    "            route_decision = \"prd\"\n",
    "        else:\n",
    "            route_decision = \"technical\"\n",
    "    else:\n",
    "        route_decision = \"prd\"  # Default fallback\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Routing decision: {route_decision.upper()}\")\n",
    "    \n",
    "    state[\"route_decision\"] = route_decision\n",
    "    return state\n",
    "\n",
    "def prd_researcher_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Specialized agent for researching product requirements.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(f\"ðŸ“‹ PRD Researcher investigating: {question}\")\n",
    "    \n",
    "    if prd_retriever:\n",
    "        # Use PRD-specific retriever\n",
    "        documents = prd_retriever.invoke(question)\n",
    "        print(f\"ðŸ“„ Found {len(documents)} PRD-related documents\")\n",
    "        \n",
    "        state[\"documents\"] = documents\n",
    "        state[\"research_type\"] = \"Product Requirements\"\n",
    "    else:\n",
    "        print(\"âŒ PRD retriever not available\")\n",
    "        state[\"documents\"] = []\n",
    "        state[\"research_type\"] = \"Product Requirements (unavailable)\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "def tech_researcher_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Specialized agent for researching technical details.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    print(f\"ðŸ”§ Tech Researcher investigating: {question}\")\n",
    "    \n",
    "    if tech_retriever:\n",
    "        # Use technical-specific retriever\n",
    "        documents = tech_retriever.invoke(question)\n",
    "        print(f\"ðŸ“„ Found {len(documents)} technical documents\")\n",
    "        \n",
    "        state[\"documents\"] = documents\n",
    "        state[\"research_type\"] = \"Technical Implementation\"\n",
    "    else:\n",
    "        print(\"âŒ Tech retriever not available\")\n",
    "        state[\"documents\"] = []\n",
    "        state[\"research_type\"] = \"Technical Implementation (unavailable)\"\n",
    "    \n",
    "    return state\n",
    "\n",
    "def synthesizer_agent(state: MultiAgentState) -> MultiAgentState:\n",
    "    \"\"\"Synthesizes the final answer from the collected research.\"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    research_type = state.get(\"research_type\", \"Unknown\")\n",
    "    \n",
    "    print(f\"ðŸ§  Synthesizer creating final answer from {research_type} research...\")\n",
    "    \n",
    "    if not documents:\n",
    "        state[\"answer\"] = f\"I apologize, but I couldn't find sufficient {research_type.lower()} information to answer your question.\"\n",
    "        return state\n",
    "    \n",
    "    # Create context from research documents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    # Synthesis prompt that acknowledges the research type\n",
    "    synthesis_prompt = f\"\"\"As a specialist synthesizer, create a comprehensive answer using the {research_type.lower()} research provided below.\n",
    "\n",
    "Research Type: {research_type}\n",
    "Question: {question}\n",
    "\n",
    "Research Findings:\n",
    "{context}\n",
    "\n",
    "Instructions:\n",
    "- Provide a clear, accurate answer based on the research\n",
    "- Acknowledge the type of research conducted\n",
    "- Be specific and cite relevant details from the findings\n",
    "- If information is incomplete, state what additional research might be needed\n",
    "\n",
    "Synthesized Answer:\"\"\"\n",
    "    \n",
    "    # Generate the final synthesized answer\n",
    "    if api_provider == \"openai\":\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": synthesis_prompt}],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        answer = response.choices[0].message.content\n",
    "    else:\n",
    "        answer = \"Error: Provider not supported in this example\"\n",
    "    \n",
    "    print(f\"âœ… Synthesis complete\")\n",
    "    \n",
    "    state[\"answer\"] = answer\n",
    "    return state\n",
    "\n",
    "# Step 4: Create routing function for conditional edges\n",
    "def route_to_researcher(state: MultiAgentState) -> str:\n",
    "    \"\"\"Routes to the appropriate researcher based on the manager's decision.\"\"\"\n",
    "    route_decision = state.get(\"route_decision\", \"prd\")\n",
    "    \n",
    "    if route_decision == \"prd\":\n",
    "        print(\"âž¡ï¸  Routing to PRD Researcher\")\n",
    "        return \"PRD_RESEARCHER\"\n",
    "    else:\n",
    "        print(\"âž¡ï¸  Routing to Tech Researcher\")\n",
    "        return \"TECH_RESEARCHER\"\n",
    "\n",
    "# Step 5: Build the Multi-Agent Research Team Graph\n",
    "def create_multi_agent_research_team():\n",
    "    \"\"\"Creates the sophisticated multi-agent research team graph.\"\"\"\n",
    "    \n",
    "    # Create the state graph\n",
    "    workflow = StateGraph(MultiAgentState)\n",
    "    \n",
    "    # Add all the agent nodes\n",
    "    workflow.add_node(\"PROJECT_MANAGER\", project_manager_agent)\n",
    "    workflow.add_node(\"PRD_RESEARCHER\", prd_researcher_agent)\n",
    "    workflow.add_node(\"TECH_RESEARCHER\", tech_researcher_agent)\n",
    "    workflow.add_node(\"SYNTHESIZER\", synthesizer_agent)\n",
    "    \n",
    "    # Define the flow\n",
    "    # Entry point: PROJECT_MANAGER\n",
    "    workflow.set_entry_point(\"PROJECT_MANAGER\")\n",
    "    \n",
    "    # Conditional routing from PROJECT_MANAGER to researchers\n",
    "    workflow.add_conditional_edges(\n",
    "        \"PROJECT_MANAGER\",\n",
    "        route_to_researcher,\n",
    "        {\n",
    "            \"PRD_RESEARCHER\": \"PRD_RESEARCHER\",\n",
    "            \"TECH_RESEARCHER\": \"TECH_RESEARCHER\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Both researchers route to SYNTHESIZER\n",
    "    workflow.add_edge(\"PRD_RESEARCHER\", \"SYNTHESIZER\")\n",
    "    workflow.add_edge(\"TECH_RESEARCHER\", \"SYNTHESIZER\")\n",
    "    \n",
    "    # SYNTHESIZER routes to END\n",
    "    workflow.add_edge(\"SYNTHESIZER\", END)\n",
    "    \n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Step 6: Test the Multi-Agent Research Team\n",
    "print(\"\\nðŸš€ Building Multi-Agent Research Team...\")\n",
    "research_team_app = create_multi_agent_research_team()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ§ª TEST 1: Product Requirements Question\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "prd_question = \"What are the main goals and success metrics for this project?\"\n",
    "print_wrapped(f\"ðŸ“‹ Testing PRD routing with: '{prd_question}'\", width=70)\n",
    "\n",
    "prd_result = research_team_app.invoke({\n",
    "    \"question\": prd_question,\n",
    "    \"route_decision\": \"\",\n",
    "    \"documents\": [],\n",
    "    \"answer\": \"\",\n",
    "    \"research_type\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"\\nðŸŽ¯ PRD Research Result:\")\n",
    "print(f\"Research Type: {prd_result.get('research_type', 'Unknown')}\")\n",
    "print(\"Answer:\")\n",
    "print_wrapped(prd_result['answer'], width=80)  # Show full answer without truncation\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ§ª TEST 2: Technical Implementation Question\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "tech_question = \"What database technology was chosen and why?\"\n",
    "print_wrapped(f\"ðŸ“‹ Testing Tech routing with: '{tech_question}'\", width=70)\n",
    "\n",
    "tech_result = research_team_app.invoke({\n",
    "    \"question\": tech_question,\n",
    "    \"route_decision\": \"\",\n",
    "    \"documents\": [],\n",
    "    \"answer\": \"\",\n",
    "    \"research_type\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Tech Research Result:\")\n",
    "print(f\"Research Type: {tech_result.get('research_type', 'Unknown')}\")\n",
    "print(\"Answer:\")\n",
    "print_wrapped(tech_result['answer'], width=80)  # Show full answer without truncation\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ§ª TEST 3: Ambiguous Question (Router Decision)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "ambiguous_question = \"How will users interact with the system?\"\n",
    "print_wrapped(f\"ðŸ“‹ Testing Router intelligence with: '{ambiguous_question}'\", width=70)\n",
    "\n",
    "ambiguous_result = research_team_app.invoke({\n",
    "    \"question\": ambiguous_question,\n",
    "    \"route_decision\": \"\",\n",
    "    \"documents\": [],\n",
    "    \"answer\": \"\",\n",
    "    \"research_type\": \"\"\n",
    "})\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Router Decision Result:\")\n",
    "print(f\"Routed to: {ambiguous_result.get('research_type', 'Unknown')}\")\n",
    "print(\"Answer:\")\n",
    "print_wrapped(ambiguous_result['answer'], width=80)  # Show full answer without truncation\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Challenge 3 Complete! The Multi-Agent Research Team features:\")\n",
    "print(\"   âœ… Intelligent routing based on question content\")\n",
    "print(\"   âœ… Specialized retrievers for different document types\")\n",
    "print(\"   âœ… Collaborative workflow with distinct agent roles\")\n",
    "print(\"   âœ… Synthesis of research findings into comprehensive answers\")\n",
    "print(\"   âœ… Production-ready multi-agent architecture\")\n",
    "\n",
    "print(f\"\\nðŸ† LAB COMPLETE! You've built a sophisticated RAG system that:\")\n",
    "print(\"   ðŸ”¹ Started with simple retrieval and generation\")\n",
    "print(\"   ðŸ”¹ Added quality control with grading\")\n",
    "print(\"   ðŸ”¹ Evolved into a multi-agent research team\")\n",
    "print(\"   ðŸ”¹ Demonstrates real-world enterprise AI patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Incredible work! You have now built a truly sophisticated AI system. You've learned how to create a knowledge base for an agent and how to use LangGraph to orchestrate a team of specialized agents to solve a complex problem. You progressed from a simple RAG chain to a system that includes quality checks (the Grader) and intelligent task delegation (the Router). These are the core patterns for building production-ready RAG applications.\n",
    "\n",
    "> **Key Takeaway:** LangGraph allows you to define complex, stateful, multi-agent workflows as a graph. Using nodes for agents and conditional edges for decision-making enables the creation of sophisticated systems that can reason, delegate, and collaborate to solve problems more effectively than a single agent could alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
