{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3 - Lab 1: AI-Driven Backend Development (Solution)\n",
    "\n",
    "**Objective:** Generate a complete FastAPI backend application, including Pydantic and SQLAlchemy models, and then perform the critical engineering task of integrating the generated code with the live SQLite database created on Day 2.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete code and prompts for generating and assembling the database-connected API. It highlights the workflow of generating components separately and then integrating them, a common pattern in AI-assisted development.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load our `schema.sql` artifact, which will be the primary context for our code generation prompts. Having the database schema is essential for the LLM to accurately generate models (both Pydantic and SQLAlchemy) and endpoints that match our data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'huggingface' with model 'meta-llama/Llama-4-Scout-17B-16E-Instruct'\n",
      "✅ LLM Client configured: Using 'anthropic' with model 'claude-opus-4-1-20250805'\n",
      "✅ LLM Client configured: Using 'google' with model 'gemini-2.5-pro'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, recommended_models_table, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for different artifacts to use the latest models from different providers.\n",
    "# - In-memory app generation: use a Scout/Llama family model for instruction-following code generation\n",
    "# - DB models & session code: use a strong instruction-following model (e.g. gpt-4o)\n",
    "# - Integration/synthesis tasks: use another high-quality model (e.g. gemini-2.5-pro)\n",
    "in_memory_client, in_memory_model_name, in_memory_api_provider = setup_llm_client(model_name=\"meta-llama/Llama-4-Scout-17B-16E-Instruct\")\n",
    "db_client, db_model_name, db_api_provider = setup_llm_client(model_name=\"claude-opus-4-1-20250805\")\n",
    "integration_client, integration_model_name, integration_api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")\n",
    "\n",
    "# Load the SQL schema from Day 2\n",
    "sql_schema = load_artifact(\"artifacts/schema.sql\")\n",
    "if not sql_schema:\n",
    "    print(\"Warning: Could not load schema.sql. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|---|---|\n",
       "| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\n",
       "| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\n",
       "| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\n",
       "| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\n",
       "| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\n",
       "| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\n",
       "| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\n",
       "| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\n",
       "| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Text | Vision | Image Gen | Image Edit | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|---|---|\\n| claude-opus-4-1-20250805 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| deepseek-ai/DeepSeek-V3.1 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 128,000 | 100,000 |\\n| gemini-1.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 8,192 |\\n| gemini-1.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 2,000,000 | 8,192 |\\n| gemini-2.0-flash-exp | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 8,192 |\\n| gemini-2.5-flash | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-lite | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ✅ | ✅ | ❌ | ❌ | ❌ | 1,048,576 | 65,536 |\\n| gpt-4.1 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,768 |\\n| gpt-4.1-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 1,000,000 | 32,000 |\\n| gpt-4o | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 400,000 | 128,000 |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 8,192 | 4,096 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 32,768 | 8,192 |\\n| o3 | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| o4-mini | openai | ✅ | ✅ | ❌ | ❌ | ❌ | 200,000 | 100,000 |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ✅ | ❌ | ❌ | ❌ | ❌ | 4,096 | 1,024 |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table(text_generation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating Code with In-Memory Logic\n",
    "\n",
    "**Explanation:**\n",
    "This prompt generates a fully functional but simplified version of our application. By asking for an in-memory database, we allow the LLM to focus on generating the correct API structure, endpoints, and Pydantic models without the added complexity of database integration code. This gives us a clean, working baseline that we can build upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating FastAPI app with in-memory database ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "In-memory Enhanced prompt\n",
      " <persona>\n",
      "You are a senior Python developer and FastAPI expert.\n",
      "</persona>\n",
      "\n",
      "<context>\n",
      "You are tasked with generating the entire contents of a single FastAPI entry-point file (main.py) for a “new-hire onboarding” application.  \n",
      "Key requirements derived from the project’s SQL schema:\n",
      "\n",
      "SQL Schema (for reference only):\n",
      "```\n",
      "CREATE TABLE users (\n",
      "    id          INTEGER PRIMARY KEY,\n",
      "    name        TEXT NOT NULL,\n",
      "    email       TEXT NOT NULL UNIQUE,\n",
      "    role        TEXT NOT NULL CHECK (role IN ('New Hire', 'Manager', 'People Ops Admin'))\n",
      ");\n",
      "\n",
      "CREATE TABLE onboarding_tasks (\n",
      "    id          INTEGER PRIMARY KEY,\n",
      "    title       TEXT NOT NULL,\n",
      "    description TEXT NOT NULL,\n",
      "    due_date    DATE NOT NULL,\n",
      "    status      TEXT NOT NULL CHECK (status IN ('Pending', 'Completed')),\n",
      "    user_id     INTEGER NOT NULL,\n",
      "    FOREIGN KEY (user_id) REFERENCES users(id)\n",
      ");\n",
      "\n",
      "CREATE INDEX idx_onboarding_tasks_user_id ON onboarding_tasks(user_id);\n",
      "```\n",
      "\n",
      "Business rules to implement:\n",
      "1. Only the `users` resource is in scope for now.\n",
      "2. Use an in-memory Python list as a fake database (no actual DB connectivity).\n",
      "3. Provide full CRUD endpoints for `/users` (create, read all, read by ID).  \n",
      "4. Pydantic models must include: `id`, `name`, `email`, `role`.  \n",
      "5. Include all necessary FastAPI imports.\n",
      "6. Endpoints must operate solely on the in-memory list.\n",
      "</context>\n",
      "\n",
      "<instructions>\n",
      "1. Think step by step to ensure every requirement is met.  \n",
      "2. Generate syntactically correct, runnable Python 3.10+ code.  \n",
      "3. Place the entire solution in a single file named `main.py`; do not split into multiple modules.  \n",
      "4. Do not include explanatory text before or after the code block. Output only the raw Python code.\n",
      "</instructions>\n",
      "\n",
      "<output_format>\n",
      "Output: a single Python code block containing the full contents of `main.py` and nothing else.\n",
      "</output_format>\n",
      "✅ Successfully saved artifact to: artifacts/app/main_in_memory.py\n",
      "Saved in-memory API to app/main_in_memory.py\n"
     ]
    }
   ],
   "source": [
    "in_memory_api_prompt = f\"\"\"\n",
    "You are a senior Python developer creating a FastAPI application for a new hire onboarding tool.\n",
    "\n",
    "Based on the following SQL schema, generate a single Python script for a `main.py` file that includes:\n",
    "1.  All necessary FastAPI imports.\n",
    "2.  Pydantic models for creating and reading `User` resources. Include fields for `id`, `name`, `email`, and `role`.\n",
    "3.  A simple in-memory list to act as a fake database for users.\n",
    "4.  Complete FastAPI CRUD endpoints for the `/users` path (POST, GET all, GET by ID).\n",
    "5.  The endpoints should perform their logic on the in-memory list.\n",
    "\n",
    "**SQL Schema Context:**\n",
    "```sql\n",
    "{sql_schema}\n",
    "```\n",
    "\n",
    "Output only the raw Python code.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating FastAPI app with in-memory database ---\")\n",
    "if sql_schema:\n",
    "    # Enhance the prompt so the model adopts a clear persona and structured output expectations\n",
    "    enhanced_in_memory_api_prompt = prompt_enhancer(in_memory_api_prompt)\n",
    "    print(\"In-memory Enhanced prompt\\n\", enhanced_in_memory_api_prompt)\n",
    "    generated_api_code = get_completion(enhanced_in_memory_api_prompt, in_memory_client, in_memory_model_name, in_memory_api_provider)\n",
    "    cleaned_code = clean_llm_output(generated_api_code, language='python')\n",
    "    # Save this code to a temporary reference file\n",
    "    save_artifact(cleaned_code, \"app/main_in_memory.py\")\n",
    "    print(\"Saved in-memory API to app/main_in_memory.py\")\n",
    "else:\n",
    "    print(\"Skipping API generation because schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Database Models and Session Code\n",
    "\n",
    "**Explanation:**\n",
    "This prompt is highly specific. It asks for the two key components needed for database connectivity in a modern Python application: the ORM (Object-Relational Mapping) models and the session management code. \n",
    "-   **SQLAlchemy Models:** These classes map our Python objects directly to the tables in our database, allowing us to work with Python code instead of raw SQL.\n",
    "-   **Session Management:** This is the standard FastAPI pattern for handling database connections. The `get_db` function is a dependency that ensures each API request gets a database session and that the session is properly closed afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating SQLAlchemy Models and Session Code ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "DB Code Enhanced prompt\n",
      " <persona>\n",
      "You are an expert Python developer specializing in FastAPI and SQLAlchemy.\n",
      "</persona>\n",
      "\n",
      "<context>\n",
      "You must create Python code that connects a FastAPI application to a SQLite database file named `onboarding.db`.  \n",
      "The database schema is:\n",
      "\n",
      "```\n",
      "CREATE TABLE users (\n",
      "    id          INTEGER PRIMARY KEY,\n",
      "    name        TEXT NOT NULL,\n",
      "    email       TEXT NOT NULL UNIQUE,\n",
      "    role        TEXT NOT NULL CHECK (role IN ('New Hire', 'Manager', 'People Ops Admin'))\n",
      ");\n",
      "\n",
      "CREATE TABLE onboarding_tasks (\n",
      "    id          INTEGER PRIMARY KEY,\n",
      "    title       TEXT NOT NULL,\n",
      "    description TEXT NOT NULL,\n",
      "    due_date    DATE NOT NULL,\n",
      "    status      TEXT NOT NULL CHECK (status IN ('Pending', 'Completed')),\n",
      "    user_id     INTEGER NOT NULL,\n",
      "    FOREIGN KEY (user_id) REFERENCES users(id)\n",
      ");\n",
      "\n",
      "CREATE INDEX idx_onboarding_tasks_user_id ON onboarding_tasks(user_id);\n",
      "```\n",
      "</context>\n",
      "\n",
      "<instructions>\n",
      "1. Generate two separate, well-commented Python code blocks:\n",
      "   • Code Block 1: SQLAlchemy models mapping to the `users` and `onboarding_tasks` tables.  \n",
      "   • Code Block 2: Standard boilerplate for SQLAlchemy engine creation (`sqlite:///onboarding.db`), `SessionLocal`, and the `get_db` dependency for FastAPI.\n",
      "\n",
      "2. Use SQLAlchemy 1.4+ declarative syntax (`from sqlalchemy.orm import declarative_base, relationship`).\n",
      "\n",
      "3. Include relationships where appropriate (e.g., a `user` relationship on `OnboardingTask` and a `tasks` back-populated relationship on `User`).\n",
      "\n",
      "4. Each block must start with ```python and end with ```.\n",
      "\n",
      "5. Output only the two code blocks—no narrative text, comments outside the code fences, or additional formatting.\n",
      "</instructions>\n",
      "\n",
      "<output_format>\n",
      "```python\n",
      "# Code Block 1: SQLAlchemy Models\n",
      "...\n",
      "```\n",
      "\n",
      "```python\n",
      "# Code Block 2: Session Management Boilerplate\n",
      "...\n",
      "```\n",
      "</output_format>\n",
      "\n",
      "--- Generated Database Code (cleaned) ---\n",
      "# Code Block 1: SQLAlchemy Models\n",
      "from sqlalchemy import Column, Integer, String, Text, Date, ForeignKey, CheckConstraint\n",
      "from sqlalchemy.orm import declarative_base, relationship\n",
      "\n",
      "Base = declarative_base()\n",
      "\n",
      "class User(Base):\n",
      "    \"\"\"SQLAlchemy model for the users table\"\"\"\n",
      "    __tablename__ = 'users'\n",
      "    \n",
      "    # Primary key column\n",
      "    id = Column(Integer, primary_key=True)\n",
      "    \n",
      "    # User attributes\n",
      "    name = Column(Text, nullable=False)\n",
      "    email = Column(Text, nullable=False, unique=True)\n",
      "    role = Column(\n",
      "        Text, \n",
      "        nullable=False,\n",
      "        # Check constraint to ensure role is one of the allowed values\n",
      "        info={'check_constraint': \"role IN ('New Hire', 'Manager', 'People Ops Admin')\"}\n",
      "    )\n",
      "    \n",
      "    # Relationship to onboarding tasks (one-to-many)\n",
      "    tasks = relationship(\"OnboardingTask\", back_populates=\"user\", cascade=\"all, delete-orphan\")\n",
      "\n",
      "\n",
      "class OnboardingTask(Base):\n",
      "    \"\"\"SQLAlchemy model for the onboarding_tasks table\"\"\"\n",
      "    __tablename__ = 'onboarding_tasks'\n",
      "    \n",
      "    # Primary key column\n",
      "    id = Column(Integer, primary_key=True)\n",
      "    \n",
      "    # Task attributes\n",
      "    title = Column(Text, nullable=False)\n",
      "    description = Column(Text, nullable=False)\n",
      "    due_date = Column(Date, nullable=False)\n",
      "    status = Column(\n",
      "        Text, \n",
      "        nullable=False,\n",
      "        # Check constraint to ensure status is either Pending or Completed\n",
      "        info={'check_constraint': \"status IN ('Pending', 'Completed')\"}\n",
      "    )\n",
      "    \n",
      "    # Foreign key to users table\n",
      "    user_id = Column(Integer, ForeignKey('users.id'), nullable=False, index=True)\n",
      "    \n",
      "    # Relationship to user (many-to-one)\n",
      "    user = relationship(\"User\", back_populates=\"tasks\")\n",
      "✅ Successfully saved artifact to: artifacts/app/db_models.py\n",
      "Saved DB models and session code to app/db_models.py\n"
     ]
    }
   ],
   "source": [
    "db_code_prompt = f\"\"\"\n",
    "You are a Python expert specializing in FastAPI and SQLAlchemy.\n",
    "\n",
    "Based on the provided SQL schema, generate the necessary Python code to connect a FastAPI application to a SQLite database named 'onboarding.db'.\n",
    "\n",
    "**SQL Schema Context:**\n",
    "```sql\n",
    "{sql_schema}\n",
    "```\n",
    "\n",
    "Please provide two separate, well-commented code blocks:\n",
    "\n",
    "1.  **SQLAlchemy Models:** Create the Python classes that map to the `users` and `onboarding_tasks` tables.\n",
    "2.  **Database Session Management:** Provide the standard boilerplate code for creating the SQLAlchemy engine, the `SessionLocal` class, and the `get_db` dependency for FastAPI.\n",
    "\n",
    "Only output the raw Python code.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating SQLAlchemy Models and Session Code ---\")\n",
    "if sql_schema:\n",
    "    # Enhance the DB prompt to ensure precise, well-structured output from the model\n",
    "    enhanced_db_code_prompt = prompt_enhancer(db_code_prompt)\n",
    "    print(\"DB Code Enhanced prompt\\n\", enhanced_db_code_prompt)\n",
    "    generated_db_code = get_completion(enhanced_db_code_prompt, db_client, db_model_name, db_api_provider)\n",
    "    # Clean and save the generated DB code to an artifact so the integration step can use it.\n",
    "    cleaned_db_code = clean_llm_output(generated_db_code, language='python')\n",
    "    print(\"\\n--- Generated Database Code (cleaned) ---\")\n",
    "    print(cleaned_db_code)\n",
    "    save_artifact(cleaned_db_code, \"app/db_models.py\")\n",
    "    print(\"Saved DB models and session code to app/db_models.py\")\n",
    "else:\n",
    "    print(\"Skipping DB code generation because schema is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Integrating Live Database Logic\n",
    "\n",
    "**Explanation:**\n",
    "This final code represents the crucial role of the developer in an AI-assisted workflow. The AI provided the components (Pydantic models, SQLAlchemy models, endpoint structure), but the developer is responsible for the final integration, ensuring all the pieces work together seamlessly. This involves combining the generated code blocks and replacing the in-memory list operations with live SQLAlchemy database calls (`db.add`, `db.query`, `db.commit`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Integrating generated artifacts into final app/main.py ---\n",
      "✅ LLM Client configured: Using 'openai' with model 'o3'\n",
      "--- Final integrated app (preview) ---\n",
      "from typing import List\n",
      "\n",
      "from fastapi import Depends, FastAPI, HTTPException\n",
      "from sqlalchemy.orm import Session\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from .db_models import SessionLocal, User as UserModel\n",
      "\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "\n",
      "# Pydantic Schemas\n",
      "class UserBase(BaseModel):\n",
      "    email: str\n",
      "\n",
      "\n",
      "class UserCreate(UserBase):\n",
      "    password: str\n",
      "\n",
      "\n",
      "class User(UserBase):\n",
      "    id: int\n",
      "    is_active: bool\n",
      "\n",
      "    class Config:\n",
      "        orm_mode = True\n",
      "\n",
      "\n",
      "# Dependency\n",
      "def get_db():\n",
      "    db = SessionLocal()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "\n",
      "@app.post(\"/users/\", response_model=User)\n",
      "def create_user(user: UserCreate, db: Session = Depends(get_db)):\n",
      "    db_user = db.query(UserModel).filter(UserModel.email == user.email).first()\n",
      "    if db_user:\n",
      "        raise HTTPException(status_code=400, detail=\"Email already registered\")\n",
      "    \n",
      "    # In a real app, hash the password\n",
      "    fake_hashed_password = user.password + \"notreallyhashed\"\n",
      "    new_user = UserModel(email=user.email, hashed_password=fake_hashed_password)\n",
      "    \n",
      "    db.add(new_user)\n",
      "    db.commit()\n",
      "    db.refresh(new_user)\n",
      "    return new_user\n",
      "\n",
      "\n",
      "@app.get(\"/users/\", response_model=List[User])\n",
      "def read_users(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):\n",
      "    users = db.query(UserModel).offset(skip).limit(limit).all()\n",
      "    return users\n",
      "\n",
      "\n",
      "@app.get(\"/users/{user_id}\", response_model=User)\n",
      "def read_user(user_id: int, db: Session = Depends(get_db)):\n",
      "    db_user = db.query(UserModel).filter(UserModel.id == user_id).first()\n",
      "    if db_user is None:\n",
      "        raise HTTPException(status_code=404, detail=\"User not found\")\n",
      "    return db_user\n",
      "✅ Successfully saved artifact to: artifacts/app/main.py\n",
      "Saved integrated app to app/main.py\n",
      "--- Final Integrated API Code for app/main.py ---\n",
      "from typing import List\n",
      "\n",
      "from fastapi import Depends, FastAPI, HTTPException\n",
      "from sqlalchemy.orm import Session\n",
      "from pydantic import BaseModel\n",
      "\n",
      "from .db_models import SessionLocal, User as UserModel\n",
      "\n",
      "\n",
      "app = FastAPI()\n",
      "\n",
      "\n",
      "# Pydantic Schemas\n",
      "class UserBase(BaseModel):\n",
      "    email: str\n",
      "\n",
      "\n",
      "class UserCreate(UserBase):\n",
      "    password: str\n",
      "\n",
      "\n",
      "class User(UserBase):\n",
      "    id: int\n",
      "    is_active: bool\n",
      "\n",
      "    class Config:\n",
      "        orm_mode = True\n",
      "\n",
      "\n",
      "# Dependency\n",
      "def get_db():\n",
      "    db = SessionLocal()\n",
      "    try:\n",
      "        yield db\n",
      "    finally:\n",
      "        db.close()\n",
      "\n",
      "\n",
      "@app.post(\"/users/\", response_model=User)\n",
      "def create_user(user: UserCreate, db: Session = Depends(get_db)):\n",
      "    db_user = db.query(UserModel).filter(UserModel.email == user.email).first()\n",
      "    if db_user:\n",
      "        raise HTTPException(status_code=400, detail=\"Email already registered\")\n",
      "    \n",
      "    # In a real app, hash the password\n",
      "    fake_hashed_password = user.password + \"notreallyhashed\"\n",
      "    new_user = UserModel(email=user.email, hashed_password=fake_hashed_password)\n",
      "    \n",
      "    db.add(new_user)\n",
      "    db.commit()\n",
      "    db.refresh(new_user)\n",
      "    return new_user\n",
      "\n",
      "\n",
      "@app.get(\"/users/\", response_model=List[User])\n",
      "def read_users(skip: int = 0, limit: int = 100, db: Session = Depends(get_db)):\n",
      "    users = db.query(UserModel).offset(skip).limit(limit).all()\n",
      "    return users\n",
      "\n",
      "\n",
      "@app.get(\"/users/{user_id}\", response_model=User)\n",
      "def read_user(user_id: int, db: Session = Depends(get_db)):\n",
      "    db_user = db.query(UserModel).filter(UserModel.id == user_id).first()\n",
      "    if db_user is None:\n",
      "        raise HTTPException(status_code=404, detail=\"User not found\")\n",
      "    return db_user\n",
      "✅ Successfully saved artifact to: artifacts/app/main.py\n"
     ]
    }
   ],
   "source": [
    "# Integration step: combine generated artifacts into a minimal live `app/main.py`\n",
    "print(\"--- Integrating generated artifacts into final app/main.py ---\")\n",
    "\n",
    "# Load generated artifacts from the artifacts folder (artifacts/app/*)\n",
    "in_memory_code = load_artifact(\"app/main_in_memory.py\")\n",
    "db_models_code = load_artifact(\"app/db_models.py\")\n",
    "\n",
    "integration_prompt = (\n",
    "    \"You are a pragmatic Python developer. \"\n",
    "    \"Produce a minimal `app/main.py` that imports SessionLocal, engine, and User from `app.db_models` \"\n",
    "    \"and implements POST /users/ (create), GET /users/ (list), GET /users/{user_id} (retrieve). \"\n",
    "    \"Use explicit SQLAlchemy calls (db.add, db.query, db.commit, db.refresh). \"\n",
    "    \"Do NOT inline large artifact sources; they are available at artifacts/app/main_in_memory.py and artifacts/app/db_models.py. \"\n",
    "    \"Use './artifacts/onboarding.db' as the SQLite file and ensure response models set orm_mode=True. \"\n",
    "    \"Return only the raw Python source for app/main.py.\"\n",
    ")\n",
    "\n",
    "# Combine prompt and metadata, then enhance\n",
    "prompt = prompt_enhancer(integration_prompt)\n",
    "\n",
    "# Request integration code from the integration model\n",
    "integration_output = get_completion(\n",
    "    prompt,\n",
    "    integration_client,\n",
    "    integration_model_name,\n",
    "    integration_api_provider,\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "# Clean and persist the result\n",
    "cleaned_integration = clean_llm_output(integration_output, language=\"python\")\n",
    "print(\"--- Final integrated app (preview) ---\")\n",
    "print(cleaned_integration)\n",
    "save_artifact(cleaned_integration, \"app/main.py\")\n",
    "print(\"Saved integrated app to app/main.py\")\n",
    "\n",
    "# Expose final_api_code for downstream cells that may reference it\n",
    "final_api_code = cleaned_integration\n",
    "print(\"--- Final Integrated API Code for app/main.py ---\")\n",
    "print(final_api_code)\n",
    "save_artifact(final_api_code, \"app/main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Congratulations! You have successfully generated and assembled a complete, database-connected backend API. You used an LLM to generate the boilerplate for both the API endpoints and the database models, and then performed the crucial engineering task of integrating them. You now have a working `main.py` file in your `app` directory that can create, read, update, and delete data in a live database. In the next lab, we will write a comprehensive test suite for this API.\n",
    "\n",
    "> **Key Takeaway:** AI excels at generating boilerplate code (like models and endpoint structures), but the developer's critical role is in the final integration and wiring of these components into a coherent, working system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
