{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1 - Lab 1: AI-Powered Requirements & User Stories (Solution)\n",
    "\n",
    "**Objective:** Use a Large Language Model (LLM) to decompose a vague problem statement into structured features, user personas, and Agile user stories, culminating in a machine-readable JSON artifact.\n",
    "\n",
    "**Introduction:**\n",
    "This notebook contains the complete solution for Lab 1. It demonstrates how to use an LLM to systematically break down a problem, generate structured requirements, and programmatically validate the output. Each step includes explanations of the code and the reasoning behind the prompts.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Purpose:** This initial block of code prepares our environment for the lab. It adds the project root to the system path to ensure our `utils.py` helper script can be imported, and then initializes the LLM API client.\n",
    "\n",
    "**Model Selection:**\n",
    "Our `utils.py` script is configured to work with multiple AI providers. You can change the `model_name` parameter in the `setup_llm_client()` function to any of the models listed in the `RECOMMENDED_MODELS` dictionary in `utils.py`. For example, to use a Hugging Face model, you could change the line to: `client, model_name, api_provider = setup_llm_client(model_name=\"meta-llama/Llama-3.3-70B-Instruct\")`\n",
    "\n",
    "**Libraries Explained:**\n",
    "- **`os`**, **`sys`**: Standard Python libraries for interacting with the file system and Python's path, ensuring our modules are discoverable.\n",
    "- **`json`**: A standard library for working with JSON data. We use `json.loads` to parse the LLM's text output into a Python dictionary or list, and `json.dumps` to format Python objects into a pretty-printed JSON string for saving.\n",
    "- **`utils`**: Our custom helper script. \n",
    "  - `setup_llm_client()`: Handles reading the `.env` file and initializing the API client.\n",
    "  - `get_completion()`: Simplifies the process of sending a prompt to the LLM and receiving a text response.\n",
    "  - `save_artifact()`: Ensures our project artifacts are stored consistently in the `artifacts` directory.\n",
    "  - `clean_llm_output()`: A new standardized function to remove markdown fences from LLM outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/armando/Documents/Github/AG-AISOFTDEV/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM Client configured: Using 'huggingface' with model 'deepseek-ai/DeepSeek-V3'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    # Assumes the notebook is in 'labs/Day_01_.../'\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    # Fallback for different execution environments\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, clean_llm_output, recommended_models_table\n",
    "\n",
    "# Initialize the LLM client. You can change the model here.\n",
    "# For example: setup_llm_client(model_name=\"gemini-2.5-flash\")\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"deepseek-ai/DeepSeek-V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Model | Provider | Vision | Image Gen | Audio Transcription | Context Window | Max Output Tokens |\n",
       "|---|---|---|---|---|---|---|\n",
       "| claude-opus-4-1-20250805 | anthropic | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\n",
       "| claude-opus-4-20250514 | anthropic | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\n",
       "| claude-sonnet-4-20250514 | anthropic | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 100,000 |\n",
       "| codex-mini | openai | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\n",
       "| dall-e-3 | openai | ‚ùå | ‚úÖ | ‚ùå | - | - |\n",
       "| deepseek-ai/DeepSeek-V3 | huggingface | ‚ùå | ‚ùå | ‚ùå | 128,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-V3-Small | huggingface | ‚ùå | ‚ùå | ‚ùå | 128,000 | 100,000 |\n",
       "| deepseek-ai/DeepSeek-VL2 | huggingface | ‚úÖ | ‚ùå | ‚ùå | 32,000 | 8,000 |\n",
       "| deepseek-ai/DeepSeek-VL2-Small | huggingface | ‚úÖ | ‚ùå | ‚ùå | 32,000 | 8,000 |\n",
       "| deepseek-ai/DeepSeek-VL2-Tiny | huggingface | ‚úÖ | ‚ùå | ‚ùå | 32,000 | 8,000 |\n",
       "| deepseek-ai/Janus-Pro-7B | huggingface | ‚úÖ | ‚ùå | ‚ùå | 8,192 | 2,048 |\n",
       "| gemini-2.0-flash | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-lite | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 8,192 |\n",
       "| gemini-2.0-flash-live-001 | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 8,192 |\n",
       "| gemini-2.5-flash | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-flash-image-preview | google | ‚úÖ | ‚úÖ | ‚ùå | 32,768 | 32,768 |\n",
       "| gemini-2.5-flash-lite | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 65,536 |\n",
       "| gemini-2.5-pro | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 65,536 |\n",
       "| gemini-deep-think | google | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 100,000 |\n",
       "| gemini-live-2.5-flash-preview | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 8,192 |\n",
       "| gemini-veo-3 | google | ‚úÖ | ‚ùå | ‚ùå | - | - |\n",
       "| google-cloud/speech-to-text/latest_long | google | ‚ùå | ‚ùå | ‚úÖ | - | - |\n",
       "| google-cloud/speech-to-text/latest_short | google | ‚ùå | ‚ùå | ‚úÖ | - | - |\n",
       "| gpt-4.1 | openai | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-mini | openai | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 32,000 |\n",
       "| gpt-4.1-nano | openai | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 32,000 |\n",
       "| gpt-4.5 | openai | ‚úÖ | ‚ùå | ‚ùå | 128,000 | 16,384 |\n",
       "| gpt-4o | openai | ‚úÖ | ‚ùå | ‚ùå | 128,000 | 16,384 |\n",
       "| gpt-4o-mini | openai | ‚úÖ | ‚ùå | ‚ùå | 128,000 | 16,384 |\n",
       "| gpt-5-2025-08-07 | openai | ‚úÖ | ‚ùå | ‚ùå | 400,000 | 128,000 |\n",
       "| gpt-5-mini-2025-08-07 | openai | ‚úÖ | ‚ùå | ‚ùå | 400,000 | 128,000 |\n",
       "| gpt-5-nano-2025-08-07 | openai | ‚úÖ | ‚ùå | ‚ùå | 400,000 | 128,000 |\n",
       "| gpt-image-1 | openai | ‚úÖ | ‚úÖ | ‚ùå | - | - |\n",
       "| imagen-3.0-generate-002 | google | ‚ùå | ‚úÖ | ‚ùå | - | - |\n",
       "| imagen-4.0-generate-001 | google | ‚ùå | ‚úÖ | ‚ùå | 480 | - |\n",
       "| meta-llama/Llama-3.3-70B-Instruct | huggingface | ‚ùå | ‚ùå | ‚ùå | 4,096 | 1,024 |\n",
       "| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 100,000 |\n",
       "| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ‚úÖ | ‚ùå | ‚ùå | 10,000,000 | 100,000 |\n",
       "| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ‚ùå | ‚ùå | ‚ùå | 32,768 | 8,192 |\n",
       "| o3 | openai | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\n",
       "| o4-mini | openai | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3 | huggingface | ‚ùå | ‚ùå | ‚ùå | 4,096 | 1,024 |\n",
       "| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ‚ùå | ‚ùå | ‚ùå | 4,096 | 1,024 |\n",
       "| whisper-1 | openai | ‚ùå | ‚ùå | ‚úÖ | - | - |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'| Model | Provider | Vision | Image Gen | Audio Transcription | Context Window | Max Output Tokens |\\n|---|---|---|---|---|---|---|\\n| claude-opus-4-1-20250805 | anthropic | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\\n| claude-opus-4-20250514 | anthropic | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\\n| claude-sonnet-4-20250514 | anthropic | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 100,000 |\\n| codex-mini | openai | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\\n| dall-e-3 | openai | ‚ùå | ‚úÖ | ‚ùå | - | - |\\n| deepseek-ai/DeepSeek-V3 | huggingface | ‚ùå | ‚ùå | ‚ùå | 128,000 | 100,000 |\\n| deepseek-ai/DeepSeek-V3-Small | huggingface | ‚ùå | ‚ùå | ‚ùå | 128,000 | 100,000 |\\n| deepseek-ai/DeepSeek-VL2 | huggingface | ‚úÖ | ‚ùå | ‚ùå | 32,000 | 8,000 |\\n| deepseek-ai/DeepSeek-VL2-Small | huggingface | ‚úÖ | ‚ùå | ‚ùå | 32,000 | 8,000 |\\n| deepseek-ai/DeepSeek-VL2-Tiny | huggingface | ‚úÖ | ‚ùå | ‚ùå | 32,000 | 8,000 |\\n| deepseek-ai/Janus-Pro-7B | huggingface | ‚úÖ | ‚ùå | ‚ùå | 8,192 | 2,048 |\\n| gemini-2.0-flash | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-lite | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 8,192 |\\n| gemini-2.0-flash-live-001 | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 8,192 |\\n| gemini-2.5-flash | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 65,536 |\\n| gemini-2.5-flash-image-preview | google | ‚úÖ | ‚úÖ | ‚ùå | 32,768 | 32,768 |\\n| gemini-2.5-flash-lite | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 65,536 |\\n| gemini-2.5-pro | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 65,536 |\\n| gemini-deep-think | google | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 100,000 |\\n| gemini-live-2.5-flash-preview | google | ‚úÖ | ‚ùå | ‚ùå | 1,048,576 | 8,192 |\\n| gemini-veo-3 | google | ‚úÖ | ‚ùå | ‚ùå | - | - |\\n| google-cloud/speech-to-text/latest_long | google | ‚ùå | ‚ùå | ‚úÖ | - | - |\\n| google-cloud/speech-to-text/latest_short | google | ‚ùå | ‚ùå | ‚úÖ | - | - |\\n| gpt-4.1 | openai | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 32,000 |\\n| gpt-4.1-mini | openai | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 32,000 |\\n| gpt-4.1-nano | openai | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 32,000 |\\n| gpt-4.5 | openai | ‚úÖ | ‚ùå | ‚ùå | 128,000 | 16,384 |\\n| gpt-4o | openai | ‚úÖ | ‚ùå | ‚ùå | 128,000 | 16,384 |\\n| gpt-4o-mini | openai | ‚úÖ | ‚ùå | ‚ùå | 128,000 | 16,384 |\\n| gpt-5-2025-08-07 | openai | ‚úÖ | ‚ùå | ‚ùå | 400,000 | 128,000 |\\n| gpt-5-mini-2025-08-07 | openai | ‚úÖ | ‚ùå | ‚ùå | 400,000 | 128,000 |\\n| gpt-5-nano-2025-08-07 | openai | ‚úÖ | ‚ùå | ‚ùå | 400,000 | 128,000 |\\n| gpt-image-1 | openai | ‚úÖ | ‚úÖ | ‚ùå | - | - |\\n| imagen-3.0-generate-002 | google | ‚ùå | ‚úÖ | ‚ùå | - | - |\\n| imagen-4.0-generate-001 | google | ‚ùå | ‚úÖ | ‚ùå | 480 | - |\\n| meta-llama/Llama-3.3-70B-Instruct | huggingface | ‚ùå | ‚ùå | ‚ùå | 4,096 | 1,024 |\\n| meta-llama/Llama-4-Maverick-17B-128E-Instruct | huggingface | ‚úÖ | ‚ùå | ‚ùå | 1,000,000 | 100,000 |\\n| meta-llama/Llama-4-Scout-17B-16E-Instruct | huggingface | ‚úÖ | ‚ùå | ‚ùå | 10,000,000 | 100,000 |\\n| mistralai/Mistral-7B-Instruct-v0.3 | huggingface | ‚ùå | ‚ùå | ‚ùå | 32,768 | 8,192 |\\n| o3 | openai | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\\n| o4-mini | openai | ‚úÖ | ‚ùå | ‚ùå | 200,000 | 100,000 |\\n| tokyotech-llm/Llama-3.1-Swallow-70B-Instruct-v0.3 | huggingface | ‚ùå | ‚ùå | ‚ùå | 4,096 | 1,024 |\\n| tokyotech-llm/Llama-3.1-Swallow-8B-Instruct-v0.5 | huggingface | ‚ùå | ‚ùå | ‚ùå | 4,096 | 1,024 |\\n| whisper-1 | openai | ‚ùå | ‚ùå | ‚úÖ | - | - |'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_models_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Problem Statement\n",
    "\n",
    "We define our starting point‚Äîa simple, high-level problem statement‚Äîas a Python variable. This makes it easy to reuse in multiple prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_statement = \"We need a tool to help our company's new hires get up to speed.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges\n",
    "\n",
    "Here are the complete solutions for each challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Brainstorming Features\n",
    "\n",
    "**Explanation:**\n",
    "This first challenge is about exploration. We use simple, direct prompts to get the LLM's initial thoughts on the problem. The goal is to generate a broad set of ideas (features and personas) that will serve as the raw material for the more structured tasks to follow. We expect the output to be human-readable markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Brainstorming Features ---\n",
      "- **Welcome Portal**: A centralized dashboard for new hires to access all necessary resources and information.\n",
      "- **Onboarding Checklist**: Step-by-step tasks and milestones to guide new hires through their first weeks.\n",
      "- **Company Handbook**: Digital access to company policies, culture, and expectations.\n",
      "- **Training Modules**: Interactive courses on company tools, processes, and role-specific skills.\n",
      "- **Mentorship Program**: Pairing new hires with experienced employees for guidance and support.\n",
      "- **Employee Directory**: A searchable database of team members with contact information and roles.\n",
      "- **Document Upload**: A secure way for new hires to submit required paperwork (e.g., ID, tax forms).\n",
      "- **FAQ Section**: Answers to common questions about onboarding, benefits, and company practices.\n",
      "- **Calendar Integration**: Sync important onboarding events and deadlines with personal calendars.\n",
      "- **Feedback Mechanism**: A way for new hires to share their onboarding experience and suggest improvements.\n",
      "- **Goal Setting**: Tools for new hires to set and track personal and professional goals.\n",
      "- **Social Integration**: Features to help new hires connect with colleagues (e.g., team introductions, social events).\n",
      "- **Progress Tracker**: Visual indicators of onboarding progress and remaining tasks.\n",
      "- **Multi-language Support**: Options for onboarding materials in multiple languages.\n",
      "- **Mobile Accessibility**: A mobile-friendly version or app for on-the-go access.\n",
      "- **Gamification**: Rewards or badges for completing onboarding tasks and milestones.\n",
      "- **IT Setup Assistance**: Guidance on setting up email, software, and hardware.\n",
      "- **Benefits Overview**: Detailed information on company benefits, enrollment, and contacts.\n",
      "- **Compliance Training**: Mandatory courses on workplace safety, harassment, and legal requirements.\n",
      "- **Customizable Content**: Tailored onboarding materials based on role, department, or location.\n",
      "\n",
      "--- Identifying User Personas ---\n",
      "Here are three distinct user personas who would interact with the tool designed to help new hires get up to speed, along with their roles and main goals:\n",
      "\n",
      "---\n",
      "\n",
      "### **1. New Hire (Primary User)**  \n",
      "**Role:** Recent hire joining the company, potentially unfamiliar with the organization‚Äôs processes, culture, and tools.  \n",
      "**Main Goal:** To quickly acclimate to the company, understand their role, and gain the knowledge and skills needed to perform effectively in their new position.  \n",
      "**Needs:**  \n",
      "- Clear onboarding instructions and resources.  \n",
      "- Guidance on company policies, tools, and workflows.  \n",
      "- Support to build confidence and feel integrated into the team.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. HR/Onboarding Specialist**  \n",
      "**Role:** Professional responsible for managing the onboarding process and ensuring new hires have a smooth transition into the company.  \n",
      "**Main Goal:** To streamline the onboarding experience, reduce administrative burden, and ensure new hires are well-prepared and engaged.  \n",
      "**Needs:**  \n",
      "- A centralized tool to manage onboarding tasks and resources.  \n",
      "- Metrics to track new hire progress and satisfaction.  \n",
      "- Ability to customize onboarding content for different roles or departments.\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Team Manager/Supervisor**  \n",
      "**Role:** Manager or supervisor of the new hire, responsible for their integration into the team and ensuring they meet job expectations.  \n",
      "**Main Goal:** To efficiently onboard new team members, minimize downtime, and help them become productive contributors as quickly as possible.  \n",
      "**Needs:**  \n",
      "- Insights into the new hire‚Äôs progress and areas where they may need additional support.  \n",
      "- Access to onboarding materials aligned with team-specific goals.  \n",
      "- Tools to facilitate communication and collaboration with the new hire during the onboarding process.\n",
      "\n",
      "---\n",
      "\n",
      "These personas highlight the diverse needs and goals of users interacting with the tool, ensuring it addresses the requirements of new hires, HR professionals, and managers alike.\n"
     ]
    }
   ],
   "source": [
    "# This prompt is direct and open-ended, encouraging the LLM to be creative.\n",
    "features_prompt = f\"\"\"\n",
    "Based on the problem statement: '{problem_statement}', brainstorm a list of potential features for a new hire onboarding tool. \n",
    "Format the output as a simple markdown list.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Brainstorming Features ---\")\n",
    "brainstormed_features = get_completion(features_prompt, client, model_name, api_provider)\n",
    "print(brainstormed_features)\n",
    "\n",
    "# This prompt asks for specific roles to ground the brainstorming in user-centric thinking.\n",
    "personas_prompt = f\"\"\"\n",
    "Based on the problem statement: '{problem_statement}', identify and describe three distinct user personas who would interact with this tool. \n",
    "For each persona, describe their role and main goal.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Identifying User Personas ---\")\n",
    "user_personas = get_completion(personas_prompt, client, model_name, api_provider)\n",
    "print(user_personas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Formal User Stories\n",
    "\n",
    "**Explanation:**\n",
    "This challenge represents a significant increase in complexity and value. We are no longer asking for simple text; we are demanding a specific, structured data format (JSON). \n",
    "\n",
    "The prompt is carefully engineered:\n",
    "1.  **Persona:** `You are a Senior Product Manager...` tells the LLM the role it should adopt.\n",
    "2.  **Context:** We provide the previous outputs (`problem_statement`, `brainstormed_features`, `user_personas`) inside `<context>` tags to give the LLM all the necessary information.\n",
    "3.  **Format:** The `OUTPUT REQUIREMENTS` section is extremely explicit. It tells the LLM to *only* output JSON, defines the exact keys for each object, and specifies the format for nested data (like the array of Gherkin strings). This strictness is key to getting reliable, machine-readable output.\n",
    "4.  **Parsing:** The `try...except` block is a crucial step. It attempts to parse the LLM's string output into a Python list of dictionaries. If it succeeds, we know the LLM followed our instructions perfectly. If it fails, we print the raw output to help debug the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating User Stories as JSON ---\n",
      "Raw LLM response length: 3122 characters\n",
      "First 200 characters of response:\n",
      "'[\\n    {\\n        \"id\": 1,\\n        \"persona\": \"New Hire\",\\n        \"user_story\": \"As a New Hire, I want to access a centralized Welcome Portal, so that I can easily find all the resources and information'\n",
      "\n",
      "Cleaned JSON length: 3122 characters\n",
      "‚úÖ Successfully parsed LLM output as JSON.\n",
      "Number of user stories generated: 5\n",
      "\n",
      "--- Sample User Story ---\n",
      "{\n",
      "  \"id\": 1,\n",
      "  \"persona\": \"New Hire\",\n",
      "  \"user_story\": \"As a New Hire, I want to access a centralized Welcome Portal, so that I can easily find all the resources and information I need to get started.\",\n",
      "  \"acceptance_criteria\": [\n",
      "    \"Given I am a new hire, When I log into the Welcome Portal, Then I should see a dashboard with links to all necessary resources.\",\n",
      "    \"Given I am a new hire, When I navigate the Welcome Portal, Then I should be able to access the Company Handbook, Onboarding Checklist, and Training Modules.\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# The prompt is highly-structured to guide the LLM toward a perfect JSON output.\n",
    "json_user_stories_prompt = f\"\"\"\n",
    "You are a Senior Product Manager creating a product backlog for a new hire onboarding tool.\n",
    "\n",
    "Based on the following context:\n",
    "<context>\n",
    "Problem Statement: {problem_statement}\n",
    "Potential Features: {brainstormed_features}\n",
    "User Personas: {user_personas}\n",
    "</context>\n",
    "\n",
    "Your task is to generate a list of 5 detailed user stories.\n",
    "\n",
    "**OUTPUT REQUIREMENTS**:\n",
    "- You MUST output a valid JSON array. Your response must begin with [ and end with ]. Do not include any text or markdown before or after the JSON array.\n",
    "- Each object in the array must represent a single user story.\n",
    "- Each object must have the following keys: 'id' (an integer), 'persona' (a string from the personas), 'user_story' (a string in the format 'As a [persona], I want [goal], so that [benefit].'), and 'acceptance_criteria' (an array of strings, with each string in Gherkin format 'Given/When/Then').\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating User Stories as JSON ---\")\n",
    "# We set a lower temperature to encourage the LLM to stick to the requested format.\n",
    "json_output_str = get_completion(json_user_stories_prompt, client, model_name, api_provider, temperature=0.2)\n",
    "\n",
    "print(f\"Raw LLM response length: {len(json_output_str)} characters\")\n",
    "print(\"First 200 characters of response:\")\n",
    "print(repr(json_output_str[:200]))\n",
    "\n",
    "# Attempt to parse the string output into a Python list.\n",
    "try:\n",
    "    # Use our new standardized cleaning function from utils.py\n",
    "    cleaned_json_str = clean_llm_output(json_output_str, language='json')\n",
    "    print(f\"\\nCleaned JSON length: {len(cleaned_json_str)} characters\")\n",
    "    \n",
    "    user_stories_json = json.loads(cleaned_json_str)\n",
    "    print(\"‚úÖ Successfully parsed LLM output as JSON.\")\n",
    "    print(f\"Number of user stories generated: {len(user_stories_json)}\")\n",
    "    \n",
    "    # Pretty-print the first user story to verify its structure\n",
    "    print(\"\\n--- Sample User Story ---\")\n",
    "    print(json.dumps(user_stories_json[0], indent=2))\n",
    "    \n",
    "except (json.JSONDecodeError, TypeError, IndexError) as e:\n",
    "    print(f\"‚ùå Error: Failed to parse LLM output as JSON. Error: {e}\")\n",
    "    print(\"\\n--- DEBUGGING INFO ---\")\n",
    "    print(\"Raw LLM Output:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(json_output_str)\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if 'cleaned_json_str' in locals():\n",
    "        print(\"\\nCleaned JSON:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(cleaned_json_str)\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    user_stories_json = [] # Assign an empty list to prevent errors in the next cell\n",
    "    print(\"\\n‚ö†Ô∏è  Set user_stories_json to empty list to prevent downstream errors.\")\n",
    "    print(\"   Please check the API key configuration and re-run this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Programmatic Validation and Artifact Creation\n",
    "\n",
    "**Explanation:**\n",
    "This is the final and most critical step. We treat the LLM's output as untrusted input and subject it to programmatic validation. This ensures that the artifact we create is reliable and can be consumed by other automated tools in later stages of the SDLC without causing errors. \n",
    "\n",
    "The `validate_and_save_stories` function acts as a gatekeeper. It checks for the correct data types (a list of objects) and ensures that all required fields are present in each object. Only if all checks pass do we proceed to save the file using `save_artifact`. This creates a trustworthy `day1_user_stories.json` file that can be confidently used as an input for other automated processes in our SDLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_save_stories(stories_data):\n",
    "    \"\"\"Validates the structure of the user stories data and saves it if valid.\"\"\"\n",
    "    if not isinstance(stories_data, list) or not stories_data:\n",
    "        print(\"Validation Failed: Data is not a non-empty list.\")\n",
    "        return False\n",
    "\n",
    "    required_keys = ['id', 'persona', 'user_story', 'acceptance_criteria']\n",
    "    all_stories_valid = True\n",
    "\n",
    "    # Loop through each story object in the list.\n",
    "    for i, story in enumerate(stories_data):\n",
    "        # Check for the presence of all required keys.\n",
    "        if not all(key in story for key in required_keys):\n",
    "            print(f\"Validation Failed: Story at index {i} is missing one or more required keys.\")\n",
    "            print(f\"   Expected keys: {required_keys}\")\n",
    "            print(f\"   Found keys: {list(story.keys()) if isinstance(story, dict) else 'Not a dictionary'}\")\n",
    "            all_stories_valid = False\n",
    "            continue # Don't bother with further checks for this invalid story\n",
    "        \n",
    "        # Check that the acceptance criteria is a list with at least one item.\n",
    "        ac = story.get('acceptance_criteria')\n",
    "        if not isinstance(ac, list) or not ac:\n",
    "            print(f\"Validation Failed: Story at index {i} (ID: '{story.get('id')}') has invalid or empty acceptance criteria.\")\n",
    "            print(f\"   Expected: list with at least one item\")\n",
    "            print(f\"   Found: {type(ac)} with value {ac}\")\n",
    "            all_stories_valid = False\n",
    "\n",
    "    # Only save the artifact if all stories in the list are valid.\n",
    "    if all_stories_valid:\n",
    "        print(f\"\\n‚úÖ All {len(stories_data)} user stories passed validation.\")\n",
    "        artifact_path = \"artifacts/day1_user_stories.json\"\n",
    "        \n",
    "        # Use the helper function to save the file, creating the 'artifacts' directory if needed.\n",
    "        # We use json.dumps with an indent to make the saved file human-readable.\n",
    "        save_artifact(json.dumps(stories_data, indent=2), artifact_path)\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Validation failed for one or more stories. Artifact not saved.\")\n",
    "        return False\n",
    "\n",
    "# Note: The actual validation call is now in the next cell with better error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DIAGNOSTIC INFO ===\n",
      "user_stories_json exists: <class 'list'>\n",
      "Length: 5\n",
      "Sample content: {'id': 1, 'persona': 'New Hire - Entry-Level Employee', 'user_story': 'As a New Hire - Entry-Level Employee, I want an interactive onboarding checklist, so that I can track my progress and ensure I complete all necessary tasks.', 'acceptance_criteria': ['Given I am a new hire, When I log into the onboarding tool, Then I should see an interactive checklist with tasks to complete.', 'Given I have completed a task, When I mark it as done, Then the checklist should update to reflect my progress.', 'Given I have pending tasks, When a deadline is approaching, Then I should receive a reminder notification.']}\n",
      "\n",
      "Raw LLM output length: 3161 characters\n",
      "First 200 characters of raw output:\n",
      "'```json\\n[\\n    {\\n        \"id\": 1,\\n        \"persona\": \"New Hire - Entry-Level Employee\",\\n        \"user_story\": \"As a New Hire - Entry-Level Employee, I want an interactive onboarding checklist, so that '\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic: Check the current state of user_stories_json\n",
    "print(\"=== DIAGNOSTIC INFO ===\")\n",
    "if 'user_stories_json' in locals():\n",
    "    print(f\"user_stories_json exists: {type(user_stories_json)}\")\n",
    "    print(f\"Length: {len(user_stories_json) if hasattr(user_stories_json, '__len__') else 'N/A'}\")\n",
    "    if user_stories_json:\n",
    "        print(\"Sample content:\", user_stories_json[0] if len(user_stories_json) > 0 else \"Empty list\")\n",
    "    else:\n",
    "        print(\"user_stories_json is empty or falsy\")\n",
    "        print(\"This means JSON parsing likely failed in the previous cell.\")\n",
    "        print(\"Check the raw LLM output above for formatting issues.\")\n",
    "else:\n",
    "    print(\"user_stories_json variable does not exist\")\n",
    "    print(\"This means the previous cell never executed successfully\")\n",
    "\n",
    "# Also check if we have the raw output\n",
    "if 'json_output_str' in locals():\n",
    "    print(f\"\\nRaw LLM output length: {len(json_output_str)} characters\")\n",
    "    print(\"First 200 characters of raw output:\")\n",
    "    print(repr(json_output_str[:200]))\n",
    "else:\n",
    "    print(\"json_output_str not available\")\n",
    "print(\"========================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDATION STEP ===\n",
      "‚úÖ Found user_stories_json with 5 stories\n",
      "\n",
      "‚úÖ All 5 user stories passed validation.\n",
      "‚úÖ Successfully saved artifact to: artifacts/day1_user_stories.json\n"
     ]
    }
   ],
   "source": [
    "# Run the validation function on the data we parsed from the LLM.\n",
    "print(\"=== VALIDATION STEP ===\")\n",
    "\n",
    "if 'user_stories_json' not in locals():\n",
    "    print(\"‚ùå ERROR: user_stories_json variable not found.\")\n",
    "    print(\"   Make sure to run the previous cell that generates user stories.\")\n",
    "elif not user_stories_json:\n",
    "    print(\"‚ùå ERROR: user_stories_json is empty or None.\")\n",
    "    print(\"   This usually means JSON parsing failed in the previous step.\")\n",
    "    print(\"   Solutions:\")\n",
    "    print(\"   1. Check that your API keys are correctly configured\")\n",
    "    print(\"   2. Re-run the previous cell to generate user stories\")\n",
    "    print(\"   3. Examine the raw LLM output for formatting issues\")\n",
    "    \n",
    "    # Try to re-parse if we have the raw output\n",
    "    if 'json_output_str' in locals() and json_output_str.strip():\n",
    "        print(\"\\nüîÑ Attempting to re-parse the JSON...\")\n",
    "        try:\n",
    "            cleaned_json_str = clean_llm_output(json_output_str, language='json')\n",
    "            user_stories_json = json.loads(cleaned_json_str)\n",
    "            print(\"‚úÖ Re-parsing successful! Proceeding with validation...\")\n",
    "            validate_and_save_stories(user_stories_json)\n",
    "        except (json.JSONDecodeError, TypeError) as e:\n",
    "            print(f\"‚ùå Re-parsing failed: {e}\")\n",
    "            print(\"Raw output that failed to parse:\")\n",
    "            print(\"-\" * 50)\n",
    "            print(json_output_str)\n",
    "            print(\"-\" * 50)\n",
    "else:\n",
    "    print(f\"‚úÖ Found user_stories_json with {len(user_stories_json)} stories\")\n",
    "    validate_and_save_stories(user_stories_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Congratulations! You have completed the first lab. You started with a vague, one-sentence problem and finished with a structured, validated, machine-readable requirements artifact. This is the critical first step in an AI-assisted software development lifecycle. The `day1_user_stories.json` file you created will be the direct input for our next lab, where we will generate a formal Product Requirements Document (PRD).\n",
    "\n",
    "> **Key Takeaway:** The single most important skill demonstrated in this lab is turning unstructured ideas into structured, machine-readable data (JSON). This transformation is what enables automation and integration with other tools later in the SDLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
