{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance (Solution)\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Introduction:**\n",
    "This solution notebook provides the complete prompts and explanations for generating a robust test suite. It covers generating simple tests, brainstorming edge cases, and creating the necessary fixtures for professional-grade database testing.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "**Explanation:**\n",
    "We load the application's source code to provide the LLM with the necessary context to write accurate tests. A good prompt for test generation should always include the code that needs to be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o'\n",
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o-mini'\n",
      "✅ LLM Client configured: Using 'openai' with model 'gpt-4o-mini'\n",
      "Happy-path model: gpt-4o (provider: openai)\n",
      "Edge-case model: gpt-4o-mini (provider: openai)\n",
      "Fixture model: gpt-4o-mini (provider: openai)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output, prompt_enhancer\n",
    "\n",
    "# Initialize separate LLM clients for each lab task so we can experiment with different models per task.\n",
    "# You can change the model_name for each client to a model available in your environment or the RECOMMENDED_MODELS table.\n",
    "# Use a stronger model for generation of structured tests, and lighter models for edge cases/fixtures to save quota in examples.\n",
    "happy_client, happy_model, happy_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "edge_client, edge_model, edge_provider = setup_llm_client(model_name=\"gpt-4o-mini\")\n",
    "fixture_client, fixture_model, fixture_provider = setup_llm_client(model_name=\"gpt-4o-mini\")\n",
    "\n",
    "print(f\"Happy-path model: {happy_model} (provider: {happy_provider})\")\n",
    "print(f\"Edge-case model: {edge_model} (provider: {edge_provider})\")\n",
    "print(f\"Fixture model: {fixture_model} (provider: {fixture_provider})\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Explanation:**\n",
    "This prompt asks for the most straightforward type of test: one that verifies the application works as expected when given valid input. We specifically ask for tests for the `POST` and `GET` endpoints. The prompt includes the full application code as context, which is crucial for the LLM to understand the API's structure, expected payloads, and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "from fastapi.testclient import TestClient\n",
      "from .main import app  # Assuming the FastAPI app is in a file named main.py\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "def test_create_user():\n",
      "    # Send a request to create a user\n",
      "    response = client.post(\"/users/\", json={\"email\": \"test@example.com\", \"password\": \"testpassword\"})\n",
      "    # Assert that the response status code is 200\n",
      "    assert response.status_code == 200\n",
      "    # Assert that the response email matches the input email\n",
      "    assert response.json()[\"email\"] == \"test@example.com\"\n",
      "\n",
      "def test_read_users():\n",
      "    # Create a user first\n",
      "    client.post(\"/users/\", json={\"email\": \"test@example.com\", \"password\": \"testpassword\"})\n",
      "    \n",
      "    # Send a request to retrieve the list of users\n",
      "    response = client.get(\"/users/\")\n",
      "    # Assert that the response status code is 200\n",
      "    assert response.status_code == 200\n",
      "    # Assert that the response contains a list with at least one user\n",
      "    assert len(response.json()) > 0\n",
      "✅ Successfully saved artifact to: artifacts/tests/test_main_simple.py\n",
      "from fastapi.testclient import TestClient\n",
      "from .main import app  # Assuming the FastAPI app is in a file named main.py\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "def test_create_user():\n",
      "    # Send a request to create a user\n",
      "    response = client.post(\"/users/\", json={\"email\": \"test@example.com\", \"password\": \"testpassword\"})\n",
      "    # Assert that the response status code is 200\n",
      "    assert response.status_code == 200\n",
      "    # Assert that the response email matches the input email\n",
      "    assert response.json()[\"email\"] == \"test@example.com\"\n",
      "\n",
      "def test_read_users():\n",
      "    # Create a user first\n",
      "    client.post(\"/users/\", json={\"email\": \"test@example.com\", \"password\": \"testpassword\"})\n",
      "    \n",
      "    # Send a request to retrieve the list of users\n",
      "    response = client.get(\"/users/\")\n",
      "    # Assert that the response status code is 200\n",
      "    assert response.status_code == 200\n",
      "    # Assert that the response contains a list with at least one user\n",
      "    assert len(response.json()) > 0\n",
      "✅ Successfully saved artifact to: artifacts/tests/test_main_simple.py\n"
     ]
    }
   ],
   "source": [
    "happy_path_tests_prompt = f\"\"\"\n",
    "You are a Senior QA Engineer writing tests for a FastAPI application using pytest.\n",
    "\n",
    "Based on the application code provided below, please generate two 'happy path' test functions in a single Python script:\n",
    "1. A test named `test_create_user` for the `POST /users/` endpoint. It should create a user and assert that the status code is 200 and the response email matches the input.\n",
    "2. A test named `test_read_users` for the `GET /users/` endpoint. It should first create a user and then assert the status code is 200 and that the response is a list containing at least one user.\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Your response should be only the raw Python code for the tests, including necessary imports like `TestClient` from `fastapi.testclient`.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    # Enhance the prompt for more consistent and focused output\n",
    "    enhanced_happy_prompt = prompt_enhancer(happy_path_tests_prompt, model_name=happy_model, client=happy_client, api_provider=happy_provider)\n",
    "    generated_happy_path_tests = get_completion(enhanced_happy_prompt, happy_client, happy_model, happy_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\")\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Explanation:**\n",
    "Good testing goes beyond the happy path. This prompt asks the LLM to think about what could go wrong. We specifically request tests for two common failure modes: creating a duplicate resource (which should be disallowed) and requesting a resource that doesn't exist. This demonstrates how AI can be used as a creative partner to brainstorm potential failure points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Edge Case Tests ---\n",
      "The provided code consists of two test functions designed to validate the behavior of a user management API. Let's break down each test case:\n",
      "\n",
      "1. **Test for Duplicate Email Registration**:\n",
      "   - The first function, `test_create_user_duplicate_email`, tests the scenario where a user tries to register with an email that is already in use.\n",
      "   - It begins by creating a user with a unique email (`\"test@example.com\"`).\n",
      "   - It then attempts to create another user with the same email, expecting the API to respond with a status code of 400 Bad Request and a specific error message indicating that the email has already been registered.\n",
      "\n",
      "2. **Test for Non-Existent User Retrieval**:\n",
      "   - The second function, `test_read_user_not_found`, tests the scenario where a user attempts to retrieve a user by an ID that does not exist in the system (in this case, ID `999`).\n",
      "   - The test expects the API to respond with a status code of 404 Not Found and a message indicating that the user was not found.\n",
      "\n",
      "Both tests are well-structured and utilize assertions to verify that the API behaves as expected under these conditions.\n",
      "\n",
      "Here’s a summary of the key points from each test:\n",
      "\n",
      "### `test_create_user_duplicate_email`:\n",
      "- **Purpose**: Ensure that the API does not allow duplicate email registrations.\n",
      "- **Expected Outcome**: Status code `400` and message `{\"detail\": \"Email already registered\"}`.\n",
      "\n",
      "### `test_read_user_not_found`:\n",
      "- **Purpose**: Ensure that the API returns a not found error when attempting to retrieve a non-existent user.\n",
      "- **Expected Outcome**: Status code `404` and message `{\"detail\": \"User not found\"}`.\n",
      "\n",
      "### Improvements or Additional Considerations:\n",
      "- **Setup and Teardown**: Depending on the test framework in use, you may want to consider adding setup and teardown steps to ensure that tests do not interfere with each other. For example, if the `db_session` is shared, it might be wise to clean it up after each test.\n",
      "- **More Edge Cases**: Additional tests could be added for other scenarios, such as:\n",
      "  - Validating the password complexity requirements.\n",
      "  - Checking the behavior when trying to create a user with missing fields.\n",
      "  - Testing the retrieval of valid users.\n",
      "- **Mocking External Dependencies**: If the API interacts with any external services (e.g., email verification), you might want to mock those interactions in your tests to ensure they run quickly and reliably. \n",
      "\n",
      "Overall, the provided tests are a solid foundation for ensuring the user management functionality works correctly in the specified scenarios.\n",
      "The provided code consists of two test functions designed to validate the behavior of a user management API. Let's break down each test case:\n",
      "\n",
      "1. **Test for Duplicate Email Registration**:\n",
      "   - The first function, `test_create_user_duplicate_email`, tests the scenario where a user tries to register with an email that is already in use.\n",
      "   - It begins by creating a user with a unique email (`\"test@example.com\"`).\n",
      "   - It then attempts to create another user with the same email, expecting the API to respond with a status code of 400 Bad Request and a specific error message indicating that the email has already been registered.\n",
      "\n",
      "2. **Test for Non-Existent User Retrieval**:\n",
      "   - The second function, `test_read_user_not_found`, tests the scenario where a user attempts to retrieve a user by an ID that does not exist in the system (in this case, ID `999`).\n",
      "   - The test expects the API to respond with a status code of 404 Not Found and a message indicating that the user was not found.\n",
      "\n",
      "Both tests are well-structured and utilize assertions to verify that the API behaves as expected under these conditions.\n",
      "\n",
      "Here’s a summary of the key points from each test:\n",
      "\n",
      "### `test_create_user_duplicate_email`:\n",
      "- **Purpose**: Ensure that the API does not allow duplicate email registrations.\n",
      "- **Expected Outcome**: Status code `400` and message `{\"detail\": \"Email already registered\"}`.\n",
      "\n",
      "### `test_read_user_not_found`:\n",
      "- **Purpose**: Ensure that the API returns a not found error when attempting to retrieve a non-existent user.\n",
      "- **Expected Outcome**: Status code `404` and message `{\"detail\": \"User not found\"}`.\n",
      "\n",
      "### Improvements or Additional Considerations:\n",
      "- **Setup and Teardown**: Depending on the test framework in use, you may want to consider adding setup and teardown steps to ensure that tests do not interfere with each other. For example, if the `db_session` is shared, it might be wise to clean it up after each test.\n",
      "- **More Edge Cases**: Additional tests could be added for other scenarios, such as:\n",
      "  - Validating the password complexity requirements.\n",
      "  - Checking the behavior when trying to create a user with missing fields.\n",
      "  - Testing the retrieval of valid users.\n",
      "- **Mocking External Dependencies**: If the API interacts with any external services (e.g., email verification), you might want to mock those interactions in your tests to ensure they run quickly and reliably. \n",
      "\n",
      "Overall, the provided tests are a solid foundation for ensuring the user management functionality works correctly in the specified scenarios.\n"
     ]
    }
   ],
   "source": [
    "edge_case_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer focused on identifying edge cases.\n",
    "\n",
    "Based on the FastAPI application code provided, write two test functions for common error scenarios:\n",
    "1.  A test named `test_create_user_duplicate_email` that attempts to create a user with an email that already exists. It must assert that the API returns a 400 status code.\n",
    "2.  A test named `test_read_user_not_found` that attempts to GET a user with an ID that does not exist (e.g., 999). It must assert that the API returns a 404 status code.\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Output only the raw Python code for these two test functions.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    enhanced_edge_prompt = prompt_enhancer(edge_case_tests_prompt, model_name=edge_model, client=edge_client, api_provider=edge_provider)\n",
    "    generated_edge_case_tests = get_completion(enhanced_edge_prompt, edge_client, edge_model, edge_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "    # In a real scenario, you'd append these to your test file.\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Explanation:**\n",
    "This is the most advanced and most important concept in this lab. A `pytest` fixture is a function that runs before each test to set up a specific state or resource. \n",
    "\n",
    "The prompt asks the LLM to generate a fixture that creates an isolated, in-memory database for testing. This is a best practice because it ensures tests are independent and don't interfere with each other or with the real development database. \n",
    "\n",
    "We specifically instruct the LLM to save this fixture in `tests/conftest.py`. `conftest.py` is a special file that pytest automatically discovers. Fixtures defined here are globally available to all test files in the same directory and subdirectories, making it the ideal place to put shared setup code like a database connection. \n",
    "\n",
    "Finally, we ask the LLM to refactor our original happy-path tests to *use* this fixture by simply adding it as a function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Pytest DB Fixture for conftest.py ---\n",
      "import pytest\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker, scoped_session\n",
      "from myapp.database import Base  # Adjust the import according to your project structure\n",
      "\n",
      "@pytest.fixture(scope='function')\n",
      "def db_session():\n",
      "    engine = create_engine('sqlite:///:memory:')\n",
      "    Base.metadata.create_all(engine)\n",
      "    \n",
      "    Session = scoped_session(sessionmaker(bind=engine))\n",
      "    session = Session()\n",
      "\n",
      "    yield session\n",
      "\n",
      "    session.close()\n",
      "    Session.remove()\n",
      "    Base.metadata.drop_all(engine)\n",
      "✅ Successfully saved artifact to: artifacts/tests/conftest.py\n",
      "\n",
      "--- Generating Refactored Tests for test_main_with_fixture.py ---\n",
      "import pytest\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker, scoped_session\n",
      "from myapp.database import Base  # Adjust the import according to your project structure\n",
      "\n",
      "@pytest.fixture(scope='function')\n",
      "def db_session():\n",
      "    engine = create_engine('sqlite:///:memory:')\n",
      "    Base.metadata.create_all(engine)\n",
      "    \n",
      "    Session = scoped_session(sessionmaker(bind=engine))\n",
      "    session = Session()\n",
      "\n",
      "    yield session\n",
      "\n",
      "    session.close()\n",
      "    Session.remove()\n",
      "    Base.metadata.drop_all(engine)\n",
      "✅ Successfully saved artifact to: artifacts/tests/conftest.py\n",
      "\n",
      "--- Generating Refactored Tests for test_main_with_fixture.py ---\n",
      "def test_create_user(client, db_session):\n",
      "    response = client.post(\"/users/\", json={\"email\": \"test@example.com\", \"password\": \"testpassword\"})\n",
      "    assert response.status_code == 200\n",
      "    assert response.json()[\"email\"] == \"test@example.com\"\n",
      "\n",
      "def test_get_user(client, db_session):\n",
      "    # First, create a user\n",
      "    create_response = client.post(\"/users/\", json={\"email\": \"test@example.com\", \"password\": \"testpassword\"})\n",
      "    assert create_response.status_code == 200\n",
      "\n",
      "    # Now, retrieve the user\n",
      "    user_id = create_response.json()[\"id\"]\n",
      "    response = client.get(f\"/users/{user_id}\")\n",
      "    assert response.status_code == 200\n",
      "    assert response.json()[\"email\"] == \"test@example.com\"\n",
      "\n",
      "def test_get_nonexistent_user(client, db_session):\n",
      "    response = client.get(\"/users/99999\")  # Assuming 99999 is a non-existent ID\n",
      "    assert response.status_code == 404\n",
      "✅ Successfully saved artifact to: artifacts/tests/test_main_with_fixture.py\n",
      "def test_create_user(client, db_session):\n",
      "    response = client.post(\"/users/\", json={\"email\": \"test@example.com\", \"password\": \"testpassword\"})\n",
      "    assert response.status_code == 200\n",
      "    assert response.json()[\"email\"] == \"test@example.com\"\n",
      "\n",
      "def test_get_user(client, db_session):\n",
      "    # First, create a user\n",
      "    create_response = client.post(\"/users/\", json={\"email\": \"test@example.com\", \"password\": \"testpassword\"})\n",
      "    assert create_response.status_code == 200\n",
      "\n",
      "    # Now, retrieve the user\n",
      "    user_id = create_response.json()[\"id\"]\n",
      "    response = client.get(f\"/users/{user_id}\")\n",
      "    assert response.status_code == 200\n",
      "    assert response.json()[\"email\"] == \"test@example.com\"\n",
      "\n",
      "def test_get_nonexistent_user(client, db_session):\n",
      "    response = client.get(\"/users/99999\")  # Assuming 99999 is a non-existent ID\n",
      "    assert response.status_code == 404\n",
      "✅ Successfully saved artifact to: artifacts/tests/test_main_with_fixture.py\n"
     ]
    }
   ],
   "source": [
    "db_fixture_prompt = f\"\"\"\n",
    "You are an expert in Python testing with pytest and FastAPI.\n",
    "\n",
    "I need to create a `pytest` fixture to provide an isolated, in-memory SQLite database session for each test run. This is a critical best practice for testing database-connected applications.\n",
    "\n",
    "Please generate the Python code for a file named `tests/conftest.py` that contains this fixture.\n",
    "\n",
    "The fixture should:\n",
    "1. Be named `db_session`.\n",
    "2. Configure a SQLAlchemy engine for an in-memory SQLite database.\n",
    "3. Create all database tables before the tests run.\n",
    "4. Yield a database session.\n",
    "5. Clean up the database tables after the tests are complete.\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Output only the raw Python code for the `conftest.py` file.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture for conftest.py ---\")\n",
    "if app_code:\n",
    "    enhanced_fixture_prompt = prompt_enhancer(db_fixture_prompt, model_name=fixture_model, client=fixture_client, api_provider=fixture_provider)\n",
    "    generated_db_fixture = get_completion(enhanced_fixture_prompt, fixture_client, fixture_model, fixture_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\")\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer refactoring a test suite to use a new database fixture.\n",
    "\n",
    "Given the following tests and the knowledge that a fixture named `db_session` and a `TestClient` instance named `client` are now available from `conftest.py`, please rewrite the tests to use them. The tests should no longer create their own client instances.\n",
    "\n",
    "**Original Tests:**\n",
    "```python\n",
    "{generated_happy_path_tests if 'generated_happy_path_tests' in locals() else ''}\n",
    "```\n",
    "\n",
    "**Application Code Context:**\n",
    "```python\n",
    "{app_code}\n",
    "```\n",
    "\n",
    "Output only the raw Python code for the refactored tests.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests for test_main_with_fixture.py ---\")\n",
    "if app_code and 'generated_happy_path_tests' in locals():\n",
    "    enhanced_refactor_prompt = prompt_enhancer(refactor_tests_prompt, model_name=fixture_model, client=fixture_client, api_provider=fixture_provider)\n",
    "    refactored_tests = get_completion(enhanced_refactor_prompt, fixture_client, fixture_model, fixture_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\")\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context or original tests are missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
